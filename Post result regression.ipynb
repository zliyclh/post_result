{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Packages import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sheet(dataframe):\n",
    "    '''\n",
    "    cleaning function for raw CSV import\n",
    "    '''\n",
    "    adj=dataframe.dropna(axis=1,how='all')\n",
    "    adj=dataframe.set_index(adj.columns[0]) #use stock tickers as the index\n",
    "    return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_date_transform(CSV_date,index=False):\n",
    "    '''\n",
    "    Transform the CSV price style string into dateframe string style\n",
    "    The CSV date follows US style which is MM/DD/YYYY\n",
    "    '''\n",
    "    if index==False:\n",
    "        timestamp=pd.Timestamp(int(CSV_date[CSV_date.find(\"/\",3)+1:]),\n",
    "                            int(CSV_date[:CSV_date.find(\"/\")]),\n",
    "                            int(CSV_date[CSV_date.find(\"/\",1)+1:CSV_date.find(\"/\",3)]))\n",
    "        return timestamp.strftime(\"%d/%b/%Y\")\n",
    "    else:\n",
    "        timestamp=pd.Timestamp(int(CSV_date[-4:]),int(CSV_date[3:5]),int(CSV_date[:2]))\n",
    "        return timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fundamental_date_transform(CSV_date):\n",
    "    '''\n",
    "    Transform the fundamental style string into dateframe string style\n",
    "    CSV date follow following style yyyy-mm-dd or MM/DD/YYYY\n",
    "    '''\n",
    "    if '-' in CSV_date:\n",
    "        timestamp=pd.Timestamp(int(CSV_date[:4]),\n",
    "                            int(CSV_date[5:7]),\n",
    "                            int(CSV_date[8:]))\n",
    "    else:\n",
    "        timestamp=pd.Timestamp(int(CSV_date[CSV_date.find(\"/\",3)+1:]),\n",
    "                               int(CSV_date[CSV_date.find(\"/\",1)+1:CSV_date.find(\"/\",3)]),\n",
    "                               int(CSV_date[:CSV_date.find(\"/\")]))        \n",
    "    return timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CSV_price(region,start,end,VWAP):\n",
    "    '''\n",
    "    Grab the pricing data from CSV\n",
    "    region= US, Europe,Asia,or Canada\n",
    "    start,end are in year\n",
    "    key is the reference to search\n",
    "    return the target price dataframe with timestamp on the column\n",
    "    '''\n",
    "    mylist=[]\n",
    "    for year in range(start,end+1):\n",
    "        if VWAP==False:\n",
    "            csv=pd.read_csv(r\"C:\\Users\\Eric.Li\\Documents\\Post result data\\{0} CSV\\{0}_price_{1}.csv\".format(region,year))\n",
    "        else:\n",
    "            csv=pd.read_csv(r\"C:\\Users\\Eric.Li\\Documents\\Post result data\\{0} CSV\\{0}_VWAP_{1}.csv\".format(region,year))\n",
    "        data=csv.set_index(\"Ticker\")\n",
    "        adj_data=data.loc[[x for x in data.index if type(x)==str]].replace('#N/A N/A','').replace(' #N/A N/A ','')\n",
    "        adj_data=adj_data.loc[[x for x in adj_data.index if len(x)>0]]\n",
    "        mylist.append(adj_data)\n",
    "\n",
    "    price=pd.concat(mylist,axis=1)\n",
    "    price=price.apply(lambda x:pd.to_numeric(x),axis=1)\n",
    "    \n",
    "    csv_index=pd.read_csv(r\"C:\\Users\\Eric.Li\\Documents\\Post result data\\{0} CSV\\{0}_price_index.csv\".format(region))\n",
    "    data_index=csv_index.set_index(\"Ticker\").T\n",
    "    price_index=data_index.replace('#N/A N/A','')\n",
    "    #price_index=data_index.apply(lambda x:pd.to_numeric(x),axis=1)\n",
    "    \n",
    "    price.columns=[price_date_transform(i) for i in price.columns]\n",
    "    '''\n",
    "    Need to sort the columns for index price, and then transform to date string\n",
    "    '''\n",
    "    price_index.columns=[price_date_transform(i) for i in price_index.columns]\n",
    "    #price_index=price_index.reindex(sorted(price_index.columns))\n",
    "    #price_index.columns=[i.strftime(\"%d/%b/%Y\") for i  in price_index.columns]\n",
    "    \n",
    "    abs_return=price.diff(1,axis=1)/price.shift(1,axis=1)\n",
    "    abs_return_index=price_index.diff(1,axis=1)/price_index.shift(1,axis=1)\n",
    "    return price,abs_return,price_index,abs_return_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CSV_EPS(region,start,end):\n",
    "    '''\n",
    "    Grab the pricing data from CSV database\n",
    "    region= US, Europe,Asia,or Canada\n",
    "    start,end are in year\n",
    "    key is the reference to search\n",
    "    return the target EPS dataframe with timestamp on the column\n",
    "    '''\n",
    "    mylist=[]\n",
    "    for year in range(start,end+1):\n",
    "        csv=pd.read_csv(r\"C:\\Users\\Eric.Li\\Documents\\Post result data\\{0} CSV\\{0}_EPS_{1}.csv\".format(region,year))\n",
    "        data=csv.set_index(\"Ticker\")\n",
    "        adj_data=data.loc[[x for x in data.index if type(x)==str]].replace('#N/A N/A','').replace(\" #N/A N/A \",\"\")\n",
    "        adj_data=adj_data.loc[[x for x in adj_data.index if len(x)>0]]\n",
    "        mylist.append(adj_data)\n",
    "\n",
    "    EPS=pd.concat(mylist,axis=1)\n",
    "    EPS=EPS.apply(lambda x:pd.to_numeric(x),axis=1)\n",
    "    \n",
    "    EPS.columns=[price_date_transform(i) for i in EPS.columns]\n",
    "    return EPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revision_calc(ticker,date,EPS_df,period):\n",
    "    '''\n",
    "    Calculate revision from ticker and reference date\n",
    "    '''\n",
    "    if type(date)==pd.lib.tslib.NaTType:\n",
    "        return None\n",
    "    elif type(date)==pd.lib.tslib.Timestamp:\n",
    "        date=date.strftime(\"%d/%b/%Y\")\n",
    "    elif type(date)==str:\n",
    "        date=date\n",
    "    \n",
    "    if ticker in EPS_df.index:\n",
    "        eps_series=EPS_df.loc[ticker]\n",
    "        date_series=eps_series.index.tolist()\n",
    "        if date in date_series:\n",
    "            day0=date_series.index(date)\n",
    "            post_series=eps_series.iloc[day0+period-1:day0+period+10]\n",
    "            pre_series=eps_series.iloc[day0-10:day0]\n",
    "            if len(post_series.dropna())==0 or len(pre_series.dropna())==0 or pre_series.dropna().iloc[-1]==0:\n",
    "                revision=None\n",
    "            else:\n",
    "                try:\n",
    "                    revision=np.divide(post_series.dropna().iloc[0],pre_series.dropna().iloc[-1])-1\n",
    "                except:\n",
    "                    revision=None\n",
    "            return revision\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv=pd.read_csv(r'C:\\Users\\Eric.Li\\Documents\\Post result code\\{0}.csv'.format('US'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#N/A Requesting Data...'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv[\"SI\"].dropna().sort_values().iloc[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CSV_fundamentals(region,price,EPS_df,revision_period,min_history,min_vol,use_cache):\n",
    "    '''\n",
    "    Grab the fundamental data from the spreadsheet\n",
    "    region= US, Europe,Asia,or Canada\n",
    "    return the post result fundamental dataframe\n",
    "    '''\n",
    "    \n",
    "    if use_cache is True:\n",
    "        try:\n",
    "            data=pd.read_csv(r'C:\\Users\\Eric.Li\\Documents\\Post result code\\{0}{1}_clean_reg.csv'.format(region,\\\n",
    "                                                                                                        str(revision_period)))\n",
    "            new_index=pd.MultiIndex.from_tuples(list(zip(data.iloc[:,0],data.iloc[:,1],data.iloc[:,2],data.iloc[:,3])))\n",
    "            data.index=new_index\n",
    "            target_data=data.iloc[:,4:]\n",
    "        except:\n",
    "            print(\"No such file!\")\n",
    "    else:\n",
    "        csv=pd.read_csv(r'C:\\Users\\Eric.Li\\Documents\\Post result code\\{0}.csv'.format(region))\n",
    "        data=csv.set_index(\"Ticker\").drop_duplicates().replace('#N/A Invalid Security','').\\\n",
    "        replace('#N/A Requesting Data...','')\n",
    "\n",
    "        data=data[data.index!='']\n",
    "        data=data.dropna(how=\"all\")\n",
    "\n",
    "        data[\"date_copy\"]=[fundamental_date_transform(i) for i in data[\"Date\"].copy()]\n",
    "        data[\"ticker_copy\"]=data.index\n",
    "        data=data.copy().sort_values(by=[\"ticker_copy\",\"date_copy\"])\n",
    "        data[\"next_date\"]=data[\"date_copy\"].shift(-1)\n",
    "        data[\"ticker_copy\"]=data[\"ticker_copy\"].shift(-1)\n",
    "        data[\"Date\"]=data[\"date_copy\"].copy().apply(lambda x: x.strftime(\"%d/%b/%Y\") if x!='' else np.nan)\n",
    "        data[\"Next\"]=data.apply(lambda x: x[\"next_date\"].strftime(\"%d/%b/%Y\") if type(x[\"next_date\"])==pd.Timestamp and \\\n",
    "                                x.name==x[\"ticker_copy\"] else np.nan,axis=1)\n",
    "\n",
    "#         data[\"period\"]=data.apply(lambda x:str(pd.Timestamp(datetime.strptime(x[\"Date\"],\"%d/%b/%Y\")).year)\\\n",
    "#                                             +\" \"+str(pd.Timestamp(datetime.strptime(x[\"Date\"],\"%d/%b/%Y\")).quarter),\\\n",
    "#                                             axis=1)\n",
    "\n",
    "        data[\"end_period\"]=data.apply(lambda x: pd.offsets.BQuarterEnd().rollforward(x[\"date_copy\"]).strftime(\"%d/%b/%Y\"),\\\n",
    "                                      axis=1)\n",
    "\n",
    "        data.index=pd.MultiIndex.from_tuples(list(zip(data.index,data[\"Date\"],data[\"Next\"],data[\"end_period\"])))\n",
    "\n",
    "        del data[\"ticker_copy\"]\n",
    "        del data[\"date_copy\"]\n",
    "        del data[\"next_date\"]\n",
    "        del data[\"Revision\"]\n",
    "        del data[\"end_period\"]\n",
    "        del data[\"Next\"]\n",
    "\n",
    "        for s in [\"Market cap\",\"Volume\",\"SI\",\"Broker\"]:\n",
    "            try:\n",
    "                data[s]=pd.to_numeric(data[s])\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "        '''\n",
    "        Add more realistic version of revision\n",
    "        '''\n",
    "        data[\"Revision_real\"]=data.apply(lambda x: revision_calc(x.name[0],x.name[1],EPS_df,revision_period),axis=1)\n",
    "        \n",
    "        data[\"Revision_20\"]=data.apply(lambda x: revision_calc(x.name[0],x.name[1],EPS_df,20),axis=1)\n",
    "        data=data[(data[\"Revision_20\"]>0)|(data[\"Revision_20\"]<0)]\n",
    "\n",
    "        '''\n",
    "        take out data with zero or none revision/market cap\n",
    "        '''\n",
    "        data=data[(data[\"Revision_real\"]>0)|(data[\"Revision_real\"]<0)]\n",
    "        data=data[(data[\"Market cap\"]>500)] #universe above 500mn\n",
    "        \n",
    "        '''\n",
    "        take out cases where there is a short history\n",
    "        '''\n",
    "        count_history=data.apply(lambda x: price.loc[x.name[0],:x.name[1]][-2*min_history:].count() if x.name[1] in \\\n",
    "                                   price.columns else None,axis=1)\n",
    "        \n",
    "        data=data.copy()[count_history>min_history]\n",
    "        \n",
    "        '''\n",
    "        Add momentum\n",
    "        '''\n",
    "        data[\"mom\"]=data.apply(lambda x: price.loc[x.name[0],:x.name[1]][-260:-23].dropna()[-1]/\\\n",
    "                               price.loc[x.name[0],:x.name[1]][-260:-23].dropna()[0] if x.name[1] in \\\n",
    "                               price.columns else None, axis=1)\n",
    "        \n",
    "        data=data[(data[\"mom\"]>0)|(data[\"mom\"]<0)]\n",
    "        \n",
    "        data[\"mom_short\"]=data.apply(lambda x: price.loc[x.name[0],:x.name[1]][-24:-1].dropna()[-1]/\\\n",
    "                               price.loc[x.name[0],:x.name[1]][-24:-1].dropna()[0] if x.name[1] in \\\n",
    "                               price.columns else None, axis=1)      \n",
    "\n",
    "        '''\n",
    "        Add historic volatility\n",
    "        '''\n",
    "        \n",
    "        abs_return=price.diff(1,axis=1)/price.shift(1,axis=1)\n",
    "        \n",
    "        data[\"30d_vol\"]=data.apply(lambda x: abs_return.loc[x.name[0],:x.name[1]][-31:-1].std() if x.name[1] in \\\n",
    "                                   abs_return.columns else None,axis=1)      \n",
    "        \n",
    "        data=data[data[\"30d_vol\"]>=min_vol]\n",
    "        \n",
    "        '''\n",
    "        Final cleaning and export the data\n",
    "        '''\n",
    "        target_data=data.drop_duplicates()\n",
    "        target_data.to_csv(r'C:\\Users\\Eric.Li\\Documents\\Post result code\\{0}{1}_clean_reg.csv'.format(region,\\\n",
    "                                                                                                      str(revision_period)))\n",
    "    return target_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - US data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_price,abs_return_US,US_index_price,abs_return_index_US=CSV_price('US',2006,2019,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_VWAP,abs_return_VWAP_US,US_index_VWAP,abs_return_VWAP_index_US=CSV_price('US',2006,2019,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_EPS =CSV_EPS(\"US\",2006,2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "US1=CSV_fundamentals(\"US\",US_price,US_EPS,1,120,0.005,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Asian data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asia index mmapping\n",
    "Asia_mapping=clean_sheet(pd.read_csv(\"Asia mapping.csv\")).dropna(how=\"all\").iloc[:,:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia_price,abs_return_Asia,Asia_index_price,abs_return_index_Asia=CSV_price('Asia',2010,2019,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia_EPS=CSV_EPS(\"Asia\",2010,2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia2_old=CSV_fundamentals(\"Asia\",abs_return_Asia,Asia_EPS,2,\"old\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia2_new=CSV_fundamentals(\"Asia\",abs_return_Asia,Asia_EPS,2,\"new\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - European data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Europe_price,abs_return_Europe,Europe_index_price,abs_return_index_Europe=CSV_price('Europe',2010,2019,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Europe_VWAP,abs_return_VWAP_Europe,Europe_index_VWAP,abs_return_index_Europe=CSV_price('Europe',2010,2019,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Europe_EPS=CSV_EPS(\"Europe\",2010,2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Europe2_old=CSV_fundamentals(\"Europe\",abs_return_Europe,Europe_EPS,2,'old')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Europe2_new=CSV_fundamentals(\"Europe\",abs_return_Europe,Europe_EPS,2,'new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_calc(ticker,date,price_df,index_df,period_tuple,abs_rel):\n",
    "    '''\n",
    "    Calculate return from the period tuple, abs_return if assigned abs\n",
    "    Day starts from zero, so 1 means start the return calculation one day after result, second element is the number of days\n",
    "    '''\n",
    "    if type(date)==pd.tslib.NaTType:\n",
    "        return None\n",
    "    elif type(date)==pd.tslib.Timestamp:\n",
    "        date=date.strftime(\"%d/%b/%Y\")\n",
    "    elif type(date)==str:\n",
    "        date=date\n",
    "    price_series=price_df.loc[ticker].dropna()\n",
    "    date_series=price_series.index.tolist()\n",
    "    \n",
    "    index_data_series=index_df.index.tolist()\n",
    "    \n",
    "    if date in date_series:\n",
    "        day0=date_series.index(date)\n",
    "        day0_index=index_data_series.index(date)\n",
    "        \n",
    "        start_price=price_series.iloc[:day0+period_tuple[0]].dropna().iloc[-1]\n",
    "        end_price=price_series.iloc[:day0+period_tuple[0]+period_tuple[1]].dropna().iloc[-1]\n",
    "        \n",
    "        target_series=price_series.iloc[day0+period_tuple[0]-2:day0+period_tuple[0]+period_tuple[1]]\n",
    "        \n",
    "        if start_price!=0:\n",
    "            abs_return=end_price/start_price-1\n",
    "        else:\n",
    "            abs_return=None\n",
    "        \n",
    "\n",
    "        \n",
    "        if abs_rel=='abs':\n",
    "            target_return=abs_return\n",
    "        else:\n",
    "\n",
    "            start_index=index_df.iloc[:day0_index+period_tuple[0]].dropna().iloc[-1]\n",
    "            end_index=index_df.iloc[:day0_index+period_tuple[0]+period_tuple[1]].dropna().iloc[-1]\n",
    "            if start_index!=0:\n",
    "                index_return=end_index/start_index-1\n",
    "            else:\n",
    "                index_return=None\n",
    "\n",
    "            if abs_return is None or index_return is None:\n",
    "                target_return=None\n",
    "            else:\n",
    "                target_return=abs_return-index_return\n",
    "        return target_return\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_US=US1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eric.Li\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "target_US[\"pre_announcement_return\"]=np.log(1+target_US.apply(lambda x:return_calc(x.name[0],x.name[1],US_price,\\\n",
    "                                                                           US_index_price.loc['SPX Index'],(-11,10),'rel')\\\n",
    "                                                              ,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eric.Li\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "target_US[\"EAR\"]=np.log(1+target_US.apply(lambda x:return_calc(x.name[0],x.name[1],US_price,\\\n",
    "                                                                           US_index_price.loc['SPX Index'],(0,2),'rel'),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eric.Li\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "target_US[\"target_return\"]=target_US.apply(lambda x:return_calc(x.name[0],x.name[1],US_price,\\\n",
    "                                                                           US_index_price.loc['SPX Index'],(2,20),'rel'),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in target_US[\"Supersector\"].dropna().unique():\n",
    "    target_US[i]=(target_US[\"Supersector\"]==i)*1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eric.Li\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\Eric.Li\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:5984: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n",
      "C:\\Users\\Eric.Li\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2910: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "C:\\Users\\Eric.Li\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "target_US[\"Revision_20\"][target_US[\"Revision_20\"]>0.99]=0.99\n",
    "target_US[\"Revision_20\"][target_US[\"Revision_20\"]<-0.99]=-0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eric.Li\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\Eric.Li\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "target_US[\"mom\"]=np.log(target_US[\"mom\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eric.Li\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "target_US[\"target_return_positive\"]=(target_US[\"target_return\"]>0.02)*1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eric.Li\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "target_US[\"target_return_negative\"]=(target_US[\"target_return\"]<-0.02)*1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=target_US[['Market cap','Revision_real','mom','30d_vol','pre_announcement_return','EAR','Broker','SI']+target_US[\"Supersector\"].dropna().unique().tolist()]\n",
    "y=target_US['target_return']\n",
    "y_log_positive=target_US['target_return_positive']\n",
    "y_log_negative=target_US['target_return_negative']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_result=x_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_result[\"y_predict\"]=regr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_result=y_test.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00956222,  0.00357225,  0.00885337, ...,  0.0111883 ,\n",
       "        0.008259  , -0.01010672])"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_result[\"predict\"]=regr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=sm.OLS(y_train,x_train).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>target_return</td>  <th>  R-squared:         </th>  <td>   0.005</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.005</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   12.66</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 18 Jun 2019</td> <th>  Prob (F-statistic):</th>  <td>3.53e-46</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:30:01</td>     <th>  Log-Likelihood:    </th>  <td>  58431.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 55771</td>      <th>  AIC:               </th> <td>-1.168e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 55748</td>      <th>  BIC:               </th> <td>-1.166e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    22</td>      <th>                     </th>      <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "               <td></td>                  <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Market cap</th>                  <td> -2.02e-08</td> <td> 9.76e-09</td> <td>   -2.069</td> <td> 0.039</td> <td>-3.93e-08</td> <td>-1.06e-09</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Revision_real</th>               <td>    0.0144</td> <td>    0.003</td> <td>    5.339</td> <td> 0.000</td> <td>    0.009</td> <td>    0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mom</th>                         <td>   -0.0052</td> <td>    0.001</td> <td>   -5.266</td> <td> 0.000</td> <td>   -0.007</td> <td>   -0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>30d_vol</th>                     <td>   -0.1363</td> <td>    0.027</td> <td>   -4.990</td> <td> 0.000</td> <td>   -0.190</td> <td>   -0.083</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Utilities</th>                   <td>    0.0044</td> <td>    0.002</td> <td>    2.065</td> <td> 0.039</td> <td>    0.000</td> <td>    0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Oil & Gas</th>                   <td>   -0.0047</td> <td>    0.002</td> <td>   -3.082</td> <td> 0.002</td> <td>   -0.008</td> <td>   -0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Health Care</th>                 <td>    0.0149</td> <td>    0.001</td> <td>   11.835</td> <td> 0.000</td> <td>    0.012</td> <td>    0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Personal & Household Goods</th>  <td>    0.0053</td> <td>    0.002</td> <td>    3.163</td> <td> 0.002</td> <td>    0.002</td> <td>    0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Retail</th>                      <td>    0.0073</td> <td>    0.001</td> <td>    5.085</td> <td> 0.000</td> <td>    0.005</td> <td>    0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Technology</th>                  <td>    0.0125</td> <td>    0.001</td> <td>   10.499</td> <td> 0.000</td> <td>    0.010</td> <td>    0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Telecommunications</th>          <td>    0.0109</td> <td>    0.003</td> <td>    3.155</td> <td> 0.002</td> <td>    0.004</td> <td>    0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Financial Services</th>          <td>    0.0018</td> <td>    0.002</td> <td>    0.976</td> <td> 0.329</td> <td>   -0.002</td> <td>    0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Travel & Leisure</th>            <td>    0.0038</td> <td>    0.002</td> <td>    2.126</td> <td> 0.034</td> <td>    0.000</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Industrial Goods & Services</th> <td>    0.0066</td> <td>    0.001</td> <td>    6.339</td> <td> 0.000</td> <td>    0.005</td> <td>    0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Banks</th>                       <td>    0.0051</td> <td>    0.002</td> <td>    3.105</td> <td> 0.002</td> <td>    0.002</td> <td>    0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Media</th>                       <td>    0.0041</td> <td>    0.002</td> <td>    1.887</td> <td> 0.059</td> <td>   -0.000</td> <td>    0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Automobiles & Parts</th>         <td>    0.0002</td> <td>    0.003</td> <td>    0.077</td> <td> 0.938</td> <td>   -0.006</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Insurance</th>                   <td>    0.0073</td> <td>    0.002</td> <td>    3.794</td> <td> 0.000</td> <td>    0.004</td> <td>    0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Basic Resources</th>             <td>    0.0020</td> <td>    0.003</td> <td>    0.778</td> <td> 0.437</td> <td>   -0.003</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Real Estate</th>                 <td>   -0.0023</td> <td>    0.006</td> <td>   -0.383</td> <td> 0.702</td> <td>   -0.014</td> <td>    0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Food & Beverage</th>             <td>    0.0143</td> <td>    0.002</td> <td>    6.481</td> <td> 0.000</td> <td>    0.010</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Construction & Materials</th>    <td>    0.0103</td> <td>    0.002</td> <td>    4.436</td> <td> 0.000</td> <td>    0.006</td> <td>    0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Chemicals</th>                   <td>    0.0079</td> <td>    0.002</td> <td>    3.234</td> <td> 0.001</td> <td>    0.003</td> <td>    0.013</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>17115.944</td> <th>  Durbin-Watson:     </th>  <td>   1.989</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>471793.137</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 0.889</td>   <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>17.137</td>   <th>  Cond. No.          </th>  <td>3.01e+06</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:          target_return   R-squared:                       0.005\n",
       "Model:                            OLS   Adj. R-squared:                  0.005\n",
       "Method:                 Least Squares   F-statistic:                     12.66\n",
       "Date:                Tue, 18 Jun 2019   Prob (F-statistic):           3.53e-46\n",
       "Time:                        16:30:01   Log-Likelihood:                 58431.\n",
       "No. Observations:               55771   AIC:                        -1.168e+05\n",
       "Df Residuals:                   55748   BIC:                        -1.166e+05\n",
       "Df Model:                          22                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===============================================================================================\n",
       "                                  coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------------------\n",
       "Market cap                   -2.02e-08   9.76e-09     -2.069      0.039   -3.93e-08   -1.06e-09\n",
       "Revision_real                   0.0144      0.003      5.339      0.000       0.009       0.020\n",
       "mom                            -0.0052      0.001     -5.266      0.000      -0.007      -0.003\n",
       "30d_vol                        -0.1363      0.027     -4.990      0.000      -0.190      -0.083\n",
       "Utilities                       0.0044      0.002      2.065      0.039       0.000       0.008\n",
       "Oil & Gas                      -0.0047      0.002     -3.082      0.002      -0.008      -0.002\n",
       "Health Care                     0.0149      0.001     11.835      0.000       0.012       0.017\n",
       "Personal & Household Goods      0.0053      0.002      3.163      0.002       0.002       0.009\n",
       "Retail                          0.0073      0.001      5.085      0.000       0.005       0.010\n",
       "Technology                      0.0125      0.001     10.499      0.000       0.010       0.015\n",
       "Telecommunications              0.0109      0.003      3.155      0.002       0.004       0.018\n",
       "Financial Services              0.0018      0.002      0.976      0.329      -0.002       0.005\n",
       "Travel & Leisure                0.0038      0.002      2.126      0.034       0.000       0.007\n",
       "Industrial Goods & Services     0.0066      0.001      6.339      0.000       0.005       0.009\n",
       "Banks                           0.0051      0.002      3.105      0.002       0.002       0.008\n",
       "Media                           0.0041      0.002      1.887      0.059      -0.000       0.008\n",
       "Automobiles & Parts             0.0002      0.003      0.077      0.938      -0.006       0.006\n",
       "Insurance                       0.0073      0.002      3.794      0.000       0.004       0.011\n",
       "Basic Resources                 0.0020      0.003      0.778      0.437      -0.003       0.007\n",
       "Real Estate                    -0.0023      0.006     -0.383      0.702      -0.014       0.009\n",
       "Food & Beverage                 0.0143      0.002      6.481      0.000       0.010       0.019\n",
       "Construction & Materials        0.0103      0.002      4.436      0.000       0.006       0.015\n",
       "Chemicals                       0.0079      0.002      3.234      0.001       0.003       0.013\n",
       "==============================================================================\n",
       "Omnibus:                    17115.944   Durbin-Watson:                   1.989\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           471793.137\n",
       "Skew:                           0.889   Prob(JB):                         0.00\n",
       "Kurtosis:                      17.137   Cond. No.                     3.01e+06\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 3.01e+06. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0111145D US  23/Feb/2007  27/Apr/2007  30/Mar/2007    1.0\n",
       "             01/Nov/2007  22/Feb/2008  31/Dec/2007    1.0\n",
       "             22/Feb/2008  01/May/2008  31/Mar/2008    0.0\n",
       "             01/May/2008  04/Aug/2008  30/Jun/2008    1.0\n",
       "             03/Nov/2008  25/Feb/2009  31/Dec/2008    0.0\n",
       "             01/May/2009  31/Jul/2009  30/Jun/2009    0.0\n",
       "             30/Oct/2009  24/Feb/2010  31/Dec/2009    0.0\n",
       "             24/Feb/2010  03/May/2010  31/Mar/2010    0.0\n",
       "             03/May/2010  03/Aug/2010  30/Jun/2010    1.0\n",
       "             03/Aug/2010  02/Nov/2010  30/Sep/2010    0.0\n",
       "             01/Nov/2011  NaN          30/Dec/2011    1.0\n",
       "0167866D US  06/May/2008  06/Aug/2008  30/Jun/2008    1.0\n",
       "             06/Aug/2008  06/Nov/2008  30/Sep/2008    0.0\n",
       "             06/Nov/2008  25/Feb/2009  31/Dec/2008    0.0\n",
       "             25/Feb/2009  06/May/2009  31/Mar/2009    1.0\n",
       "             06/May/2009  05/Aug/2009  30/Jun/2009    0.0\n",
       "             05/Aug/2009  05/Nov/2009  30/Sep/2009    0.0\n",
       "             05/Nov/2009  23/Feb/2010  31/Dec/2009    0.0\n",
       "             05/May/2010  03/Aug/2010  30/Jun/2010    1.0\n",
       "             22/Feb/2011  05/May/2011  31/Mar/2011    1.0\n",
       "             05/May/2011  03/Aug/2011  30/Jun/2011    1.0\n",
       "             03/Aug/2011  NaN          30/Sep/2011    0.0\n",
       "0202445Q US  02/Feb/2007  03/May/2007  30/Mar/2007    0.0\n",
       "             03/May/2007  01/Aug/2007  29/Jun/2007    0.0\n",
       "             01/Aug/2007  02/Nov/2007  28/Sep/2007    0.0\n",
       "             02/Nov/2007  01/Feb/2008  31/Dec/2007    1.0\n",
       "             01/Feb/2008  01/May/2008  31/Mar/2008    1.0\n",
       "             01/Aug/2008  31/Oct/2008  30/Sep/2008    1.0\n",
       "             31/Oct/2008  06/Feb/2009  31/Dec/2008    1.0\n",
       "             08/May/2009  07/Aug/2009  30/Jun/2009    0.0\n",
       "                                                     ... \n",
       "ZU US        06/Aug/2015  NaN          30/Sep/2015    1.0\n",
       "ZUMZ US      23/May/2007  22/Aug/2007  29/Jun/2007    0.0\n",
       "             22/Aug/2007  29/Nov/2007  28/Sep/2007    0.0\n",
       "             23/May/2008  12/Mar/2010  30/Jun/2008    0.0\n",
       "             12/Mar/2010  21/May/2010  31/Mar/2010    0.0\n",
       "             21/May/2010  01/Dec/2010  30/Jun/2010    0.0\n",
       "             01/Dec/2010  10/Mar/2011  31/Dec/2010    0.0\n",
       "             10/Mar/2011  19/May/2011  31/Mar/2011    1.0\n",
       "             19/May/2011  01/Sep/2011  30/Jun/2011    0.0\n",
       "             01/Sep/2011  02/Dec/2011  30/Sep/2011    1.0\n",
       "             02/Dec/2011  09/Mar/2012  30/Dec/2011    0.0\n",
       "             09/Mar/2012  18/May/2012  30/Mar/2012    0.0\n",
       "             18/May/2012  31/Aug/2012  29/Jun/2012    1.0\n",
       "             31/Aug/2012  30/Nov/2012  28/Sep/2012    0.0\n",
       "             30/Nov/2012  15/Mar/2013  31/Dec/2012    0.0\n",
       "             15/Mar/2013  24/May/2013  29/Mar/2013    1.0\n",
       "             24/May/2013  06/Sep/2013  28/Jun/2013    0.0\n",
       "             06/Sep/2013  06/Dec/2013  30/Sep/2013    0.0\n",
       "             06/Dec/2013  14/Mar/2014  31/Dec/2013    1.0\n",
       "             14/Mar/2014  23/May/2014  31/Mar/2014    0.0\n",
       "             23/May/2014  05/Sep/2014  30/Jun/2014    0.0\n",
       "             05/Sep/2014  05/Dec/2014  30/Sep/2014    0.0\n",
       "             05/Dec/2014  13/Mar/2015  31/Dec/2014    1.0\n",
       "             13/Mar/2015  05/Jun/2015  31/Mar/2015    0.0\n",
       "             05/Jun/2015  11/Sep/2015  30/Jun/2015    1.0\n",
       "             11/Sep/2015  11/Mar/2016  30/Sep/2015    1.0\n",
       "             11/Mar/2016  02/Dec/2016  31/Mar/2016    0.0\n",
       "             02/Dec/2016  10/Mar/2017  30/Dec/2016    0.0\n",
       "             10/Mar/2017  01/Dec/2017  31/Mar/2017    0.0\n",
       "             01/Dec/2017  15/Mar/2019  29/Dec/2017    0.0\n",
       "Name: target_return_positive, Length: 69714, dtype: float64"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_log_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train_positive, y_test_positive = train_test_split(x, y_log_positive, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial').fit(x_train, y_train_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6245898405981604"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(x_train, y_train_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_log=x_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_log[\"predict\"]=clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result_log[\"predict\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.659017\n",
      "         Iterations 5\n"
     ]
    }
   ],
   "source": [
    "model_log=sm.Logit(y_train_positive,x_train).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>                 <td>Logit</td>          <td>No. Iterations:</td>    <td>5.0000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td> <td>target_return_positive</td> <td>Pseudo R-squared:</td>    <td>0.004</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>           <td>2019-06-18 16:19</td>          <td>AIC:</td>        <td>73562.1187</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>           <td>55771</td>               <td>BIC:</td>        <td>73803.2020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>                <td>26</td>            <td>Log-Likelihood:</td>    <td>-36754.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>             <td>55744</td>             <td>LL-Null:</td>        <td>-36908.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>              <td>1.0000</td>              <td>Scale:</td>         <td>1.0000</td>  \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "               <td></td>                <th>Coef.</th>  <th>Std.Err.</th>     <th>z</th>     <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Market cap</th>                  <td>-0.0000</td>  <td>0.0000</td>   <td>-5.2682</td> <td>0.0000</td> <td>-0.0000</td> <td>-0.0000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Revision_real</th>               <td>0.1125</td>   <td>0.0656</td>   <td>1.7143</td>  <td>0.0865</td> <td>-0.0161</td> <td>0.2411</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mom</th>                         <td>-0.0864</td>  <td>0.0244</td>   <td>-3.5432</td> <td>0.0004</td> <td>-0.1343</td> <td>-0.0386</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>30d_vol</th>                     <td>5.1430</td>   <td>0.6812</td>   <td>7.5496</td>  <td>0.0000</td> <td>3.8078</td>  <td>6.4781</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pre_announcement_return</th>     <td>-0.1745</td>  <td>0.1369</td>   <td>-1.2742</td> <td>0.2026</td> <td>-0.4429</td> <td>0.0939</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>EAR</th>                         <td>-0.0508</td>  <td>0.1072</td>   <td>-0.4737</td> <td>0.6357</td> <td>-0.2608</td> <td>0.1593</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Broker</th>                      <td>-0.0018</td>  <td>0.0014</td>   <td>-1.2600</td> <td>0.2077</td> <td>-0.0046</td> <td>0.0010</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SI</th>                          <td>0.0033</td>   <td>0.0017</td>   <td>1.9528</td>  <td>0.0508</td> <td>-0.0000</td> <td>0.0066</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Utilities</th>                   <td>-0.6869</td>  <td>0.0559</td>  <td>-12.2908</td> <td>0.0000</td> <td>-0.7965</td> <td>-0.5774</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Oil & Gas</th>                   <td>-0.7155</td>  <td>0.0482</td>  <td>-14.8573</td> <td>0.0000</td> <td>-0.8099</td> <td>-0.6211</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Health Care</th>                 <td>-0.4627</td>  <td>0.0396</td>  <td>-11.6751</td> <td>0.0000</td> <td>-0.5404</td> <td>-0.3850</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Personal & Household Goods</th>  <td>-0.6151</td>  <td>0.0470</td>  <td>-13.0782</td> <td>0.0000</td> <td>-0.7072</td> <td>-0.5229</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Retail</th>                      <td>-0.5829</td>  <td>0.0447</td>  <td>-13.0457</td> <td>0.0000</td> <td>-0.6705</td> <td>-0.4954</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Technology</th>                  <td>-0.4604</td>  <td>0.0378</td>  <td>-12.1648</td> <td>0.0000</td> <td>-0.5346</td> <td>-0.3862</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Telecommunications</th>          <td>-0.5054</td>  <td>0.0877</td>   <td>-5.7621</td> <td>0.0000</td> <td>-0.6774</td> <td>-0.3335</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Financial Services</th>          <td>-0.7107</td>  <td>0.0512</td>  <td>-13.8861</td> <td>0.0000</td> <td>-0.8111</td> <td>-0.6104</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Travel & Leisure</th>            <td>-0.6037</td>  <td>0.0507</td>  <td>-11.9134</td> <td>0.0000</td> <td>-0.7030</td> <td>-0.5044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Industrial Goods & Services</th> <td>-0.6189</td>  <td>0.0341</td>  <td>-18.1624</td> <td>0.0000</td> <td>-0.6857</td> <td>-0.5521</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Banks</th>                       <td>-0.7334</td>  <td>0.0488</td>  <td>-15.0251</td> <td>0.0000</td> <td>-0.8291</td> <td>-0.6377</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Media</th>                       <td>-0.6649</td>  <td>0.0591</td>  <td>-11.2443</td> <td>0.0000</td> <td>-0.7808</td> <td>-0.5490</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Automobiles & Parts</th>         <td>-0.7313</td>  <td>0.0783</td>   <td>-9.3407</td> <td>0.0000</td> <td>-0.8848</td> <td>-0.5779</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Insurance</th>                   <td>-0.7465</td>  <td>0.0522</td>  <td>-14.3095</td> <td>0.0000</td> <td>-0.8487</td> <td>-0.6443</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Basic Resources</th>             <td>-0.6660</td>  <td>0.0654</td>  <td>-10.1861</td> <td>0.0000</td> <td>-0.7941</td> <td>-0.5378</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Real Estate</th>                 <td>-0.7311</td>  <td>0.1474</td>   <td>-4.9616</td> <td>0.0000</td> <td>-1.0200</td> <td>-0.4423</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Food & Beverage</th>             <td>-0.4606</td>  <td>0.0575</td>   <td>-8.0034</td> <td>0.0000</td> <td>-0.5734</td> <td>-0.3478</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Construction & Materials</th>    <td>-0.4590</td>  <td>0.0602</td>   <td>-7.6240</td> <td>0.0000</td> <td>-0.5771</td> <td>-0.3410</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Chemicals</th>                   <td>-0.5775</td>  <td>0.0627</td>   <td>-9.2110</td> <td>0.0000</td> <td>-0.7004</td> <td>-0.4547</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                               Results: Logit\n",
       "============================================================================\n",
       "Model:                 Logit                   No. Iterations:    5.0000    \n",
       "Dependent Variable:    target_return_positive  Pseudo R-squared:  0.004     \n",
       "Date:                  2019-06-18 16:19        AIC:               73562.1187\n",
       "No. Observations:      55771                   BIC:               73803.2020\n",
       "Df Model:              26                      Log-Likelihood:    -36754.   \n",
       "Df Residuals:          55744                   LL-Null:           -36908.   \n",
       "Converged:             1.0000                  Scale:             1.0000    \n",
       "----------------------------------------------------------------------------\n",
       "                             Coef.  Std.Err.    z     P>|z|   [0.025  0.975]\n",
       "----------------------------------------------------------------------------\n",
       "Market cap                  -0.0000   0.0000  -5.2682 0.0000 -0.0000 -0.0000\n",
       "Revision_real                0.1125   0.0656   1.7143 0.0865 -0.0161  0.2411\n",
       "mom                         -0.0864   0.0244  -3.5432 0.0004 -0.1343 -0.0386\n",
       "30d_vol                      5.1430   0.6812   7.5496 0.0000  3.8078  6.4781\n",
       "pre_announcement_return     -0.1745   0.1369  -1.2742 0.2026 -0.4429  0.0939\n",
       "EAR                         -0.0508   0.1072  -0.4737 0.6357 -0.2608  0.1593\n",
       "Broker                      -0.0018   0.0014  -1.2600 0.2077 -0.0046  0.0010\n",
       "SI                           0.0033   0.0017   1.9528 0.0508 -0.0000  0.0066\n",
       "Utilities                   -0.6869   0.0559 -12.2908 0.0000 -0.7965 -0.5774\n",
       "Oil & Gas                   -0.7155   0.0482 -14.8573 0.0000 -0.8099 -0.6211\n",
       "Health Care                 -0.4627   0.0396 -11.6751 0.0000 -0.5404 -0.3850\n",
       "Personal & Household Goods  -0.6151   0.0470 -13.0782 0.0000 -0.7072 -0.5229\n",
       "Retail                      -0.5829   0.0447 -13.0457 0.0000 -0.6705 -0.4954\n",
       "Technology                  -0.4604   0.0378 -12.1648 0.0000 -0.5346 -0.3862\n",
       "Telecommunications          -0.5054   0.0877  -5.7621 0.0000 -0.6774 -0.3335\n",
       "Financial Services          -0.7107   0.0512 -13.8861 0.0000 -0.8111 -0.6104\n",
       "Travel & Leisure            -0.6037   0.0507 -11.9134 0.0000 -0.7030 -0.5044\n",
       "Industrial Goods & Services -0.6189   0.0341 -18.1624 0.0000 -0.6857 -0.5521\n",
       "Banks                       -0.7334   0.0488 -15.0251 0.0000 -0.8291 -0.6377\n",
       "Media                       -0.6649   0.0591 -11.2443 0.0000 -0.7808 -0.5490\n",
       "Automobiles & Parts         -0.7313   0.0783  -9.3407 0.0000 -0.8848 -0.5779\n",
       "Insurance                   -0.7465   0.0522 -14.3095 0.0000 -0.8487 -0.6443\n",
       "Basic Resources             -0.6660   0.0654 -10.1861 0.0000 -0.7941 -0.5378\n",
       "Real Estate                 -0.7311   0.1474  -4.9616 0.0000 -1.0200 -0.4423\n",
       "Food & Beverage             -0.4606   0.0575  -8.0034 0.0000 -0.5734 -0.3478\n",
       "Construction & Materials    -0.4590   0.0602  -7.6240 0.0000 -0.5771 -0.3410\n",
       "Chemicals                   -0.5775   0.0627  -9.2110 0.0000 -0.7004 -0.4547\n",
       "============================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_log.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EAR_calc(ticker,date,return_df,EAR_period,vol_lookback):\n",
    "    '''\n",
    "    Calculate EAR from ticker and reference date\n",
    "    '''\n",
    "    if type(date)==pd.tslib.NaTType:\n",
    "        return None\n",
    "    elif type(date)==pd.tslib.Timestamp:\n",
    "        date=date.strftime(\"%d/%b/%Y\")\n",
    "    elif type(date)==str:\n",
    "        date=date\n",
    "    return_series=return_df.loc[ticker].dropna()\n",
    "    date_series=return_series.index.tolist()\n",
    "    if date in date_series:\n",
    "        day0=date_series.index(date)\n",
    "        post_series=return_series.iloc[day0:]\n",
    "        pre_series=return_series.iloc[:day0]\n",
    "        vol= return_series.iloc[day0-min(len(pre_series),vol_lookback+1):day0].std()\n",
    "        ret=(return_series.iloc[day0:day0+EAR_period]+1).prod()-1\n",
    "        nmove=ret/vol\n",
    "        return nmove\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_vol(signal_column,return_df,vol_lookback):\n",
    "    '''\n",
    "    Calculate simple vol from signal tuple\n",
    "    '''\n",
    "    signal_series=return_df.loc[signal_column.name[0]]\n",
    "    location=signal_series.index.tolist().index(signal_column.name[1])\n",
    "    vol_range=min(vol_lookback,len(signal_series[:location]))\n",
    "    signal_vol=signal_series[location-vol_range-1:location].std()\n",
    "    return signal_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_universe(signal_df,start_datetime,end_datetime,old_position):\n",
    "    '''\n",
    "    Slice the signal_df, both the index and entry date have to be \n",
    "    '''\n",
    "    \n",
    "\n",
    "    signal_df=signal_df.loc[start_datetime:end_datetime]\n",
    "    \n",
    "    if old_position is True:  \n",
    "        adj_signal_df=signal_df\n",
    "    else:\n",
    "        entry=signal_df.apply(lambda x:datetime.strptime(x.name[1],\"%d/%b/%Y\"),axis=0)\n",
    "        period_evaluate=(entry>=start_datetime)&(entry<=end_datetime)\n",
    "        adj_signal_df=signal_df.loc[:,period_evaluate]\n",
    "    \n",
    "    \n",
    "    zero_index=pd.Series(1,index=pd.date_range(start_datetime,end_datetime,freq='B')).to_frame()\n",
    "    adj_signal_df=pd.concat([adj_signal_df.drop_duplicates(),zero_index],axis=1).iloc[:,:-1]\n",
    "    return adj_signal_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_filter_stop(signal_df,stop_level,return_df,vol_lookback,stop_type,index_df):\n",
    "    '''\n",
    "    Input - signal_df\n",
    "    Get the updated signal df after the stop loss\n",
    "    stop_type:abs,rel\n",
    "    \n",
    "    '''\n",
    "    if stop_type=='abs':\n",
    "        vol_row=signal_df.apply(lambda column:signal_vol(column,return_df,vol_lookback),axis=0)\n",
    "        signal_cum_nmove=((1+signal_df).cumprod()-1).ffill()/vol_row\n",
    "        signal_df_stop=signal_df[-(signal_cum_nmove.expanding().min().shift(1,axis=0)<-stop_level)]\n",
    "    elif stop_type=='rel':\n",
    "        if index_df.shape[1]==1:\n",
    "            signal_count=signal_df.copy()\n",
    "            signal_count[((signal_count)>0) | ((signal_count)<0)]=1.0\n",
    "            signal_hedge=signal_count.apply(lambda x:x.multiply(index_df.iloc[:,0],axis=0))\n",
    "            \n",
    "            vol_row=signal_df.apply(lambda x:signal_vol(x,return_df,vol_lookback),axis=0)\n",
    "            rel_signal_cum_nmove=((1+signal_df).cumprod()-(1+signal_hedge).cumprod()).ffill()/vol_row\n",
    "            signal_df_stop=signal_df[-(rel_signal_cum_nmove.expanding().min().shift(1,axis=0)<-stop_level)]\n",
    "        else:\n",
    "            signal_count=signal_df.copy()\n",
    "            signal_count[((signal_count)>0) | ((signal_count)<0)]=1.0\n",
    "            signal_hedge=signal_count.apply(lambda x:x.multiply(index_df[Asia_mapping.loc[x.name[0][-2:]].iloc[0]],axis=0))\n",
    "            \n",
    "            vol_row=signal_df.apply(lambda x:signal_vol(x,return_df,vol_lookback),axis=0)\n",
    "            rel_signal_cum_nmove=((1+signal_df).cumprod()-(1+signal_hedge).cumprod()).ffill()/vol_row\n",
    "            signal_df_stop=signal_df[-(rel_signal_cum_nmove.expanding().min().shift(1,axis=0)<-stop_level)]            \n",
    "            \n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    return signal_df_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revision_adjusted_size(reference_signal_df,lower_revision,higher_revision,size_multiple,revision_row,revision_row_reference,gross,long):\n",
    "    \n",
    "    ''' \n",
    "    Use positive size\n",
    "    '''\n",
    "    \n",
    "    lower_size=0.01\n",
    "    higher_size=lower_size*size_multiple\n",
    "    \n",
    "    if long is True:\n",
    "        \n",
    "        size_row_reference=revision_row_reference.to_frame().copy().apply(lambda x: lower_size+(higher_size-lower_size)\\\n",
    "                                                      *(np.abs(x.iloc[0]-lower_revision))/np.abs(higher_revision-lower_revision) \\\n",
    "                                                      if np.abs(x.iloc[0])<=np.abs(higher_revision) else higher_size,axis=1)\n",
    "\n",
    "        size_df_reference=(1+reference_signal_df).cumprod()*size_row_reference\n",
    "\n",
    "        trial_gross=np.abs(size_df_reference.sum(axis=1).mean())\n",
    "        new_lower_size=lower_size/(trial_gross*100/gross)\n",
    "        new_higher_size=higher_size/(trial_gross*100/gross)\n",
    "\n",
    "        size_row=revision_row.to_frame().copy().apply(lambda x: new_lower_size+(new_higher_size-new_lower_size)\\\n",
    "                                                      *(np.abs(x.iloc[0]-lower_revision))/np.abs(higher_revision-lower_revision) \\\n",
    "                                                      if np.abs(x.iloc[0])<=np.abs(higher_revision) else new_higher_size,axis=1)\n",
    "    else:\n",
    "        size_row_reference=revision_row_reference.to_frame().copy().apply(lambda x: lower_size+(higher_size-lower_size)\\\n",
    "                                                      *(np.abs(x.iloc[0]-lower_revision))/np.abs(higher_revision-lower_revision) \\\n",
    "                                                      if np.abs(x.iloc[0])<=np.abs(lower_revision) else higher_size,axis=1)\n",
    "\n",
    "        size_df_reference=(1-reference_signal_df).cumprod()*size_row_reference\n",
    "\n",
    "        trial_gross=np.abs(size_df_reference.sum(axis=1).mean())\n",
    "        new_lower_size=lower_size/(trial_gross*100/gross)\n",
    "        new_higher_size=higher_size/(trial_gross*100/gross)\n",
    "\n",
    "        size_row=revision_row.to_frame().copy().apply(lambda x: new_lower_size+(new_higher_size-new_lower_size)\\\n",
    "                                                      *(np.abs(x.iloc[0]-lower_revision))/np.abs(higher_revision-lower_revision) \\\n",
    "                                                      if np.abs(x.iloc[0])<=np.abs(lower_revision) else new_higher_size,axis=1)\n",
    "\n",
    "    return size_row, new_lower_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sizing(signal_df,reference_signal_df,gross,fundamental_df,new_signal,return_df,risk_parity,liquidity,capital,\\\n",
    "           revision_adjust,long):\n",
    "    '''\n",
    "    Use historical signal_df range to calculate the size row for the current signal_df range\n",
    "    Idea is to use historical as a benchmark for future sizing\n",
    "    '''\n",
    "    \n",
    "    fundamental_df=fundamental_df.copy().sort_index()\n",
    "    vol_reference=reference_signal_df.apply(lambda x:signal_vol(x,return_df,30),axis=0).mean()\n",
    "    vol_row=signal_df.apply(lambda x:signal_vol(x,return_df,30),axis=0)\n",
    "    \n",
    "    '''\n",
    "    Revision row needs to be updated using reference_signal\n",
    "    new sizing scheme is a linear function\n",
    "    revision_adjust=(True/False,lower_revision,higher_revision,size_multiple)\n",
    "    '''\n",
    "    \n",
    "    if revision_adjust[0] is True:\n",
    "        \n",
    "        if new_signal is True:     \n",
    "            revision_row_reference=reference_signal_df.apply(lambda x:fundamental_df.loc[x.name,\"Revision_real\"],axis=0)\n",
    "            revision_row=signal_df.apply(lambda x:fundamental_df.loc[x.name,\"Revision_real\"],axis=0)\n",
    "        else:\n",
    "            revision_row_reference=reference_signal_df.apply(lambda x:fundamental_df.loc[x.name,\"Revision_20\"],axis=0)\n",
    "            revision_row=signal_df.apply(lambda x:fundamental_df.loc[x.name,\"Revision_20\"],axis=0)            \n",
    "        \n",
    "        if long is True and revision_adjust[1] is not None:\n",
    "            lower_revision=revision_adjust[1][0]\n",
    "            higher_revision=revision_adjust[1][1]\n",
    "            size_multiple=revision_adjust[3]\n",
    "        \n",
    "            base_size,low_size=revision_adjusted_size(reference_signal_df,lower_revision,higher_revision,size_multiple,\\\n",
    "                                                      revision_row,revision_row_reference,gross,True)\n",
    "            if risk_parity is True:\n",
    "                size_row=signal_df.apply(lambda x: min(base_size[x.name]/(vol_row[x.name]/vol_reference),\\\n",
    "                                                       fundamental_df.loc[x.name[0],x.name[1]][\"Volume\"].iloc[0]*\\\n",
    "                                                       liquidity/capital),axis=0)\n",
    "            else:\n",
    "                size_row=signal_df.apply(lambda x: min(base_size[x.name], fundamental_df.loc[x.name[0],x.name[1]][\"Volume\"].iloc[0]\\\n",
    "                                                       *liquidity/capital),axis=0)            \n",
    "\n",
    "        elif long is False and revision_adjust[2] is not None:\n",
    "            lower_revision=revision_adjust[2][0]\n",
    "            higher_revision=revision_adjust[2][1]\n",
    "            size_multiple=revision_adjust[3]\n",
    "        \n",
    "            base_size,low_size=revision_adjusted_size(reference_signal_df,lower_revision,higher_revision,size_multiple,revision_row,\\\n",
    "                                             revision_row_reference,gross,False) \n",
    "\n",
    "            if risk_parity is True:\n",
    "                size_row=signal_df.apply(lambda x: min(base_size[x.name]/(vol_row[x.name]/vol_reference),\\\n",
    "                                                       fundamental_df.loc[x.name[0],x.name[1]][\"Volume\"].iloc[0]*\\\n",
    "                                                       liquidity/capital),axis=0)\n",
    "            else:\n",
    "                size_row=signal_df.apply(lambda x: min(base_size[x.name], fundamental_df.loc[x.name[0],x.name[1]][\"Volume\"].iloc[0]\\\n",
    "                                                       *liquidity/capital),axis=0)\n",
    "\n",
    "        else:\n",
    "            size_row=None\n",
    "            low_size=None\n",
    "        \n",
    "\n",
    "    else:\n",
    "        number=reference_signal_df.count(axis=1).mean()\n",
    "        avg_size=gross/100/number\n",
    "\n",
    "        if risk_parity is True:\n",
    "            size_row=signal_df.apply(lambda x: min(avg_size/(vol_row[x.name]/vol_reference),\\\n",
    "                                                   fundamental_df.loc[x.name[0],x.name[1]][\"Volume\"].iloc[0]*\\\n",
    "                                                   liquidity/capital),axis=0)\n",
    "        else:\n",
    "            size_row=signal_df.apply(lambda x: min(avg_size, fundamental_df.loc[x.name[0],x.name[1]][\"Volume\"].iloc[0]\\\n",
    "                                                   *liquidity/capital),axis=0)\n",
    "        low_size=None\n",
    "    return size_row,low_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trading_analytics_date(portfolio_cache):\n",
    "    '''\n",
    "    Key portfolio metrics from portfolio cache\n",
    "    Feed into plot function\n",
    "    '''\n",
    "    \n",
    "    ind_return=portfolio_cache[3]\n",
    "    signal_count=len(ind_return)\n",
    "    account_curve=portfolio_cache[1]\n",
    "    \n",
    "    if signal_count==0:\n",
    "        return None,None,None,None,None,None,None\n",
    "    else:\n",
    "        mean_return=ind_return.mean()\n",
    "        hit_rate=len(ind_return[ind_return>0])/len(ind_return)*1.0\n",
    "        payoff_ratio=ind_return[ind_return>0].mean()/ind_return[ind_return<0].mean()*-1.0\n",
    "        \n",
    "        account_price=account_curve+1\n",
    "        ann_vol=np.std(account_price.diff()/account_price.shift(1))*(260**0.5)\n",
    "        ann_ret=(account_price.iloc[-1]**(1/len(account_price)))**260-1\n",
    "        ann_sharpe=ann_ret/ann_vol\n",
    "        \n",
    "        max_dd=-((1+account_curve)-(1+account_curve).cummax(axis=0)).expanding().min().min()\n",
    "        \n",
    "        #low_date=(np.maximum.accumulate(account_curve)-account_curve).idxmax()\n",
    "        #high_date=account_curve[:low_date].idxmax()\n",
    "        #max_dd=1-(1+account_curve[low_date])/(1+account_curve[high_date])\n",
    "        \n",
    "        return signal_count,hit_rate,payoff_ratio,ann_ret,ann_vol,ann_sharpe,max_dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trading_analytics_simp(account_curve):\n",
    "    '''\n",
    "    Key portfolio metrics from portfolio account curve\n",
    "    Only sharpe and drawdown\n",
    "    '''\n",
    "\n",
    "\n",
    "    account_price=account_curve+1\n",
    "    ann_vol=np.std(account_price.diff()/account_price.shift(1))*(260**0.5)\n",
    "    ann_ret=(account_price.iloc[-1]**(1/len(account_price)))**260-1\n",
    "    ann_sharpe=ann_ret/ann_vol\n",
    "\n",
    "    max_dd=-((1+account_curve)-(1+account_curve).cummax(axis=0)).expanding().min().min()\n",
    "\n",
    "    #low_date=(np.maximum.accumulate(account_curve)-account_curve).idxmax()\n",
    "    #high_date=account_curve[:low_date].idxmax()\n",
    "    #max_dd=1-(1+account_curve[low_date])/(1+account_curve[high_date])\n",
    "\n",
    "    return ann_sharpe,max_dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_signal(title,figsize,portfolio_cache):\n",
    "\n",
    "    account_curve=portfolio_cache[1]\n",
    "    avg_size=np.abs(portfolio_cache[2]).mean(axis=0).mean()\n",
    "    ind_return=portfolio_cache[3]\n",
    "    gross=portfolio_cache[4]\n",
    "    turnover=portfolio_cache[5]\n",
    "\n",
    "    fig=plt.figure(figsize=figsize)\n",
    "    ax1=fig.add_subplot(1,1,1)\n",
    "    ln1=ax1.plot(account_curve,label='signal',color='b')\n",
    "\n",
    "    val1=ax1.get_yticks()\n",
    "    start=val1[0]\n",
    "    end=val1[-1]\n",
    "    ax1.set_yticks(np.arange(start,end,0.01))  \n",
    "    adj_val1=ax1.get_yticks()\n",
    "    ax1.set_yticklabels([\"{:.0%}\".format(x) for x in adj_val1])\n",
    "\n",
    "    ax2=ax1.twinx()\n",
    "    ln2=ax2.plot(gross,label='gross',color='silver')\n",
    "\n",
    "    val2=ax2.get_yticks()\n",
    "    start=val2[0]\n",
    "    end=val2[-1]\n",
    "    ax2.set_yticks(np.arange(start,end,0.1))  \n",
    "    adj_val2=ax2.get_yticks()\n",
    "    ax2.set_yticklabels([\"{:.0%}\".format(x) for x in adj_val2])\n",
    "\n",
    "    count,hit,payoff,ret,vol,sharpe,max_dd=trading_analytics_date(portfolio_cache)\n",
    "\n",
    "    plt.title(\"\\n\".join(wrap('count='+str(count)+\n",
    "                             ',avg_size='+str(\"{:.1%}\".format(avg_size))+\n",
    "                             ',hit_rate='+str(\"{:.0%}\".format(hit))+\n",
    "                             ',payoff='+str(round(payoff,1))+\n",
    "                             ',return='+str(\"{:.1%}\".format(ret))+\n",
    "                             ',vol='+str(\"{:.1%}\".format(vol))+\n",
    "                             ',sharpe='+str(round(sharpe,1))+\n",
    "                             ',turnover='+str(round(turnover,1))+'x'+                             \n",
    "                             ',max_drawdown='+str(\"{:.1%}\".format(max_dd)))),fontsize=10)\n",
    "\n",
    "    ax1.set_xlabel('Year')\n",
    "    ax1.set_ylabel('Return')\n",
    "    ax2.set_ylabel('Exposure')\n",
    "    plt.suptitle(title,y=1.05,fontsize=16)\n",
    "    plt.grid(linestyle='dashed')\n",
    "    plt.legend(ln1+ln2,[l.get_label() for l in ln1+ln2],loc=2)\n",
    "    ax1.axhline(y=0,color='k')\n",
    "\n",
    "    plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Signal class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class signal(object):\n",
    "    '''\n",
    "    Signal class is built to initialize the signal_df and account curve from base parameters \n",
    "    '''\n",
    "    def __init__(self,fundamental_df,price_df,EAR_period,entry,long_criteria,short_criteria,holding,start,end,old_position\\\n",
    "                 ,new_signal,revision_adjust,early_exit):\n",
    "        '''\n",
    "        Define the key free parameters of the signal\n",
    "        Criteria:(EAR,revision,revision_norm,size)\n",
    "        revision_adjust:(True/False,(long lower revision,long higher revision),(short low abs revision,short high abs_revision),size_multiple)\n",
    "        '''\n",
    "        self.fundamental_df=fundamental_df\n",
    "        self.price_df=price_df\n",
    "        self.abs_return=price_df.diff(1,axis=1)/price_df.shift(1,axis=1)\n",
    "        self.EAR_period=EAR_period\n",
    "        self.entry=entry\n",
    "        self.long_criteria=long_criteria\n",
    "        self.short_criteria=short_criteria\n",
    "        self.holding=holding\n",
    "        self.start=start\n",
    "        self.end=end\n",
    "        self.old_position=old_position\n",
    "        self.new_signal=new_signal\n",
    "        self.revision_adjust=revision_adjust #(True/False,lower_revision,higher_revision in absolute terms,size_multiple)\n",
    "        self.early_exit=early_exit\n",
    "        \n",
    "    def signal_base(self):\n",
    "        '''\n",
    "        Filter the signal criteria like EAR and revision, get the target signal list\n",
    "        From the fundamental information\n",
    "        For both long and short side\n",
    "        '''\n",
    "        \n",
    "        long_base=self.fundamental_df.copy()\n",
    "        short_base=self.fundamental_df.copy()\n",
    "        \n",
    "\n",
    "        long_base[\"EAR\"]=long_base.apply(lambda x:EAR_calc(x.name[0],x.name[1],self.abs_return,self.EAR_period,30)\\\n",
    "                                     if x.name[0] in self.abs_return.index else None,axis=1)\n",
    "\n",
    "        short_base[\"EAR\"]=short_base.apply(lambda x:EAR_calc(x.name[0],x.name[1],self.abs_return,self.EAR_period,30)\\\n",
    "                                     if x.name[0] in self.abs_return.index else None,axis=1)\n",
    "\n",
    "\n",
    "        if self.long_criteria is None:\n",
    "            long_base=None\n",
    "        else:\n",
    "            \n",
    "            if self.long_criteria[1] is None:\n",
    "                pass\n",
    "            else:\n",
    "                if self.new_signal is True:\n",
    "                    long_base=long_base[(long_base[\"Revision_real\"]>self.long_criteria[1][0])\\\n",
    "                                                    &(long_base[\"Revision_real\"]<self.long_criteria[1][1])]\n",
    "                else:\n",
    "                    long_base=long_base[(long_base[\"Revision_20\"]>self.long_criteria[1][0])\\\n",
    "                                                    &(long_base[\"Revision_20\"]<self.long_criteria[1][1])]                    \n",
    "\n",
    "            if self.long_criteria[2] is None:\n",
    "                pass\n",
    "            else:\n",
    "                long_base=long_base[(long_base[\"Revision_norm\"]>self.long_criteria[2][0])\\\n",
    "                                                    &(long_base[\"Revision_norm\"]<self.long_criteria[2][1])]\n",
    "                \n",
    "            if self.long_criteria[3] is None:\n",
    "                pass\n",
    "            else:\n",
    "                long_base=long_base[(long_base[\"Market cap\"]>self.long_criteria[3][0])&\\\n",
    "                                        (long_base[\"Market cap\"]<self.long_criteria[3][1])]\n",
    "            \n",
    "            if self.long_criteria[0] is None:\n",
    "                pass\n",
    "            else:\n",
    "                long_base=long_base[(long_base[\"EAR\"]>self.long_criteria[0][0])&(long_base[\"EAR\"]<self.long_criteria[0][1])]\n",
    "                \n",
    "                \n",
    "        if self.short_criteria is None:\n",
    "            short_base=None\n",
    "        else:\n",
    "            \n",
    "            if self.short_criteria[1] is None:\n",
    "                pass\n",
    "            else:\n",
    "                if self.new_signal is True:\n",
    "                    short_base=short_base[(short_base[\"Revision_real\"]>self.short_criteria[1][0])\\\n",
    "                                                    &(short_base[\"Revision_real\"]<self.short_criteria[1][1])]\n",
    "                else:\n",
    "                    short_base=short_base[(short_base[\"Revision_20\"]>self.short_criteria[1][0])\\\n",
    "                                                    &(short_base[\"Revision_20\"]<self.short_criteria[1][1])]                    \n",
    "\n",
    "            if self.short_criteria[2] is None:\n",
    "                pass\n",
    "            else:\n",
    "                short_base=short_base[(short_base[\"Revision_norm\"]>self.short_criteria[2][0])\\\n",
    "                                                    &(short_base[\"Revision_norm\"]<self.short_criteria[2][1])]\n",
    "                \n",
    "            if self.short_criteria[3] is None:\n",
    "                pass\n",
    "            else:\n",
    "                short_base=short_base[(short_base[\"Market cap\"]>self.short_criteria[3][0])&\\\n",
    "                                        (short_base[\"Market cap\"]<self.short_criteria[3][1])]\n",
    "            \n",
    "            if self.short_criteria[0] is None:\n",
    "                pass\n",
    "            else:\n",
    "                short_base=short_base[(short_base[\"EAR\"]>self.short_criteria[0][0])&\\\n",
    "                                      (short_base[\"EAR\"]<self.short_criteria[0][1])]\n",
    "        \n",
    "        self.long_base=long_base\n",
    "        self.short_base=short_base\n",
    "        \n",
    "        return long_base,short_base\n",
    "    \n",
    "    def signal_df_date(self):#if we hold them through next earning\n",
    "        '''\n",
    "        Obtain the signal_df function over the whole time period from the target signal list\n",
    "        '''\n",
    "        try:\n",
    "            long_base=self.long_base.copy()\n",
    "            short_base=self.short_base.copy()\n",
    "        except:\n",
    "            long_base,short_base=signal.signal_base(self)\n",
    "        \n",
    "        if long_base is None:\n",
    "            long_df=None\n",
    "        \n",
    "        else:\n",
    "            long_df=pd.DataFrame(index=self.price_df.columns)\n",
    "\n",
    "            for s in long_base.index:\n",
    "                return_series=self.abs_return.loc[s[0]]\n",
    "                if s[1] in return_series.index:\n",
    "                    if not np.isnan(return_series.loc[s[1]]): \n",
    "                        day0=return_series.index.tolist().index(s[1])\n",
    "                        \n",
    "                        if self.early_exit is True and datetime.strptime(s[1],\"%d/%b/%Y\").date()>=\\\n",
    "                        datetime.strptime(s[3],\"%d/%b/%Y\").date():\n",
    "                            period=None\n",
    "                        \n",
    "                        elif self.early_exit is True and type(s[2])!=float and datetime.strptime(s[1],\"%d/%b/%Y\").date()<\\\n",
    "                        datetime.strptime(s[3],\"%d/%b/%Y\").date():\n",
    "                            period=min(self.holding,np.busday_count(datetime.strptime(s[1],\"%d/%b/%Y\").date(),\\\n",
    "                                                                    datetime.strptime(s[2],\"%d/%b/%Y\").date())-self.entry,\n",
    "                                      np.busday_count(datetime.strptime(s[1],\"%d/%b/%Y\").date(),\\\n",
    "                                                                    datetime.strptime(s[3],\"%d/%b/%Y\").date())-self.entry+1)                            \n",
    "                        \n",
    "                        elif self.early_exit is True and type(s[2])==float and datetime.strptime(s[1],\"%d/%b/%Y\").date()<\\\n",
    "                        datetime.strptime(s[3],\"%d/%b/%Y\").date():\n",
    "                            period=min(self.holding,np.busday_count(datetime.strptime(s[1],\"%d/%b/%Y\").date(),\\\n",
    "                                                                    datetime.strptime(s[3],\"%d/%b/%Y\").date())-self.entry+1)\n",
    "                            \n",
    "                        elif type(s[2])==float:##basically np.nan has type float\n",
    "                            period=self.holding+1\n",
    "                        \n",
    "                        else: ##assume that we are not holding through numbers\n",
    "                            period=min(self.holding,np.busday_count(datetime.strptime(s[1],\"%d/%b/%Y\").date(),\\\n",
    "                                                                    datetime.strptime(s[2],\"%d/%b/%Y\").date())-self.entry)\n",
    "                            \n",
    "                        if period is None:\n",
    "                            pass\n",
    "                        else:\n",
    "                            target_series=return_series.iloc[day0+self.entry-1:day0+min(period+self.entry, \\\n",
    "                                                                                           len(return_series[day0:]))].dropna()\n",
    "                            \n",
    "                            if len(target_series)==0:\n",
    "                                pass\n",
    "                            else:\n",
    "                                target_series.iloc[0]=0.0\n",
    "                                long_df[s]=target_series                        \n",
    "\n",
    "                    else:\n",
    "                        pass\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "            long_df=long_df.reindex(datetime.strptime(i,\"%d/%b/%Y\") for i in long_df.index)\n",
    "            long_df=long_df.sort_index()\n",
    "\n",
    "            if self.start is not None:\n",
    "                long_df=slice_universe(long_df,self.start,self.end,self.old_position)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            long_df=long_df.dropna(how=\"all\",axis=1)\n",
    "            #long_df.columns=pd.MultiIndex.from_tuples(pd.Series(list(long_df.columns)))\n",
    "            self.long_df=long_df\n",
    "            \n",
    "            \n",
    "        \n",
    "        if short_base is None:\n",
    "            short_df=None\n",
    "        \n",
    "        else:\n",
    "            short_df=pd.DataFrame(index=self.price_df.columns)\n",
    "\n",
    "            for s in short_base.index:\n",
    "                return_series=self.abs_return.loc[s[0]]\n",
    "                if s[1] in return_series.index:\n",
    "                    if not np.isnan(return_series.loc[s[1]]): \n",
    "                        day0=return_series.index.tolist().index(s[1])\n",
    "                        \n",
    "                        if self.early_exit is True and datetime.strptime(s[1],\"%d/%b/%Y\").date()>=\\\n",
    "                        datetime.strptime(s[3],\"%d/%b/%Y\").date():\n",
    "                            period=None\n",
    "                            \n",
    "                        elif self.early_exit is True and type(s[2])!=float and datetime.strptime(s[1],\"%d/%b/%Y\").date()<\\\n",
    "                        datetime.strptime(s[3],\"%d/%b/%Y\").date():\n",
    "                            period=min(self.holding,np.busday_count(datetime.strptime(s[1],\"%d/%b/%Y\").date(),\\\n",
    "                                                                    datetime.strptime(s[2],\"%d/%b/%Y\").date())-self.entry,\n",
    "                                      np.busday_count(datetime.strptime(s[1],\"%d/%b/%Y\").date(),\\\n",
    "                                                                    datetime.strptime(s[3],\"%d/%b/%Y\").date())-self.entry+1)  \n",
    "                            \n",
    "                        elif self.early_exit is True and type(s[2])==float and datetime.strptime(s[1],\"%d/%b/%Y\").date()<\\\n",
    "                        datetime.strptime(s[3],\"%d/%b/%Y\").date():\n",
    "                            period=min(self.holding,np.busday_count(datetime.strptime(s[1],\"%d/%b/%Y\").date(),\\\n",
    "                                                                    datetime.strptime(s[3],\"%d/%b/%Y\").date())-self.entry+1)\n",
    "                        elif type(s[2])==float:##basically np.nan has type float\n",
    "                            period=self.holding\n",
    "                    \n",
    "                        else: ##assume that we are not holding through numbers\n",
    "                            period=min(self.holding,np.busday_count(datetime.strptime(s[1],\"%d/%b/%Y\").date(),\\\n",
    "                                                                    datetime.strptime(s[2],\"%d/%b/%Y\").date())-self.entry)\\\n",
    "\n",
    "\n",
    "                        if period is None:\n",
    "                            pass\n",
    "                        else:\n",
    "                            target_series=return_series.iloc[day0+self.entry-1:day0+min(period+self.entry, \\\n",
    "                                                                                           len(return_series[day0:]))].dropna()\n",
    "                            \n",
    "                            if len(target_series)==0:\n",
    "                                pass\n",
    "                            else:\n",
    "                                target_series.iloc[0]=0.0\n",
    "                                short_df[s]=target_series         \n",
    "                            \n",
    "                    else:\n",
    "                        pass\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "            short_df=short_df.reindex(datetime.strptime(i,\"%d/%b/%Y\") for i in short_df.index)\n",
    "            short_df=short_df.sort_index()\n",
    "\n",
    "            if self.start is not None:\n",
    "                short_df=slice_universe(short_df,self.start,self.end,self.old_position)\n",
    "            else:\n",
    "                pass  \n",
    "            \n",
    "            short_df=short_df.dropna(how=\"all\",axis=1)\n",
    "            #short_df.columns=pd.MultiIndex.from_tuples(pd.Series(list(short_df.columns)))\n",
    "            self.short_df=short_df\n",
    "        \n",
    "        return long_df,short_df\n",
    "\n",
    "    def signal_account(self,stop,gross,index_df,net_level,risk_parity,liquidity,capital):\n",
    "        '''\n",
    "        Build the account curve with signal_df\n",
    "        Assume quarterly rebalancing that's why the period list has quarter as the key\n",
    "        Take extra care when building the account curve, the logic is: work out the size_df, then shift by 1 and * signal_df\n",
    "        Stop=(long_stop,short_stop,type)\n",
    "        index_df has to be a dataframe with a name\n",
    "        '''\n",
    "        \n",
    "        try:\n",
    "            long_df=self.long_df.copy()\n",
    "            short_df=self.short_df.copy()\n",
    "            \n",
    "        except:\n",
    "            long_df,short_df=self.signal_df_date()\n",
    "                   \n",
    "        '''\n",
    "        Assign values for later use\n",
    "        '''\n",
    "        \n",
    "        self.capital=capital\n",
    "        \n",
    "        self.index_df=index_df\n",
    "        self.index_df.index=[datetime.strptime(i,\"%d/%b/%Y\") for i in self.index_df.index]\n",
    "       \n",
    "        \n",
    "        '''\n",
    "        Define rebalance period first\n",
    "        '''\n",
    "        if long_df is None:\n",
    "            period_quarter=short_df.apply(lambda x:str(x.name.year)+\" \"+str(x.name.quarter),axis=1)\n",
    "            period_list=list(set(period_quarter))\n",
    "            period_list.sort()    \n",
    "            \n",
    "        else:\n",
    "            period_quarter=long_df.apply(lambda x:str(x.name.year)+\" \"+str(x.name.quarter),axis=1)\n",
    "            period_list=list(set(period_quarter))\n",
    "            period_list.sort()    \n",
    "                \n",
    "        '''\n",
    "        Separate out long and short\n",
    "        '''\n",
    "        if long_df is None:\n",
    "            long_cache=(None,None,None,None)\n",
    "        else:\n",
    "            if stop is None:\n",
    "                pass\n",
    "            else:\n",
    "                long_df=signal_filter_stop(long_df,stop[0],self.abs_return,30,stop[2],self.index_df)   \n",
    "                self.long_df=long_df\n",
    "\n",
    "            long_sub_signal={}\n",
    "            long_sub_size_row={}\n",
    "            long_sub_size_df={}\n",
    "            long_sub_pnl={}\n",
    "        \n",
    "            for s in period_list:\n",
    "            \n",
    "                long_sub_signal[s]=long_df[period==s].dropna(how='all',axis=1)\n",
    "                \n",
    "                if long_sub_signal[s].shape[1]==0:\n",
    "                    long_sub_size_df[s]=long_sub_signal[s]\n",
    "                    long_sub_pnl[s]=long_sub_signal[s]\n",
    "                    \n",
    "                else:\n",
    "                    if period_list.index(s)<4:##use last quarter's sizing as reference\n",
    "                        long_sub_size_row[s]=sizing(long_sub_signal[s],long_sub_signal[s],gross[0],self.fundamental_df,\\\n",
    "                                                       self.new_signal,self.abs_return,risk_parity,liquidity,capital,\\\n",
    "                                                    self.revision_adjust,True)[0]\n",
    "                    else:\n",
    "                        try:\n",
    "                            long_sub_size_row[s]=sizing(long_sub_signal[s],long_sub_signal[period_list[period_list.index(s)-1]],\\\n",
    "                                                       gross[0],self.fundamental_df,self.new_signal,self.abs_return,\\\n",
    "                                                        risk_parity,liquidity,capital,self.revision_adjust,True)[0]\n",
    "                        except:\n",
    "                            long_sub_size_row[s]=sizing(long_sub_signal[s],long_sub_signal[s],\\\n",
    "                                                       gross[0],self.fundamental_df,self.new_signal,self.abs_return,\\\n",
    "                                                        risk_parity,liquidity,capital,self.revision_adjust,True)[0] \n",
    "\n",
    "                    long_sub_size_df[s]=(1+long_sub_signal[s]).cumprod()*long_sub_size_row[s]\n",
    "                    long_sub_pnl[s]=(long_sub_size_df[s].shift(1))*long_sub_signal[s] \n",
    "                    # need to shift by 1 as the size is end of the day\n",
    "        \n",
    "            long_daily_pnl=pd.concat(list(long_sub_pnl.values()),axis=0)\n",
    "            long_acct_curve=long_daily_pnl.cumsum().ffill().sum(axis=1)\n",
    "            long_size_df=pd.concat(list(long_sub_size_df.values()),axis=0)\n",
    "            long_ind_return=long_daily_pnl.cumsum().ffill().iloc[-1].dropna()\n",
    "            long_cache=(long_daily_pnl,long_acct_curve,long_size_df,long_ind_return)\n",
    "            \n",
    "            self.long_cache=long_cache\n",
    "\n",
    "            \n",
    "        if short_df is None:\n",
    "            short_cache=(None,None,None,None)\n",
    "        else:\n",
    "            if stop is None:\n",
    "                pass\n",
    "            else:\n",
    "                short_df=-signal_filter_stop(-short_df,stop[1],self.abs_return,30,stop[2],self.index_df)   \n",
    "                self.short_df=short_df\n",
    "\n",
    "            short_sub_signal={}\n",
    "            short_sub_size_row={}\n",
    "            short_sub_size_df={}\n",
    "            short_sub_pnl={}\n",
    "        \n",
    "            for s in period_list:\n",
    "                short_sub_signal[s]=short_df[period==s].dropna(how='all',axis=1)\n",
    "\n",
    "                if short_sub_signal[s].shape[1]==0:\n",
    "                    short_sub_size_df[s]=short_sub_signal[s]\n",
    "                    short_sub_pnl[s]=short_sub_signal[s]\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    if period_list.index(s)<4:##use last quarter's sizing as reference\n",
    "                        short_sub_size_row[s]=-sizing(short_sub_signal[s],short_sub_signal[s],gross[1],self.fundamental_df,\\\n",
    "                                                       self.new_signal,self.abs_return,risk_parity,liquidity,capital,\\\n",
    "                                                      self.revision_adjust,False)[0]\n",
    "                    else:\n",
    "                        try:\n",
    "                            short_sub_size_row[s]=-sizing(short_sub_signal[s],short_sub_signal\\\n",
    "                                                          [period_list[period_list.index(s)-1]],\\\n",
    "                                                           gross[1],self.fundamental_df,self.new_signal,self.abs_return,\\\n",
    "                                                          risk_parity,liquidity,\\\n",
    "                                                          capital,self.revision_adjust,False)[0]\n",
    "                        except:\n",
    "                            short_sub_size_row[s]=-sizing(short_sub_signal[s],short_sub_signal[s],\\\n",
    "                                                           gross[1],self.fundamental_df,self.new_signal,self.abs_return,\\\n",
    "                                                          risk_parity,liquidity,\\\n",
    "                                                          capital,self.revision_adjust,False)[0]\n",
    "\n",
    "                    short_sub_size_df[s]=(1+short_sub_signal[s]).cumprod()*short_sub_size_row[s]\n",
    "                    short_sub_pnl[s]=(short_sub_size_df[s].shift(1))*short_sub_signal[s] \n",
    "                # need to shift by 1 as the size is end of the day\n",
    "        \n",
    "            short_daily_pnl=pd.concat(list(short_sub_pnl.values()),axis=0)\n",
    "            short_acct_curve=short_daily_pnl.cumsum().ffill().sum(axis=1)\n",
    "            short_size_df=pd.concat(list(short_sub_size_df.values()),axis=0)\n",
    "            short_ind_return=short_daily_pnl.cumsum().ffill().iloc[-1].dropna()\n",
    "            \n",
    "            short_cache=(short_daily_pnl,short_acct_curve,short_size_df,short_ind_return)\n",
    "            self.short_cache=short_cache\n",
    "    \n",
    "        '''Put alpha positions together to form the alpha part'''\n",
    "        alpha_df=pd.concat([long_df,short_df],axis=1)\n",
    "        self.alpha_df=alpha_df\n",
    "        \n",
    "        alpha_daily_pnl=pd.concat([long_cache[0],short_cache[0]],axis=1)\n",
    "        alpha_acct_curve=alpha_daily_pnl.cumsum().ffill().sum(axis=1)\n",
    "        alpha_size_df=pd.concat([long_cache[2],short_cache[2]],axis=1)\n",
    "        alpha_ind_return=pd.concat([long_cache[3],short_cache[3]],axis=0)\n",
    "        \n",
    "        alpha_cache=(alpha_daily_pnl,alpha_acct_curve,alpha_size_df,alpha_ind_return)\n",
    "        \n",
    "        \n",
    "        if self.index_df is not None:\n",
    "            if self.index_df.shape[1]==1:\n",
    "                index_df=self.index_df.copy().loc[alpha_df.index] \n",
    "                index_size_df=(net_level-alpha_size_df.sum(axis=1)).to_frame(index_df.columns[0])\n",
    "                index_daily_pnl=index_size_df.shift(1)*index_df\n",
    "                index_acct_curve=index_daily_pnl.cumsum()\n",
    "                index_ind_return=index_acct_curve.iloc[-1]\n",
    "                index_cache=(index_daily_pnl,index_acct_curve,index_size_df,index_ind_return)\n",
    "            else:\n",
    "                index_df=self.index_df.copy().loc[alpha_df.index] \n",
    "                alpha_temp=alpha_cache[2].copy().T\n",
    "                alpha_temp[\"index\"]=alpha_temp.apply(lambda x:Asia_mapping.loc[x.name[0][-2:]].iloc[0],axis=1)\n",
    "                index_size_df=net_level-alpha_temp.groupby(\"index\").apply(sum).T.iloc[:-1]\n",
    "                index_daily_pnl=index_size_df.shift(1)*index_df\n",
    "                index_acct_curve=index_daily_pnl.cumsum()\n",
    "                index_ind_return=index_acct_curve.iloc[-1]\n",
    "                index_cache=(index_daily_pnl,index_acct_curve,index_size_df,index_ind_return)\n",
    "        else:\n",
    "            index_cache=(None,None,None,None)\n",
    "            \n",
    "        '''Finally put everything together'''    \n",
    "        portfolio_df=pd.concat([alpha_df,index_df],axis=1)\n",
    "            \n",
    "        portfolio_size_df=pd.concat([alpha_cache[2],index_cache[2]],axis=1).sort_index()\n",
    "  \n",
    "        portfolio_daily_pnl=pd.concat([alpha_cache[0],index_cache[0]],axis=1).sort_index()\n",
    "\n",
    "        portfolio_acct_curve=portfolio_daily_pnl.cumsum().ffill().sum(axis=1)\n",
    "        portfolio_ind_return=alpha_cache[3].copy()\n",
    "        \n",
    "        portfolio_gross=np.abs(portfolio_size_df).sum(axis=1).sort_index()\n",
    "        portfolio_turnover=(np.abs(alpha_size_df.fillna(0.0).diff(1)).sum().sum())/(portfolio_size_df.shape[0]/260)\n",
    "        \n",
    "        portfolio_cache=(portfolio_daily_pnl,portfolio_acct_curve,portfolio_size_df,portfolio_ind_return,portfolio_gross,\\\n",
    "                         portfolio_turnover,portfolio_df)\n",
    "        \n",
    "        self.portfolio_account=portfolio_cache #save for later use\n",
    "        \n",
    "        \n",
    "        return long_cache,short_cache,alpha_cache,portfolio_cache\n",
    "    \n",
    "    def plot_account(self,title,figsize=[10,4],portfolio=None):\n",
    "        '''\n",
    "        Plot the account curve\n",
    "        '''\n",
    "        if portfolio is None:\n",
    "            try:\n",
    "                portfolio_cache=self.portfolio_account\n",
    "\n",
    "            except AttributeError:\n",
    "                print(\"Execute the signal_account first!\")  \n",
    "                return None\n",
    "        else:\n",
    "            portfolio_cache=portfolio\n",
    "        \n",
    "        plot_signal(title,figsize,portfolio_cache)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_signal(portfolio_list,capital_list):\n",
    "    \n",
    "    \n",
    "    daily_pnl_list=[]\n",
    "    signal_df_list=[]\n",
    "    portfolio_size_list=[]\n",
    "    for i in range(len(portfolio_list)):\n",
    "\n",
    "        daily_pnl_list.append(portfolio_list[i][0]*capital_list[i]/np.sum(capital_list))\n",
    "        signal_df_list.append(portfolio_list[i][-1]*capital_list[i]/np.sum(capital_list))\n",
    "        portfolio_size_list.append(portfolio_list[i][2]*capital_list[i]/np.sum(capital_list))\n",
    "    \n",
    "    daily_pnl=pd.concat(daily_pnl_list,axis=1)\n",
    "    signal_df=pd.concat(signal_df_list,axis=1)\n",
    "    size_df=pd.concat(portfolio_size_list,axis=1)\n",
    "    \n",
    "    account_curve=daily_pnl.cumsum().ffill().sum(axis=1)\n",
    "    ind_return=daily_pnl.cumsum().ffill().iloc[-1]\n",
    "\n",
    "    gross=np.abs(size_df).sum(axis=1)\n",
    "    turnover=(np.abs(size_df.fillna(0.0).diff(1)).sum().sum())/(size_df.shape[0]/260)\n",
    "    \n",
    "    portfolio_cache=(daily_pnl,account_curve,size_df,ind_return,gross,turnover,signal_df)\n",
    "    return portfolio_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpe_by_year(account_curve,year_list):\n",
    "    result_dict={}\n",
    "    for i in year_list:\n",
    "        sub_curve=account_curve.iloc[(account_curve.index>=pd.Timestamp(i,1,1))&(account_curve.index<=pd.Timestamp(i,12,31))]\n",
    "        result_dict[i]=trading_analytics_simp(sub_curve)[0]\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dd_by_year(account_curve,year_list):\n",
    "    result_dict={}\n",
    "    for i in year_list:\n",
    "        sub_curve=account_curve.iloc[(account_curve.index>=pd.Timestamp(i,1,1))&(account_curve.index<=pd.Timestamp(i,12,31))]\n",
    "        result_dict[i]=trading_analytics_simp(sub_curve)[1]\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1-Europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Europe_new=signal(fundamental_df=Europe2_old,\n",
    "                   price_df=Europe_VWAP,\n",
    "                   EAR_period=1,\n",
    "                   entry=3,\n",
    "                   long_criteria=((2,1000),(0.02,1000),None,None),\n",
    "                   short_criteria=((-1000,-2),(-1000,-0.02),None,(5000,10000000)),\n",
    "                   holding=30,\n",
    "                   start=pd.Timestamp(2010,1,1),\n",
    "                   end=pd.Timestamp(2019,3,29),\n",
    "                   old_position=False,\n",
    "                   new_signal=True,\n",
    "                   revision_adjust=(True,(0.02,0.05),(-0.05,-0.02),2),\n",
    "                  early_exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_long_Europe,_short_Europe,_alpha_Europe,_portfolio_Europe=Europe_new.signal_account(stop=[9,12,\"rel\"],\n",
    "                                                           gross=(26,13),\n",
    "                                                           index_df=abs_return_index_Europe.loc[\"SX5E Index\"].to_frame('Europe'),\n",
    "                                                           net_level=0,\n",
    "                                                           risk_parity=True,\n",
    "                                                           liquidity=0.2,\n",
    "                                                           capital=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signal(\"Europe new signal - no stop\",[10,4],Europe_new.portfolio_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signal(\"Europe new signal - 100k abs stop\",[10,4],Europe_new.portfolio_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signal(\"Europe new signal - 200k abs stop\",[10,4],Europe_new.portfolio_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signal(\"Europe new signal - 100k rel stop\",[10,4],Europe_new.portfolio_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signal(\"Europe new signal - 200k rel stop\",[10,4],Europe_new.portfolio_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Europe_orig=signal(fundamental_df=Europe2_old,\n",
    "                    price_df=Europe_price,\n",
    "                    EAR_period=1,\n",
    "                    entry=2,\n",
    "                    long_criteria=((1,1000),(0.03,1000),None,None),\n",
    "                    short_criteria=((-1000,-2),(-1000,-0.03),None,(5000,10000000)),\n",
    "                    holding=30,\n",
    "                    start=pd.Timestamp(2010,1,1),\n",
    "                    end=pd.Timestamp(2019,3,29),\n",
    "                    old_position=False,\n",
    "                    new_signal=False,\n",
    "                    revision_adjust=(False,0.02,0.1,2),\n",
    "                    early_exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_long_Europe,_short_Europe,_alpha_Europe,_portfolio_Europe=Europe_orig.signal_account(stop=[4.5,6,\"rel\"],\n",
    "                                                           gross=(25,13),\n",
    "                                                           index_df=abs_return_index_Europe.loc[\"SX5E Index\"].to_frame('Europe'),\n",
    "                                                           net_level=0,\n",
    "                                                           risk_parity=True,\n",
    "                                                           liquidity=0.2,\n",
    "                                                           capital=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signal(\"Europe orig signal - no stop\",[10,4],Europe_orig.portfolio_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signal(\"Europe orig signal - abs stop 100k\",[10,4],Europe_orig.portfolio_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signal(\"Europe orig signal - abs stop 200k\",[10,4],Europe_orig.portfolio_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signal(\"Europe orig signal - rel stop 100k\",[10,4],Europe_orig.portfolio_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signal(\"Europe orig signal - rel stop 200k\",[10,4],Europe_orig.portfolio_account)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 - Asia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia_new=signal(fundamental_df=Asia2_old,\n",
    "                price_df=Asia_VWAP,\n",
    "                EAR_period=2,\n",
    "                entry=3,\n",
    "                long_criteria=((1,1000),(0.02,1000),None,None),\n",
    "                short_criteria=None,\n",
    "                holding=20,\n",
    "                start=pd.Timestamp(2010,1,1),\n",
    "                end=pd.Timestamp(2019,3,29),\n",
    "                old_position=False,\n",
    "                new_signal=True,\n",
    "                revision_adjust=(True,(0.02,0.05),(-0.1,-0.02),2),\n",
    "                early_exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_long_Asia,_short_Asia,_alpha_Asia,_portfolio_Asia=Asia_new.signal_account(stop=[5.5,None,'rel'],\n",
    "                                                           gross=(28,None),\n",
    "                                                           index_df=abs_return_index_Asia.loc[[\"AS51 Index\",\"HSI Index\",\n",
    "                                                                                                \"TPX Index\",\"KOSPI Index\",\n",
    "                                                                                                \"MXSG Index\",\"TAMSCI Index\"]].T,\n",
    "                                                           net_level=0,\n",
    "                                                           risk_parity=True,\n",
    "                                                           liquidity=0.2,\n",
    "                                                           capital=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia_new.plot_account(\"Asia new signal - no stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Asia_new.plot_account(\"Asia new signal - 100k abs stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia_new.plot_account(\"Asia new signal - 200k abs stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia_new.plot_account(\"Asia new signal - 100k rel stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia_new.plot_account(\"Asia new signal - 200k rel stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia_orig=signal(fundamental_df=Asia2_old,\n",
    "                 price_df=Asia_price,\n",
    "                 EAR_period=2,\n",
    "                 entry=2,\n",
    "                 long_criteria=((2,1000),(0.02,1000),None,None),\n",
    "                 short_criteria=None,\n",
    "                 holding=20,\n",
    "                 start=pd.Timestamp(2010,1,1),\n",
    "                 end=pd.Timestamp(2019,3,29),\n",
    "                 old_position=False,\n",
    "                 new_signal=False,\n",
    "                 revision_adjust=(False,(0.02,0.1),(-0.1,-0.02),3),\n",
    "                 early_exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_long_Asia,_short_Asia,_alpha_Asia,_portfolio_Asia=Asia_orig.signal_account(stop=[16,None,'rel'],\n",
    "                                                           gross=(29,None),\n",
    "                                                           index_df=abs_return_index_Asia.loc[[\"AS51 Index\",\"HSI Index\",\n",
    "                                                                                                \"TPX Index\",\"KOSPI Index\",\n",
    "                                                                                                \"MXSG Index\",\"TAMSCI Index\"]].T,\n",
    "                                                           net_level=0,\n",
    "                                                           risk_parity=True,\n",
    "                                                           liquidity=0.2,\n",
    "                                                           capital=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia_orig.plot_account(\"Asia orig signal - no stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia_orig.plot_account(\"Asia orig signal - 100k abs stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia_orig.plot_account(\"Asia orig signal - 200k abs stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia_orig.plot_account(\"Asia orig signal - 100k rel stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia_orig.plot_account(\"Asia orig signal - 200k rel stop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 - US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_new=signal(fundamental_df=US1_old,\n",
    "               price_df=US_VWAP,\n",
    "               EAR_period=1,\n",
    "               entry=2,\n",
    "               long_criteria=((2,1000),(0.05,1000),None,None),\n",
    "               short_criteria=((-1000,-2),(-1000,-0.03),None,(5000,10000000)),\n",
    "               holding=30,\n",
    "               start=pd.Timestamp(2010,1,1),\n",
    "               end=pd.Timestamp(2019,3,29),\n",
    "               old_position=False,\n",
    "               new_signal=True,\n",
    "               revision_adjust=(True,(0.05,0.1),(-0.1,-0.03),2),\n",
    "               early_exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_long,_short,_alpha,_portfolio=US_new.signal_account(stop=[20,18,\"rel\"],\n",
    "                                                           gross=(30,15),\n",
    "                                                           index_df=(0.5*abs_return_index_US.loc[\"SPX Index\"]+\\\n",
    "                                                          0.5*abs_return_index_US.loc[\"RTY Index\"]).to_frame(\"US_index\"),\n",
    "                                                           net_level=0,\n",
    "                                                           risk_parity=True,\n",
    "                                                           liquidity=0.2,\n",
    "                                                           capital=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signal(\"US new signal - no stop\",[10,4],US_new.portfolio_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signal(\"US new signal - abs stop 100k\",[10,4],US_new.portfolio_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signal(\"US new signal - abs stop 200k\",[10,4],US_new.portfolio_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signal(\"US new signal - rel stop 100k\",[10,4],US_new.portfolio_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signal(\"US new signal - rel stop 200k\",[10,4],US_new.portfolio_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_orig=signal(fundamental_df=US1_old,\n",
    "                price_df=US_price,\n",
    "                EAR_period=1,\n",
    "                entry=2,\n",
    "                long_criteria=((3,1000),(0.05,1000),None,None),\n",
    "                short_criteria=((-1000,-2),(-1000,-0.03),None,(5000,10000000)),\n",
    "                holding=30,\n",
    "                start=pd.Timestamp(2010,1,1),\n",
    "                end=pd.Timestamp(2019,3,29),\n",
    "                old_position=False,\n",
    "                new_signal=False,\n",
    "                revision_adjust=(False,(0.05,0.2),(-0.1,-0.03),2),\n",
    "               early_exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_long,_short,_alpha,_portfolio=US_orig.signal_account(stop=None,\n",
    "                                                           gross=(26,15),\n",
    "                                                           index_df=(0.5*abs_return_index_US.loc[\"SPX Index\"]+\\\n",
    "                                                          0.5*abs_return_index_US.loc[\"RTY Index\"]).to_frame(\"US_index\"),\n",
    "                                                           net_level=0,\n",
    "                                                           risk_parity=True,\n",
    "                                                           liquidity=0.2,\n",
    "                                                           capital=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signal(\"US orig signal - no exit\",[10,4],US_orig.portfolio_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Fitting choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitting_df(long,EAR_list,revision_list,fundamental_df,price_df,EAR_period,holding,start,end,stop,gross,index_df,\n",
    "               net_level,risk_parity,liquidity,capital,exclusive,old_position,new_signal):\n",
    "    '''\n",
    "    Create a dataframe of sharpe and drawdown for different fitting variations\n",
    "    EAR/revision list includes the upper bound\n",
    "    Exclusive means gap and non eclusive means from that to the maximum\n",
    "    '''\n",
    "    year_list=range(start.year,end.year+1)\n",
    "    EAR_list.sort()\n",
    "    revision_list.sort()\n",
    "    \n",
    "    criteria_list_short=list(itertools.product(*[EAR_list[1:],revision_list[1:]]))\n",
    "    criteria_list_long=list(itertools.product(*[EAR_list[:-1],revision_list[:-1]]))\n",
    "    \n",
    "    sharpe_dict={}\n",
    "    dd_dict={}\n",
    "    if exclusive:\n",
    "        if long:\n",
    "            for s in criteria_list_long:\n",
    "                signal_class=signal(fundamental_df=fundamental_df,\n",
    "                                   price_df=price_df,\n",
    "                                   EAR_period=EAR_period,\n",
    "                                   long_criteria=((s[0],EAR_list[EAR_list.index(s[0])+1]),\\\n",
    "                                                  (s[1],revision_list[revision_list.index(s[1])+1]),None,None),\n",
    "                                   short_criteria=None,\n",
    "                                   holding=holding,\n",
    "                                   start=start,\n",
    "                                   end=end,\n",
    "                                   old_position=old_position,\n",
    "                                   new_signal=new_signal)\n",
    "                \n",
    "                portfolio=signal_class.signal_account(stop=stop,\n",
    "                                                      gross=gross,\n",
    "                                                      index_df=index_df.copy(),\n",
    "                                                      net_level=net_level,\n",
    "                                                      risk_parity=risk_parity,\n",
    "                                                      liquidity=liquidity,\n",
    "                                                      capital=capital)\n",
    "\n",
    "                sharpe_dict[s]=sharpe_by_year(portfolio[-1][1],year_list)\n",
    "                dd_dict[s]=dd_by_year(portfolio[-1][1],year_list)\n",
    "                \n",
    "        else:\n",
    "            for s in criteria_list_short:\n",
    "                signal_class=signal(fundamental_df=fundamental_df,\n",
    "                                   price_df=price_df,\n",
    "                                   EAR_period=EAR_period,\n",
    "                                   long_criteria=None,\n",
    "                                   short_criteria=((EAR_list[EAR_list.index(s[0])-1],s[0]),\\\n",
    "                                                   (revision_list[revision_list.index(s[1])-1],s[1]),None,(5000,10000000)),\n",
    "                                   holding=holding,\n",
    "                                   start=start,\n",
    "                                   end=end,\n",
    "                                   old_position=old_position,\n",
    "                                   new_signal=new_signal)\n",
    "\n",
    "                portfolio=signal_class.signal_account(stop=stop,\n",
    "                                                      gross=gross,\n",
    "                                                      index_df=index_df.copy(),\n",
    "                                                      net_level=net_level,\n",
    "                                                      risk_parity=risk_parity,\n",
    "                                                      liquidity=liquidity,\n",
    "                                                      capital=capital)\n",
    "\n",
    "                sharpe_dict[s]=sharpe_by_year(portfolio[-1][1],year_list)\n",
    "                dd_dict[s]=dd_by_year(portfolio[-1][1],year_list)\n",
    "    \n",
    "    else:\n",
    "        if long:\n",
    "            for i in criteria_list_long:\n",
    "                signal_class=signal(fundamental_df=fundamental_df,\n",
    "                                   price_df=price_df,\n",
    "                                   EAR_period=EAR_period,\n",
    "                                   long_criteria=((i[0],10),(i[1],10),None,None),\n",
    "                                   short_criteria=None,\n",
    "                                   holding=holding,\n",
    "                                   start=start,\n",
    "                                   end=end,\n",
    "                                   old_position=old_position,\n",
    "                                   new_signal=new_signal)\n",
    "\n",
    "                portfolio=signal_class.signal_account(stop=stop,\n",
    "                                                      gross=gross,\n",
    "                                                      index_df=index_df.copy(),\n",
    "                                                      net_level=net_level,\n",
    "                                                      risk_parity=risk_parity,\n",
    "                                                      liquidity=liquidity,\n",
    "                                                      capital=capital)\n",
    "\n",
    "                sharpe_dict[i]=sharpe_by_year(portfolio[-1][1],year_list)\n",
    "                dd_dict[i]=dd_by_year(portfolio[-1][1],year_list)\n",
    "                \n",
    "        else:\n",
    "            for i in criteria_list_short:\n",
    "                signal_class=signal(fundamental_df=fundamental_df,\n",
    "                                   price_df=price_df,\n",
    "                                   EAR_period=EAR_period,\n",
    "                                   long_criteria=None,\n",
    "                                   short_criteria=((-10,i[0]),(-10,i[1]),None,(5000,10000000)),\n",
    "                                   holding=holding,\n",
    "                                   start=start,\n",
    "                                   end=end,\n",
    "                                   old_position=old_position,\n",
    "                                   new_signal=new_signal)\n",
    "                \n",
    "                portfolio=signal_class.signal_account(stop=stop,\n",
    "                                                      gross=gross,\n",
    "                                                      index_df=index_df.copy(),\n",
    "                                                      net_level=net_level,\n",
    "                                                      risk_parity=risk_parity,\n",
    "                                                      liquidity=liquidity,\n",
    "                                                      capital=capital)\n",
    "\n",
    "                sharpe_dict[i]=sharpe_by_year(portfolio[-1][1],year_list)\n",
    "                dd_dict[i]=dd_by_year(portfolio[-1][1],year_list)\n",
    "\n",
    "    sharpe_df=pd.DataFrame(sharpe_dict)\n",
    "    dd_df=pd.DataFrame(dd_dict)\n",
    "        \n",
    "    return sharpe_df,dd_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Europe_long_sharpe,Europe_long_dd=fitting_df(True,[1,2,3,4],[0.02,0.03,0.04,0.05,10],Europe2,Europe_price,2,30,\\\n",
    "                                             pd.Timestamp(2010,1,1),pd.Timestamp(2018,12,31),(8,None),(30,None)\\\n",
    "                                             ,abs_return_index_Europe.loc[\"SX5E Index\"].to_frame('Europe'),0,True,0.2,50,True,\n",
    "                                            False,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Europe_long_sharpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Europe_short_sharpe,Europe_short_dd=fitting_df(False,[-1,-2,-3,-4],[-0.02,-0.03,-0.04,-0.05,-0.06],Europe,Europe_price,2,30,\\\n",
    "                                             pd.Timestamp(2007,1,1),pd.Timestamp(2018,12,31),(8,8),(30,30)\\\n",
    "                                             ,abs_return_index_Europe.loc[\"SX5E Index\"].to_frame('Europe'),0,True,0.2,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Europe_short_sharpe.to_csv(\"Europe_short_sharpe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Europe_short_dd.to_csv(\"Europe_short_dd.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_long_sharpe,US_long_dd=fitting_df(True,[1,2,3,4],[0.02,0.03,0.04,0.05,0.06,0.07],\\\n",
    "                                     US,US_price,2,30,pd.Timestamp(2007,1,1),pd.Timestamp(2018,12,31),(8,None),(30,None)\\\n",
    "                                             ,(0.5*abs_return_index_US.loc[\"SPX Index\"]+0.5*abs_return_index_US.loc[\"RTY Index\"])\\\n",
    "                                     .to_frame('US'),0,True,0.2,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_US_long_sharpe,adj_US_long_dd=fitting_df(True,[1,2,3,4],[0.02,0.03,0.04,0.05,0.06,0.07],\\\n",
    "                                     adj_US,US_price,2,30,pd.Timestamp(2007,1,1),pd.Timestamp(2018,12,31),(8,None),(30,None)\\\n",
    "                                             ,(0.5*abs_return_index_US.loc[\"SPX Index\"]+0.5*abs_return_index_US.loc[\"RTY Index\"])\\\n",
    "                                     .to_frame('US'),0,True,0.2,50,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_short_sharpe,US_short_dd=fitting_df(False,[-1,-2,-3,-4],[-0.02,-0.03,-0.04,-0.05,-0.06,-0.07],\\\n",
    "                                     US,US_price,2,30,pd.Timestamp(2007,1,1),pd.Timestamp(2018,12,31),(8,8),(30,30)\\\n",
    "                                             ,(0.5*abs_return_index_US.loc[\"SPX Index\"]+0.5*abs_return_index_US.loc[\"RTY Index\"])\\\n",
    "                                     .to_frame('US'),0,True,0.2,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_US_short_sharpe,adj_US_short_dd=fitting_df(False,[-1,-2,-3],[-0.02,-0.03,-0.04,-0.05],\\\n",
    "                                     adj_US,US_price,2,30,pd.Timestamp(2007,1,1),pd.Timestamp(2018,12,31),(8,8),(30,30)\\\n",
    "                                             ,(0.5*abs_return_index_US.loc[\"SPX Index\"]+0.5*abs_return_index_US.loc[\"RTY Index\"])\\\n",
    "                                     .to_frame('US'),0,True,0.2,50,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_short_sharpe.to_csv(\"US_short_sharpe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_short_dd.to_csv(\"US_short_dd.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia_long_sharpe,Asia_long_dd=fitting_df(True,[1,2,3,4],[0.02,0.03,0.04,0.05,0.06,0.07],\\\n",
    "                                     Asia,Asia_price,2,20,pd.Timestamp(2007,1,1),pd.Timestamp(2018,12,31),(8,None),(30,None)\\\n",
    "                                             ,abs_return_index_Asia.loc[[\"AS51 Index\",\"HSI Index\",\n",
    "                                                                                                \"TPX Index\",\"KOSPI Index\",\n",
    "                                                                                                \"MXSG Index\",\"TAMSCI Index\"]].T\\\n",
    "                                     ,0,True,0.2,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia_long_sharpe.to_csv(\"Asian_long_sharpe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia_long_dd.to_csv(\"Asian_long_dd.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 - Drawdown analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawdown_days(acct_curve,timestamp):\n",
    "    dd=acct_curve-acct_curve.cummax()\n",
    "    previous_high=acct_curve.loc[:timestamp].max()\n",
    "    high_timestamp=acct_curve[acct_curve==previous_high].index[0]\n",
    "    days=acct_curve.index.tolist().index(timestamp)-acct_curve.index.tolist().index(high_timestamp)\n",
    "    return (high_timestamp,days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawdown_df(portfolio_cache):\n",
    "    drawdown=portfolio_cache[-1].iloc[:,-1].to_frame()\n",
    "    drawdown.columns=[\"Index\"]\n",
    "    drawdown[\"Index vol\"]=drawdown[\"Index\"].rolling(30).std()\n",
    "    drawdown[\"Portfolio\"]=portfolio_cache[1].diff(1)\n",
    "    drawdown[\"Account\"]=portfolio_cache[1].copy()\n",
    "    drawdown[\"Drawdown\"]= drawdown[\"Account\"]- drawdown[\"Account\"].cummax()\n",
    "    drawdown[\"Drawdown days\"]= drawdown.apply(lambda x: drawdown_days(drawdown[\"Account\"],x.name)[1],axis=1)\n",
    "    drawdown[\"Drawdown start\"]= drawdown.apply(lambda x: drawdown_days(drawdown[\"Account\"],x.name)[0],axis=1)\n",
    "    drawdown[\"Gross\"]=np.abs(portfolio_cache[2]).sum(axis=1)\n",
    "    return drawdown\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def portfolio_drawdown_df(portfolio_cache,stop,multiple):\n",
    "    portfolio_cache_list=list(portfolio_cache)\n",
    "    orig_size=portfolio_cache_list[2]\n",
    "    orig_account=portfolio_cache_list[1]\n",
    "    dd=orig_account-orig_account.cummax()\n",
    "    stop_evaluate=dd[dd.shift(1)<stop]\n",
    "    adj_size=orig_size.apply(lambda x: x*multiple if x.name in stop_evaluate.index else x,axis=1)\n",
    "    \n",
    "    new_portfolio_cache=portfolio_cache_list.copy()\n",
    "    new_portfolio_cache[2]=adj_size\n",
    "    new_portfolio_cache[0]=adj_size.shift(1)*new_portfolio_cache[-1]\n",
    "    new_portfolio_cache[1]=new_portfolio_cache[0].sum(axis=1).cumsum()\n",
    "    return tuple(new_portfolio_cache)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8- Quarterly review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_df(signal_class,start,end,old_position):\n",
    "    '''\n",
    "    dataframe for review purpose\n",
    "    '''\n",
    "    try:\n",
    "        long_abs_return=slice_universe(signal_class.long_df.copy(),start,end,old_position)\n",
    "        long_abs_return[long_abs_return==0]=None\n",
    "        long_rel_return=long_abs_return.subtract(signal_class.portfolio_account[-1].iloc[:,-1],axis=0)\n",
    "        long_df=((1+long_rel_return).cumprod()-1).ffill().iloc[-1].to_frame()\n",
    "        long_df[\"Ticker\"]=long_df.apply(lambda x:x.name[0],axis=1)\n",
    "        long_df[\"Date\"]=long_df.apply(lambda x:datetime.strptime(x.name[1],\"%d/%b/%Y\"),axis=1)\n",
    "        \n",
    "        long_size=slice_universe(signal_class.long_cache[2].copy(),start,end,old_position)\n",
    "        long_df[\"Size\"]=long_size.apply(lambda x: x.dropna().iloc[0],axis=0)\n",
    "    except:\n",
    "        long_df=None\n",
    "\n",
    "    try:\n",
    "        short_abs_return=-slice_universe(signal_class.short_df.copy(),start,end,old_position)\n",
    "        short_abs_return[short_abs_return==0]=None\n",
    "        short_rel_return=short_abs_return.add(signal_class.portfolio_account[-1].iloc[:,-1],axis=0)\n",
    "        short_df=((1+short_rel_return).cumprod()-1).ffill().iloc[-1].to_frame()\n",
    "        short_df[\"Ticker\"]=short_df.apply(lambda x:x.name[0],axis=1)\n",
    "        short_df[\"Date\"]=short_df.apply(lambda x:datetime.strptime(x.name[1],\"%d/%b/%Y\"),axis=1)\n",
    "        \n",
    "        short_size=slice_universe(signal_class.short_cache[2].copy(),start,end,old_position)\n",
    "        short_df[\"Size\"]=short_size.apply(lambda x: x.dropna().iloc[0],axis=0)\n",
    "    except:\n",
    "        short_df=None\n",
    "        \n",
    "    return long_df,short_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia_long,Asia_short=review_df(Asia_fit1,pd.Timestamp(2019,1,1),pd.Timestamp(2019,3,29),False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia_long.to_csv(\"Asia_long.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 - Sizing for the live trading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 -  US live sizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_live_orig=signal(fundamental_df=US2,\n",
    "                   price_df=US_price,\n",
    "                   EAR_period=1,\n",
    "               entry=2,\n",
    "                   long_criteria=((3,1000),(0.05,1000),None,None),\n",
    "                   short_criteria=((-1000,-2),(-1000,-0.03),None,(5000,10000000)),\n",
    "                   holding=30,\n",
    "                   start=pd.Timestamp(2019,1,1),\n",
    "                   end=pd.Timestamp(2019,3,29),\n",
    "                  old_position=False,\n",
    "                  new_signal=False,\n",
    "                 revision_adjust=(False,(0.05,0.2),(-0.1,-0.03),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_long_US_live,_short_US_live,_alpha_US_live,_portfolio_US_live=US_live_orig.signal_account(stop=(8,8),\n",
    "                                                           gross=(30,15),\n",
    "                                                           index_df=(0.5*abs_return_index_US.loc[\"SPX Index\"]+\\\n",
    "                                                          0.5*abs_return_index_US.loc[\"RTY Index\"]).to_frame(\"US_index\"),\n",
    "                                                           net_level=0,\n",
    "                                                           risk_parity=True,\n",
    "                                                           liquidity=0.2,\n",
    "                                                           capital=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_live.plot_account(\"US reference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "US reference sizing and volatility\n",
    "'''\n",
    "print(\"US long size is \"+str(_long_US_live[2].apply(lambda x:x.dropna().iloc[0],axis=0).mean()))\n",
    "print(\"US long volatility is \"+str(US2.loc[_long_US_live[-1].index][\"30d_vol\"].mean()))\n",
    "print(\"US long number is \"+str(_long_US_live[2].count(axis=1).mean()))\n",
    "print(\"US short size is \"+str(_short_US_live[2].apply(lambda x:x.dropna().iloc[0],axis=0).mean()))\n",
    "print(\"US short volatility is \"+str(US2.loc[_short_US_live[-1].index][\"30d_vol\"].mean()))\n",
    "print(\"US short number is \"+str(_short_US_live[2].count(axis=1).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_live_new=signal(fundamental_df=US1,\n",
    "                   price_df=US_VWAP,\n",
    "                   EAR_period=1,\n",
    "               entry=2,\n",
    "                   long_criteria=((2,1000),(0.05,1000),None,None),\n",
    "                   short_criteria=((-1000,-2),(-1000,-0.03),None,(5000,10000000)),\n",
    "                   holding=30,\n",
    "                   start=pd.Timestamp(2019,1,1),\n",
    "                   end=pd.Timestamp(2019,3,29),\n",
    "                  old_position=False,\n",
    "                  new_signal=True,\n",
    "                 revision_adjust=(True,(0.05,0.2),(-0.1,-0.03),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_long_US,_short_US,_alpha_US,_portfolio_US=US_live_new.signal_account(stop=(8,8),\n",
    "                                                           gross=(30,15),\n",
    "                                                           index_df=(0.5*abs_return_index_US.loc[\"SPX Index\"]+\\\n",
    "                                                          0.5*abs_return_index_US.loc[\"RTY Index\"]).to_frame(\"US_index\"),\n",
    "                                                                                      net_level=0,\n",
    "                                                           risk_parity=True,\n",
    "                                                           liquidity=0.2,\n",
    "                                                           capital=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,long_lower_size_US=sizing(US_live_new.long_df,US_live_new.long_df,30,US_live_new.fundamental_df,True,abs_return_US\\\n",
    "                    ,True,0.2,100, US_live_new.revision_adjust, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,short_lower_size_US=sizing(US_live_new.short_df,US_live_new.short_df,30,US_live_new.fundamental_df,True,abs_return_US\\\n",
    "                    ,True,0.2,100, US_live_new.revision_adjust,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "US reference sizing and volatility\n",
    "'''\n",
    "print(\"US long size is \"+str(long_lower_size_US))\n",
    "print(\"US long volatility is \"+str(US1.loc[_long_US[-1].index][\"30d_vol\"].mean()))\n",
    "print(\"US long number is \"+str(_long_US[2].count(axis=1).mean()))\n",
    "print(\"US short size is \"+str(short_lower_size_US))\n",
    "print(\"US short volatility is \"+str(US1.loc[_short_US[-1].index][\"30d_vol\"].mean()))\n",
    "print(\"US short number is \"+str(_short_US[2].count(axis=1).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 - Europe live sizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Europe_live_old=signal(fundamental_df=Europe2,\n",
    "                    price_df=Europe_price,\n",
    "                   EAR_period=1,\n",
    "                    entry=2,\n",
    "                   long_criteria=((1,1000),(0.03,1000),None,None),\n",
    "                   short_criteria=((-1000,-2),(-1000,-0.03),None,(5000,10000000)),\n",
    "                   holding=30,\n",
    "                   start=pd.Timestamp(2019,1,1),\n",
    "                   end=pd.Timestamp(2019,3,29),\n",
    "                  old_position=False,\n",
    "                  new_signal=False,\n",
    "                 revision_adjust=(False,0.02,0.1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_long_Europe_live,_short_Europe_live,_alpha_Europe_live,_portfolio_Europe_live=Europe_live_old.signal_account(stop=(8,8),\n",
    "                                                           gross=(30,15),\n",
    "                                                           index_df=abs_return_index_Europe.loc[\"SX5E Index\"].to_frame('Europe'),\n",
    "                                                           net_level=0,\n",
    "                                                           risk_parity=True,\n",
    "                                                           liquidity=0.2,\n",
    "                                                           capital=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Europe_live1.plot_account(\"Europe reference 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Europe reference sizing and volatility\n",
    "'''\n",
    "print(\"Europe long size is \"+str(_long_Europe_live[2].apply(lambda x:x.dropna().iloc[0],axis=0).mean()))\n",
    "print(\"Europe long volatility is \"+str(Europe2.loc[_long_Europe_live[-1].index][\"30d_vol\"].mean()))\n",
    "print(\"Europe long number is \"+str(_long_Europe_live[2].count(axis=1).mean()))\n",
    "print(\"Europe short size is \"+str(_short_Europe_live[2].apply(lambda x:x.dropna().iloc[0],axis=0).mean()))\n",
    "print(\"Europe short volatility is \"+str(Europe2.loc[_short_Europe_live[-1].index][\"30d_vol\"].mean()))\n",
    "print(\"Europe short number is \"+str(_short_Europe_live[2].count(axis=1).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Europe_live_new=signal(fundamental_df=Europe2,\n",
    "                   price_df=Europe_VWAP,\n",
    "                   EAR_period=1,\n",
    "                   entry=3,\n",
    "                   long_criteria=((2,1000),(0.02,1000),None,None),\n",
    "                   short_criteria=((-1000,-2),(-1000,-0.02),None,(5000,10000000)),\n",
    "                   holding=30,\n",
    "                   start=pd.Timestamp(2019,1,1),\n",
    "                   end=pd.Timestamp(2019,3,29),\n",
    "                  old_position=False,\n",
    "                  new_signal=True,\n",
    "                 revision_adjust=(True,(0.02,0.05),(-0.05,-0.02),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_long_Europe,_short_Europe,_alpha_Europe,_portfolio_Europe=Europe_live_new.signal_account(stop=(8,8),\n",
    "                                                           gross=(30,15),\n",
    "                                                           index_df=abs_return_index_Europe.loc[\"SX5E Index\"].to_frame('Europe'),\n",
    "                                                           net_level=0,\n",
    "                                                           risk_parity=True,\n",
    "                                                           liquidity=0.2,\n",
    "                                                           capital=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,long_lower_size=sizing(Europe_live_new.long_df,Europe_live_new.long_df,30,Europe_live_new.fundamental_df,True,abs_return_Europe\\\n",
    "                    ,True,0.2,100, Europe_live_new.revision_adjust, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,short_lower_size=sizing(Europe_live_new.short_df,Europe_live_new.short_df,30,Europe_live_new.fundamental_df,True,abs_return_Europe\\\n",
    "                    ,True,0.2,100, Europe_live_new.revision_adjust,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Europe reference sizing and volatility\n",
    "'''\n",
    "print(\"Europe long size is \"+str(long_lower_size))\n",
    "print(\"Europe long volatility is \"+str(Europe2.loc[_long_Europe[-1].index][\"30d_vol\"].mean()))\n",
    "print(\"Europe long number is \"+str(_long_Europe[2].count(axis=1).mean()))\n",
    "print(\"Europe short size is \"+str(short_lower_size))\n",
    "print(\"Europe short volatility is \"+str(Europe2.loc[_short_Europe[-1].index][\"30d_vol\"].mean()))\n",
    "print(\"Europe short number is \"+str(_short_Europe[2].count(axis=1).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia_live_new=signal(fundamental_df=Asia2,\n",
    "                   price_df=Asia_VWAP,\n",
    "                   EAR_period=1,\n",
    "                entry=2,\n",
    "                   long_criteria=((1,1000),(0.02,1000),None,None),\n",
    "                   short_criteria=None,\n",
    "                   holding=20,\n",
    "                   start=pd.Timestamp(2010,1,1),\n",
    "                   end=pd.Timestamp(2019,3,29),\n",
    "                  old_position=False,\n",
    "                  new_signal=True,\n",
    "                 revision_adjust=(True,(0.02,0.05),(-0.1,-0.02),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia_live_old=signal(fundamental_df=Asia2,\n",
    "                   price_df=Asia_price,\n",
    "                   EAR_period=1,\n",
    "                entry=2,\n",
    "                   long_criteria=((1,1000),(0.02,1000),None,None),\n",
    "                   short_criteria=None,\n",
    "                   holding=20,\n",
    "                   start=pd.Timestamp(2019,1,1),\n",
    "                   end=pd.Timestamp(2019,3,29),\n",
    "                  old_position=False,\n",
    "                  new_signal=False,\n",
    "                 revision_adjust=(False,(0.02,0.05),(-0.1,-0.02),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_long_Asia_live,_short_Asia_live,_alpha_Asia_live,_portfolio_Asia_live=Asia_live_new.signal_account(stop=(8,None),\n",
    "                                                           gross=(20,None),\n",
    "                                                           index_df=abs_return_index_Asia.loc[[\"AS51 Index\",\"HSI Index\",\n",
    "                                                                                                \"TPX Index\",\"KOSPI Index\",\n",
    "                                                                                                \"MXSG Index\",\"TAMSCI Index\"]].T,\n",
    "                                                           net_level=0,\n",
    "                                                           risk_parity=True,\n",
    "                                                           liquidity=0.2,\n",
    "                                                           capital=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Europe reference sizing and volatility\n",
    "'''\n",
    "print(\"Asia long size is \"+str(_long_Asia_live[2].apply(lambda x:x.dropna().iloc[0],axis=0).mean()))\n",
    "print(\"Asia long volatility is \"+str(Asia2.loc[_long_Asia_live[-1].index][\"30d_vol\"].mean()))\n",
    "print(\"Asia long number is \"+str(_long_Asia_live[2].count(axis=1).mean()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
