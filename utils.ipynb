{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import textwrap\n",
    "import swifter\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asia mapping reference\n",
    "Asia_mapping=pd.read_excel(r'C:\\Users\\Eric.Li\\OneDrive\\Post result data\\{0} 2007-2017.xlsx'.format('Asia'),\\\n",
    "                                       sheet_name='Mapping')\n",
    "Asia_mapping=Asia_mapping.dropna(axis=1,how=\"all\").set_index(Asia_mapping.columns[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_date_transform(CSV_date,index=False):\n",
    "    '''\n",
    "    Transform the CSV price style string into dateframe string style\n",
    "    The CSV date follows US style which is MM/DD/YYYY\n",
    "    '''\n",
    "    if index==False:\n",
    "        timestamp=pd.Timestamp(int(CSV_date[CSV_date.find(\"/\",3)+1:]),\n",
    "                            int(CSV_date[:CSV_date.find(\"/\")]),\n",
    "                            int(CSV_date[CSV_date.find(\"/\",1)+1:CSV_date.find(\"/\",3)]))\n",
    "        return timestamp.strftime(\"%d/%b/%Y\")\n",
    "    else:\n",
    "        timestamp=pd.Timestamp(int(CSV_date[-4:]),int(CSV_date[3:5]),int(CSV_date[:2]))\n",
    "        return timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fundamental_date_transform(CSV_date):\n",
    "    '''\n",
    "    Transform the fundamental style string into dateframe string style\n",
    "    CSV date follow following style yyyy-mm-dd or MM/DD/YYYY\n",
    "    '''\n",
    "    if '-' in CSV_date:\n",
    "        timestamp=pd.Timestamp(int(CSV_date[:4]),\n",
    "                            int(CSV_date[5:7]),\n",
    "                            int(CSV_date[8:]))\n",
    "    else:\n",
    "        timestamp=pd.Timestamp(int(CSV_date[CSV_date.find(\"/\",3)+1:]),\n",
    "                               int(CSV_date[CSV_date.find(\"/\",1)+1:CSV_date.find(\"/\",3)]),\n",
    "                               int(CSV_date[:CSV_date.find(\"/\")]))        \n",
    "    return timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CSV_price(region,start,end,VWAP):\n",
    "    '''\n",
    "    Grab the pricing data from CSV\n",
    "    region= US, Europe,Asia,or Canada\n",
    "    start,end are in year\n",
    "    VWAP: boolean to determine if we read price_df or VWAP_df\n",
    "    key is the reference to search\n",
    "    return the target price dataframe with timestamp on the column; also index price and return dataframe\n",
    "    '''\n",
    "    \n",
    "    # price dataframe\n",
    "    mylist=[]\n",
    "    \n",
    "    def clean_csv(csv):\n",
    "        ''' \n",
    "        csv is the read file from pandas\n",
    "        '''\n",
    "        data=csv.set_index(\"Ticker\")\n",
    "        adj_data=data.loc[[x for x in data.index if type(x)==str]].replace('#N/A N/A','').replace(' #N/A N/A ','').\\\n",
    "        replace('#N/A Invalid Security','')\n",
    "        adj_data=adj_data.loc[[x for x in adj_data.index if len(x)>0]]\n",
    "        return adj_data\n",
    "    \n",
    "    \n",
    "    if VWAP==True:\n",
    "        for year in range(start,end+1):\n",
    "            csv=pd.read_csv(r\"C:\\Users\\Eric.Li\\OneDrive\\Post result data\\{0} CSV\\{0}_VWAP_{1}.csv\".format(region,year)).dropna\\\n",
    "        (how='all',axis=0).dropna(how='all',axis=1).drop_duplicates()\n",
    "            adj_data=clean_csv(csv)\n",
    "            mylist.append(adj_data)\n",
    "               \n",
    "    else:\n",
    "        \n",
    "        csv=pd.read_csv(r\"C:\\Users\\Eric.Li\\OneDrive\\Post result data\\{0} CSV\\{0}_price_2007_2017.csv\".format(region)).dropna\\\n",
    "        (how='all',axis=0).dropna(how='all',axis=1).drop_duplicates()\n",
    "        adj_data=clean_csv(csv)\n",
    "        mylist.append(adj_data)\n",
    "        for year in range(2018,end+1):\n",
    "            csv=pd.read_csv(r\"C:\\Users\\Eric.Li\\OneDrive\\Post result data\\{0} CSV\\{0}_price_{1}.csv\".format(region,year)).dropna\\\n",
    "            (how='all',axis=0).dropna(how='all',axis=1).drop_duplicates()\n",
    "            adj_data=clean_csv(csv)\n",
    "            mylist.append(adj_data)\n",
    "\n",
    "    price=pd.concat(mylist,axis=1,sort=True)\n",
    "    price=price.apply(lambda x:pd.to_numeric(x),axis=1)\n",
    "    price.columns=[price_date_transform(i) for i in price.columns]\n",
    "    \n",
    "    # index price dataframe\n",
    "    csv_index=pd.read_csv(r\"C:\\Users\\Eric.Li\\OneDrive\\Post result data\\{0} CSV\\{0}_price_index.csv\".format(region)).dropna\\\n",
    "    (how='all',axis=0)\n",
    "    data_index=csv_index.set_index(\"Ticker\").T\n",
    "    price_index=data_index.replace('#N/A N/A','')\n",
    "    price_index.columns=[price_date_transform(i) for i in price_index.columns]\n",
    "    price_index=price_index.dropna(how=\"all\",axis=0)\n",
    "    \n",
    "    # return data\n",
    "    abs_return=price.diff(1,axis=1)/price.shift(1,axis=1)\n",
    "    abs_return_index=price_index.diff(1,axis=1)/price_index.shift(1,axis=1)\n",
    "    return price,abs_return,price_index,abs_return_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CSV_price_orig(region,start,end,VWAP):\n",
    "    '''\n",
    "    Grab the pricing data from CSV\n",
    "    region= US, Europe,Asia,or Canada\n",
    "    start,end are in year\n",
    "    key is the reference to search\n",
    "    return the target price dataframe with timestamp on the column\n",
    "    '''\n",
    "    mylist=[]\n",
    "    for year in range(start,end+1):\n",
    "        if VWAP==False:\n",
    "            csv=pd.read_csv(r\"C:\\Users\\Eric.Li\\OneDrive\\Post result data\\{0} CSV\\{0}_price_{1}.csv\".format(region,year)).dropna\\\n",
    "    (how='all',axis=0).dropna(how='all',axis=1)\n",
    "        else:\n",
    "            csv=pd.read_csv(r\"C:\\Users\\Eric.Li\\OneDrive\\Post result data\\{0} CSV\\{0}_VWAP_{1}.csv\".format(region,year)).dropna\\\n",
    "    (how='all',axis=0).dropna(how='all',axis=1)\n",
    "        data=csv.set_index(\"Ticker\")\n",
    "        adj_data=data.loc[[x for x in data.index if type(x)==str]].replace('#N/A N/A','').replace(' #N/A N/A ','').\\\n",
    "        replace('#N/A Invalid Security','')\n",
    "        adj_data=adj_data.loc[[x for x in adj_data.index if len(x)>0]]\n",
    "        mylist.append(adj_data)\n",
    "\n",
    "    price=pd.concat(mylist,axis=1,sort=True)\n",
    "    price=price.apply(lambda x:pd.to_numeric(x),axis=1)\n",
    "    \n",
    "    csv_index=pd.read_csv(r\"C:\\Users\\Eric.Li\\OneDrive\\Post result data\\{0} CSV\\{0}_price_index.csv\".format(region)).dropna\\\n",
    "    (how='all',axis=0)\n",
    "    data_index=csv_index.set_index(\"Ticker\").T\n",
    "    price_index=data_index.replace('#N/A N/A','')\n",
    "    #price_index=data_index.apply(lambda x:pd.to_numeric(x),axis=1)\n",
    "    \n",
    "    price.columns=[price_date_transform(i) for i in price.columns]\n",
    "    '''\n",
    "    Need to sort the columns for index price, and then transform to date string\n",
    "    '''\n",
    "    price_index.columns=[price_date_transform(i) for i in price_index.columns]\n",
    "    #price_index=price_index.reindex(sorted(price_index.columns))\n",
    "    #price_index.columns=[i.strftime(\"%d/%b/%Y\") for i  in price_index.columns]\n",
    "    \n",
    "    abs_return=price.diff(1,axis=1)/price.shift(1,axis=1)\n",
    "    abs_return_index=price_index.diff(1,axis=1)/price_index.shift(1,axis=1)\n",
    "    return price,abs_return,price_index,abs_return_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CSV_EPS(region,start,end):\n",
    "    '''\n",
    "    Grab the EPS data from CSV database\n",
    "    region= US, Europe,Asia,or Canada\n",
    "    start,end are in year\n",
    "    key is the reference to search\n",
    "    return the target EPS dataframe with timestamp on the column\n",
    "    '''\n",
    "    mylist=[]\n",
    "    for year in range(start,end+1):\n",
    "        csv=pd.read_csv(r\"C:\\Users\\Eric.Li\\OneDrive\\Post result data\\{0} CSV\\{0}_EPS_{1}.csv\".format(region,year))\n",
    "        data=csv.set_index(\"Ticker\")\n",
    "        adj_data=data.loc[[x for x in data.index if type(x)==str]].replace('#N/A N/A','').replace(\" #N/A N/A \",\"\").dropna\\\n",
    "        (how='all',axis=0).dropna(how='all',axis=1)\n",
    "        adj_data=adj_data.loc[[x for x in adj_data.index if len(x)>0]]\n",
    "        mylist.append(adj_data)\n",
    "\n",
    "    EPS=pd.concat(mylist,axis=1,sort=True)\n",
    "    EPS=EPS.apply(lambda x:pd.to_numeric(x),axis=1)\n",
    "    EPS.columns=[price_date_transform(i) for i in EPS.columns]\n",
    "    return EPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CSV_fundamentals(region,price,EPS_df,revision_period,min_history,min_vol,use_cache):\n",
    "    '''\n",
    "    Grab the fundamental data from the spreadsheet\n",
    "    region= US, Europe,Asia,or Canada\n",
    "    return the post result fundamental dataframe\n",
    "    use_cache: boolean, if yes we just read the last cache of fundamental_df\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if use_cache is True:\n",
    "        try:\n",
    "            data=pd.read_csv(r'C:\\Users\\Eric.Li\\OneDrive\\Post result data cache\\{0}{1}_fundamental_df.csv'.format(region,\\\n",
    "                                                                                                        str(revision_period)))\n",
    "            new_index=pd.MultiIndex.from_tuples(list(zip(data.iloc[:,0],data.iloc[:,1],data.iloc[:,2],data.iloc[:,3])))\n",
    "            data.index=new_index\n",
    "            target_data=data.iloc[:,4:]\n",
    "        except:\n",
    "            print(\"No such file!\")\n",
    "    else:\n",
    "        # import the raw fundamental_df and clean up all the nonsense\n",
    "        csv=pd.read_csv(r'C:\\Users\\Eric.Li\\OneDrive\\Post result data cache\\{0}_raw_fundamental_df.csv'.format(region))\n",
    "        data=csv.set_index(\"Ticker\").drop_duplicates().replace('#N/A Invalid Security','').\\\n",
    "        replace('#N/A Requesting Data...','')\n",
    "        data=data[data.index!='']\n",
    "        data=data.dropna(how=\"all\")\n",
    "\n",
    "        # Manipulate the data to get the next earning date, quarter end date, finally generate multi-index for the dataframe \n",
    "        data[\"date_copy\"]=[fundamental_date_transform(i) for i in data[\"Date\"].copy()]\n",
    "        data[\"ticker_copy\"]=data.index\n",
    "        data=data.copy().sort_values(by=[\"ticker_copy\",\"date_copy\"])\n",
    "        data[\"next_date\"]=data[\"date_copy\"].shift(-1)\n",
    "        data[\"ticker_copy\"]=data[\"ticker_copy\"].shift(-1)\n",
    "        data[\"Date\"]=data[\"date_copy\"].copy().swifter.apply(lambda x: x.strftime(\"%d/%b/%Y\") if x!='' else np.nan)\n",
    "        data[\"Orig date\"]=data[\"Orig date\"].copy().swifter.apply(lambda x: pd.Timestamp(x).strftime(\"%d/%b/%Y\")\\\n",
    "                                                          if x!='' else np.nan)\n",
    "        data[\"Next\"]=data.swifter.apply(lambda x: x[\"next_date\"].strftime(\"%d/%b/%Y\") \\\n",
    "                                        if type(x[\"next_date\"])==pd.Timestamp and \\\n",
    "                                x.name==x[\"ticker_copy\"] else np.nan,axis=1)\n",
    "        \n",
    "#         data[\"period\"]=data.apply(lambda x:str(pd.Timestamp(datetime.strptime(x[\"Date\"],\"%d/%b/%Y\")).year)\\\n",
    "#                                             +\" \"+str(pd.Timestamp(datetime.strptime(x[\"Date\"],\"%d/%b/%Y\")).quarter),\\\n",
    "#                                             axis=1)\n",
    "\n",
    "        data[\"end_period\"]=data.swifter.apply(lambda x: pd.offsets.BQuarterEnd().rollforward(x[\"date_copy\"])\\\n",
    "                                              .strftime(\"%d/%b/%Y\"),\\\n",
    "                                      axis=1)\n",
    "\n",
    "        data.index=pd.MultiIndex.from_tuples(list(zip(data.index,data[\"Date\"],data[\"Next\"],data[\"end_period\"])))\n",
    "\n",
    "        del data[\"ticker_copy\"]\n",
    "        del data[\"date_copy\"]\n",
    "        del data[\"next_date\"]\n",
    "        del data[\"end_period\"]\n",
    "        del data[\"Next\"]\n",
    "        \n",
    "\n",
    "        for s in [\"Market cap\",\"Volume\"]:\n",
    "            try:\n",
    "                data[s]=pd.to_numeric(data[s])\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "        '''\n",
    "        Add more forward look and realistic versions of earning revision\n",
    "        '''\n",
    "        data[\"Revision_real\"]=data.swifter.apply(lambda x: revision_calc(x.name[0],x.name[1],EPS_df,(0,revision_period)),axis=1)\n",
    "        data[\"Revision_20\"]=data.swifter.apply(lambda x: revision_calc(x.name[0],x.name[1],EPS_df,(0,20)),axis=1)\n",
    "\n",
    "        '''\n",
    "        take out data with zero or none revision/market cap\n",
    "        '''\n",
    "        data=data[(data[\"Market cap\"]>=500)] #universe above 500mn\n",
    "        data=data[(data[\"Revision_20\"]>=0)|(data[\"Revision_20\"]<0)]\n",
    "        \n",
    "        '''\n",
    "        take out cases where there is a short history\n",
    "        '''\n",
    "        count_history=data.swifter.apply(lambda x: price.loc[x.name[0],:x.name[1]][-2*min_history:].count() if x.name[1] in \\\n",
    "                                   price.columns and x.name[0] in price.index else None,axis=1)\n",
    "        \n",
    "        data=data.copy()[count_history>=min_history]\n",
    "        \n",
    "#         '''\n",
    "#         Add momentum\n",
    "#         '''\n",
    "#         data[\"mom\"]=data.apply(lambda x: np.log(price).loc[x.name[0],:x.name[1]][-260:-23].dropna()[-1]-\\\n",
    "#                                np.log(price).loc[x.name[0],:x.name[1]][-260:-23].dropna()[0] if x.name[0] in price.index\\\n",
    "#                                and price.loc[x.name[0],:x.name[1]][-260:-23].dropna().shape[0]!=0 else None, axis=1)\n",
    "        \n",
    "#         data=data[(data[\"mom\"]>=0)|(data[\"mom\"]<0)]\n",
    "        \n",
    "#         data[\"mom_short\"]=data.apply(lambda x: np.log(price).loc[x.name[0],:x.name[1]][-24:-1].dropna()[-1]/\\\n",
    "#                                price.loc[x.name[0],:x.name[1]][-24:-1].dropna()[0] if x.name[0] in price.index and\\\n",
    "#                                      price.loc[x.name[0],:x.name[1]][-24:-1].dropna().shape[0]!=0 else None, axis=1)      \n",
    "\n",
    "        '''\n",
    "        Add historic volatility\n",
    "        '''\n",
    "        \n",
    "        abs_return=price.diff(1,axis=1)/price.shift(1,axis=1)\n",
    "        data[\"30d_vol\"]=data.swifter.apply(lambda x: abs_return.loc[x.name[0],:x.name[1]][-31:-1].std() \\\n",
    "                                           if x.name[0] in abs_return.index\\\n",
    "                                   and abs_return.loc[x.name[0],:x.name[1]][-31:-1].dropna().shape[0]!=0 else None,axis=1)      \n",
    "        data=data[data[\"30d_vol\"]>=min_vol]\n",
    "        \n",
    "        '''\n",
    "        Final cleaning and export the data\n",
    "        '''\n",
    "        target_data=data.drop_duplicates()\n",
    "        target_data.to_csv(r'C:\\Users\\Eric.Li\\OneDrive\\Post result data cache\\{0}{1}_fundamental_df.csv'.format(region,\\\n",
    "                                                                                                      str(revision_period)))\n",
    "    return target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_enhancement(fundamental_df,price_df,index_df,EPS_df,EAR_tuple,post_return_period_list,\n",
    "                     index_name,MA_period_list,return_lookback_list,range_list,use_cache,region):\n",
    "    ''' \n",
    "    Enhance the existing fundamental data with more independent variaables to run machine learning\n",
    "    \n",
    "    *Parameters:\n",
    "    fundamental_df: refined fundamental_df after CSV_fundamental function\n",
    "    price_df: pandas dataframe, price data to calculate EAR, mom\n",
    "    index_df: dataframe with market index data and exogeneous variables\n",
    "    EPS_df: pandas dataframe, consensus EPS data to calculate revision\n",
    "    EAR_tuple: (start_reference,number of days), e.g. (0,1) means day1 return\n",
    "    post_return_period_list: list of periods used to calculate post return\n",
    "    index_name:dict with keys -\"index\",\"yield_long\",\"yield_short\",\"inflation\",\"currency\",\"VIX\"\n",
    "    MA_period_list: list for periods used to calculate moving average deviation\n",
    "    return_lookback_list:list for periods used to calculate past return abs\n",
    "    range_list:list for periods used to calculate the chart position\n",
    "    use_cache: boolean, if true then look for the exported csv\n",
    "    region: hard coded region for export use\n",
    "    \n",
    "    *Dependent variable:\n",
    "    post_return_net: relative return after EAR period for a list of periods\n",
    "    success: if relative return is positive\n",
    "    \n",
    "    *Independent variables:\n",
    "    adj_quarter: Q1 to Q4, string\n",
    "    Q1-Q4: dummy variable \n",
    "    Year: 2010-2017, string\n",
    "    2010-2017: dummy variable\n",
    "    Month: Jan to Dec string\n",
    "    Jan to Dec: dummy variable\n",
    "    Day: early mid or late, string\n",
    "    Early mid late: day dummy\n",
    "    Supersector dummy: dummy transformation from Supersector\n",
    "    \n",
    "    EAR_net: 3 day relative return centered the earning day, float\n",
    "    EAR_net_dummy: 1 if EAR_net above zero\n",
    "    EAR_net_last: EAR_net in the previous quarter\n",
    "    EAR_net_last_dummy: 1 if EAR_net_last above zero\n",
    "    \n",
    "    Revision_real: 2 day earning revision, overwrite any abosolute value above 90% to 90%\n",
    "    Revision_dummy: 1 if Revision real is positive\n",
    "    \n",
    "    Market cap: log market cap value in US dollars\n",
    "    mom/mom_short: log absolute stock return for last 52 weeks and last 4 weeks, up to the start of EAR period\n",
    "    \n",
    "    beta: market beta for 260 business days up to before the EAR period\n",
    "    \n",
    "    macro variables: value in yield,inflation,currency,VIX on the earning day\n",
    "    commodity variables: value in gold,oil,bloomberg commodity index on the earning day\n",
    "    \n",
    "    momentum index: long and short,deviation from MA and past return on the end of EAR period\n",
    "    market index: deviation from MA and past return on the end of EAR period\n",
    "    stock technicals: deviation n from MA, and chart position on the end of EAR period\n",
    "    \n",
    "    '''\n",
    "\n",
    "    if use_cache is True:\n",
    "        try:\n",
    "            data=pd.read_csv(r'C:\\Users\\Eric.Li\\OneDrive\\Post result data cache\\{0}_reg_data.csv'.format(region))\n",
    "            new_index=pd.MultiIndex.from_tuples(list(zip(data.iloc[:,0],data.iloc[:,1],data.iloc[:,2],data.iloc[:,3])))\n",
    "            data.index=new_index\n",
    "            fundamental_df=data.iloc[:,4:]\n",
    "        except:\n",
    "            print(\"No such file!\")\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        #Target rel index used for return calculation\n",
    "        index_series=index_df.loc[index_name[\"index\"]]\n",
    "        \n",
    "        #Add next earning date as a separate column\n",
    "        fundamental_df[\"Next date\"]=fundamental_df.apply(lambda x:x.name[2],axis=1)\n",
    "        \n",
    "        #Dependent variable\n",
    "        for i in post_return_period_list:\n",
    "            fundamental_df[\"post_return_net_{0}\".format(i)]=fundamental_df.swifter.\\\n",
    "            apply(lambda x:return_calc_log(x.name[0],x.name[1],price_df,index_series,(EAR_tuple[0]+EAR_tuple[1],i),'rel')\\\n",
    "                  ,axis=1)    \n",
    "            \n",
    "            fundamental_df[\"result_net_{0}\".format(i)]=(fundamental_df[\"post_return_net_{0}\".format(i)]>0)*1.0\n",
    "\n",
    "            fundamental_df[\"post_return_net_{0}_last\".format(i)]=fundamental_df.swifter.\\\n",
    "            apply(lambda x: fundamental_df.loc[x.name[0]].xs(x.name[1],level=1)[\"post_return_net_{0}\".format(i)].iloc[0]\\\n",
    "                            if x.name[1] in list(fundamental_df.loc[x.name[0]][\"Next date\"]) else None,axis=1)    \n",
    "\n",
    "            fundamental_df[\"result_net_{0}_last\".format(i)]=(fundamental_df[\"post_return_net_{0}_last\".format(i)]>0)*1.0\n",
    "\n",
    "        fundamental_df.to_csv(r'C:\\Users\\Eric.Li\\OneDrive\\Post result data cache\\{0}_reg_data.csv'.format(region))\n",
    "        \n",
    "        def result_category(post_return):\n",
    "            '''\n",
    "            sub category for post result returns\n",
    "            '''\n",
    "            if post_return<-0.1:\n",
    "                return \"1\"\n",
    "            elif post_return<=-0.05:\n",
    "                return \"2\"\n",
    "            elif post_return<=-0.01:\n",
    "                return \"3\"\n",
    "            elif post_return<=0:\n",
    "                return \"4\"\n",
    "            elif post_return<=0.01:\n",
    "                return \"5\"\n",
    "            elif post_return<=0.05:\n",
    "                return \"6\"\n",
    "            elif post_return<=0.1:\n",
    "                return \"7\"\n",
    "            elif post_return>0.1:\n",
    "                return \"8\"\n",
    "            else:\n",
    "                return None\n",
    "        \n",
    "        # split into 8 return category so that making 1% isn't the same as making 10%\n",
    "        fundamental_df[\"result_category_net\"]=fundamental_df.\\\n",
    "        apply(lambda x:result_category(x[\"post_return_net\"]),axis=1)\n",
    "        \n",
    "\n",
    "        def quarter(month):\n",
    "            '''\n",
    "            month: three letter string\n",
    "            '''\n",
    "            if month in [\"Jan\",\"Feb\",\"Mar\"]:\n",
    "                return \"Q1\"\n",
    "            elif month in [\"Apr\",\"May\",'Jun']:\n",
    "                return \"Q2\"\n",
    "            elif month in [\"Jul\",\"Aug\",\"Sep\"]:\n",
    "                return \"Q3\"\n",
    "            else:\n",
    "                return \"Q4\"\n",
    "\n",
    "        fundamental_df[\"adj_quarter\"]=fundamental_df.swifter.apply(lambda x:quarter(x.name[1][3:6]),axis=1)\n",
    "\n",
    "        fundamental_df[\"Year\"]=fundamental_df.swifter.apply(lambda x: x.name[1][-4:],axis=1)\n",
    "\n",
    "        fundamental_df[\"Month\"]=fundamental_df.swifter.apply(lambda x:x.name[1][3:6],axis=1)\n",
    "\n",
    "\n",
    "        def day_in_month(day):\n",
    "            ''' \n",
    "            day: int\n",
    "            '''\n",
    "            if day<=10:\n",
    "                return \"Early\"\n",
    "            elif day<=20:\n",
    "                return \"Mid\"\n",
    "            else:\n",
    "                return \"Late\"\n",
    "\n",
    "        fundamental_df[\"Day\"]=fundamental_df.swifter.apply(lambda x:day_in_month(int(x.name[1][:2])),axis=1)                         \n",
    "\n",
    "        # Dummy variable \n",
    "        for i in fundamental_df[\"Supersector\"].dropna().unique():\n",
    "            fundamental_df[i]=(fundamental_df[\"Supersector\"]==i)*1.0\n",
    "\n",
    "        for i in fundamental_df[\"adj_quarter\"].dropna().unique():\n",
    "            fundamental_df[i]=(fundamental_df[\"adj_quarter\"]==i)*1.0    \n",
    "\n",
    "        for i in fundamental_df[\"Year\"].dropna().unique():\n",
    "            fundamental_df[i]=(fundamental_df[\"Year\"]==i)*1.0  \n",
    "\n",
    "        for i in fundamental_df[\"Month\"].dropna().unique():\n",
    "            fundamental_df[i]=(fundamental_df[\"Month\"]==i)*1.0  \n",
    "\n",
    "        for i in fundamental_df[\"Day\"].dropna().unique():\n",
    "            fundamental_df[i]=(fundamental_df[\"Day\"]==i)*1.0          \n",
    "        \n",
    "        # EAR variable \n",
    "        fundamental_df[\"EAR_net\"]=fundamental_df.swifter.apply(lambda x:return_calc_log(x.name[0],x.name[1],\\\n",
    "                                                                                                         price_df,index_series,\\\n",
    "                                                                                       EAR_tuple,'rel'),axis=1)\n",
    "        \n",
    "        fundamental_df[\"EAR_net_dummy\"]=(fundamental_df[\"EAR_net\"]>0)*1.0\n",
    "        \n",
    "        \n",
    "        fundamental_df[\"EAR_net_last\"]=fundamental_df.swifter.apply(lambda x: fundamental_df.loc[x.name[0]].\\\n",
    "                                                                    xs(x.name[1],level=1)[\"EAR_net\"].iloc[0]\\\n",
    "                        if x.name[1] in list(fundamental_df.loc[x.name[0]][\"Next date\"]) else None,axis=1)\n",
    "        \n",
    "        fundamental_df[\"EAR_net_last_dummy\"]=(fundamental_df[\"EAR_net_last\"]>0)*1.0\n",
    "        \n",
    "        fundamental_df.to_csv(r'C:\\Users\\Eric.Li\\OneDrive\\Post result data cache\\{0}_reg_data.csv'.format(region))\n",
    "\n",
    "        #adjust reivision number\n",
    "        fundamental_df[\"post_revision\"]=fundamental_df[\"Revision_real\"]\n",
    "        \n",
    "        fundamental_df.loc[fundamental_df[\"post_revision\"]>=0.9,\"post_revision\"]=0.9\n",
    "        fundamental_df.loc[fundamental_df[\"post_revision\"]<=-0.9,\"post_revision\"]=-0.9\n",
    "\n",
    "        fundamental_df[\"post_revision_dummy\"]=(fundamental_df[\"post_revision\"]>0)*1.0\n",
    "\n",
    "        fundamental_df[\"post_revision_last\"]=fundamental_df.swifter.apply(lambda x: fundamental_df.loc[x.name[0]].\\\n",
    "                                                                          xs(x.name[1],level=1)[\"post_revision\"].iloc[0]\\\n",
    "                        if x.name[1] in list(fundamental_df.loc[x.name[0]][\"Next date\"]) else None,axis=1)\n",
    "        \n",
    "        fundamental_df[\"post_revision_last_dummy\"]=(fundamental_df[\"post_revision_last\"]>0)*1.0        \n",
    "        \n",
    "        fundamental_df[\"pre_revision\"]=fundamental_df.swifter.apply(lambda x: \\\n",
    "                                                                    revision_calc(x.name[0],x.name[1],EPS_df,\\\n",
    "                                                                                        (-41,40)),axis=1)\n",
    "        \n",
    "        fundamental_df.loc[fundamental_df[\"pre_revision\"]>=0.9,\"pre_revision\"]=0.9\n",
    "        fundamental_df.loc[fundamental_df[\"pre_revision\"]<=-0.9,\"pre_revision\"]=-0.9     \n",
    "        \n",
    "        fundamental_df[\"pre_revision_dummy\"]=(fundamental_df[\"pre_revision\"]>0)*1.0\n",
    "\n",
    "\n",
    "        fundamental_df[\"Market cap\"]=np.log(fundamental_df[\"Market cap\"])\n",
    "        \n",
    "        fundamental_df.to_csv(r'C:\\Users\\Eric.Li\\OneDrive\\Post result data cache\\{0}_reg_data.csv'.format(region))\n",
    "        # Stock momentum up to the start of EAR period\n",
    "        fundamental_df[\"stock_mom\"]=fundamental_df.swifter.apply(lambda x:return_calc_log(x.name[0],x.name[1],price_df,\\\n",
    "                                                                                          index_series,\\\n",
    "                                                                                       (-280+EAR_tuple[0],260),'abs'),axis=1)\n",
    "        \n",
    "        fundamental_df[\"stock_mom_short\"]=fundamental_df.swifter.apply(lambda x:\n",
    "                                                                 return_calc_log(x.name[0],x.name[1],price_df,\\\n",
    "                                                                                 index_series,(-20+EAR_tuple[0],20),'abs'),axis=1)\n",
    "        \n",
    "        fundamental_df.to_csv(r'C:\\Users\\Eric.Li\\OneDrive\\Post result data cache\\{0}_reg_data.csv'.format(region))\n",
    "        #beta\n",
    "        abs_return_df=price_df.diff(1,axis=1)/price_df.shift(1,axis=1)\n",
    "        \n",
    "        index_return_series=index_series/index_series.shift(1)-1\n",
    "        \n",
    "        fundamental_df[\"beta\"]=fundamental_df.swifter.apply(lambda x:\\\n",
    "                                                    calc_beta(x.name[0],x.name[1],abs_return_df,\\\n",
    "                                                              index_return_series,260,EAR_tuple[0]),axis=1)\n",
    "        \n",
    "        fundamental_df.to_csv(r'C:\\Users\\Eric.Li\\OneDrive\\Post result data cache\\{0}_reg_data.csv'.format(region))\n",
    "        \n",
    "        # macro variables\n",
    "        fundamental_df[\"yield_long\"]=fundamental_df.swifter.\\\n",
    "        apply(lambda x:index_df.loc[index_name[\"yield_long\"],x.name[1]],axis=1)\n",
    "        \n",
    "        fundamental_df[\"yield_short\"]=fundamental_df.swifter.\\\n",
    "        apply(lambda x:index_df.loc[index_name[\"yield_short\"],x.name[1]],axis=1)\n",
    "        \n",
    "        fundamental_df[\"yield_slope\"]=fundamental_df[\"yield_long\"]-fundamental_df[\"yield_short\"]\n",
    "        \n",
    "        fundamental_df[\"inflation\"]=fundamental_df.swifter.\\\n",
    "        apply(lambda x:index_df.loc[index_name[\"inflation\"],x.name[1]],axis=1)\n",
    "\n",
    "        fundamental_df[\"currency\"]=fundamental_df.swifter.\\\n",
    "        apply(lambda x:index_df.loc[index_name[\"currency\"],x.name[1]],axis=1)\n",
    "\n",
    "        fundamental_df[\"vix\"]=fundamental_df.swifter.\\\n",
    "        apply(lambda x:index_df.loc[index_name[\"vix\"],x.name[1]],axis=1)\n",
    "        \n",
    "        # commodity    \n",
    "        fundamental_df[\"gold\"]=fundamental_df.swifter.\\\n",
    "        apply(lambda x:index_df.loc[\"GOLDS Comdty\",x.name[1]],axis=1)        \n",
    "        \n",
    "        fundamental_df[\"oil\"]=fundamental_df.swifter.\\\n",
    "        apply(lambda x:index_df.loc[\"Cl1 Comdty\",x.name[1]],axis=1) \n",
    "        \n",
    "        fundamental_df[\"commodity\"]=fundamental_df.swifter.\\\n",
    "        apply(lambda x:index_df.loc[\"BWMING Index\",x.name[1]],axis=1)\n",
    "        \n",
    "        fundamental_df.to_csv(r'C:\\Users\\Eric.Li\\OneDrive\\Post result data cache\\{0}_reg_data.csv'.format(region))\n",
    "        \n",
    "        #momentum index\n",
    "        for s in MA_period_list: #calculate the MA on the end of EAR period\n",
    "            fundamental_df[\"momentum_long_MA_{0}\".format(s)]=fundamental_df.swifter.\\\n",
    "            apply(lambda x:MA_deviation(index_name[\"momentum_long\"],x.name[1],index_df,s,EAR_tuple[0]+EAR_tuple[1]-1),axis=1)\n",
    "\n",
    "            fundamental_df[\"momentum_short_MA_{0}\".format(s)]=fundamental_df.swifter.\\\n",
    "            apply(lambda x:MA_deviation(index_name[\"momentum_short\"],x.name[1],index_df,s,EAR_tuple[0]+EAR_tuple[1]-1),axis=1)\n",
    "        \n",
    "        fundamental_df.to_csv(r'C:\\Users\\Eric.Li\\OneDrive\\Post result data cache\\{0}_reg_data.csv'.format(region))\n",
    "        \n",
    "        for s in return_lookback_list:#calculate the return up to the end of EAR Period\n",
    "            fundamental_df[\"momentum_long_return_{0}\".format(s)]=fundamental_df.swifter.\\\n",
    "            apply(lambda x:return_calc_log(index_name[\"momentum_long\"],\\\n",
    "                                       x.name[1],index_df,index_df,(-s+EAR_tuple[0]+EAR_tuple[1]-1,\\\n",
    "                                                                    EAR_tuple[0]+EAR_tuple[1]-1),\"abs\"),axis=1)\n",
    "            \n",
    "            fundamental_df[\"momentum_short_return_{0}\".format(s)]=fundamental_df.swifter.\\\n",
    "            apply(lambda x:return_calc_log(index_name[\"momentum_short\"],\\\n",
    "                                       x.name[1],index_df,index_df,(-s+EAR_tuple[0]+EAR_tuple[1]-1,\\\n",
    "                                                                    EAR_tuple[0]+EAR_tuple[1]-1),\"abs\"),axis=1)\n",
    "        \n",
    "        # market\n",
    "        for s in MA_period_list:#calculate the MA on the end of EAR period\n",
    "            fundamental_df[\"market_MA_{0}\".format(s)]=fundamental_df.swifter.\\\n",
    "            apply(lambda x:MA_deviation(index_name[\"index\"],x.name[1],index_df,s,EAR_tuple[0]+EAR_tuple[1]-1),axis=1)\n",
    "        \n",
    "        fundamental_df.to_csv(r'C:\\Users\\Eric.Li\\OneDrive\\Post result data cache\\{0}_reg_data.csv'.format(region))\n",
    "        \n",
    "        for s in return_lookback_list:#calculate the return up to the end of EAR Period\n",
    "            fundamental_df[\"market_return_{0}\".format(s)]=fundamental_df.swifter.\\\n",
    "            apply(lambda x:return_calc_log(index_name[\"index\"],\\\n",
    "                                       x.name[1],index_df,index_df,(-s+EAR_tuple[0]+EAR_tuple[1]-1,\\\n",
    "                                                                    EAR_tuple[0]+EAR_tuple[1]-1),\"abs\"),axis=1)        \n",
    "            \n",
    "        # stock\n",
    "        for s in MA_period_list:#calculate the MA on the end of EAR period\n",
    "            fundamental_df[\"stock_MA_n_{0}\".format(s)]=fundamental_df.swifter.\\\n",
    "            apply(lambda x:MA_n_deviation(x.name[0],x.name[1],price_df,s,EAR_tuple[0]+EAR_tuple[1]-1,x[\"30d_vol\"]),axis=1)\n",
    "        \n",
    "        fundamental_df.to_csv(r'C:\\Users\\Eric.Li\\OneDrive\\Post result data cache\\{0}_reg_data.csv'.format(region))\n",
    "        \n",
    "        \n",
    "        for s in range_list:#calculate the range to the end of EAR Period\n",
    "            fundamental_df[\"stock_chart_position_{0}\".format(s)]=fundamental_df.swifter.\\\n",
    "            apply(lambda x:chart_position(x.name[0],x.name[1],price_df,s,EAR_tuple[0]+EAR_tuple[1]-1),axis=1) \n",
    "\n",
    "        fundamental_df.to_csv(r'C:\\Users\\Eric.Li\\OneDrive\\Post result data cache\\{0}_reg_data.csv'.format(region))\n",
    "        \n",
    "        \n",
    "        # Timestamp\n",
    "        fundamental_df[\"Date_timstamp\"]=fundamental_df.apply(lambda x:pd.Timestamp(x[\"Date\"]),axis=1)\n",
    "        fundamental_df.to_csv(r'C:\\Users\\Eric.Li\\OneDrive\\Post result data cache\\{0}_reg_data.csv'.format(region))\n",
    "    return fundamental_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Util function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revision_calc(ticker,date,EPS_df,period_tuple):\n",
    "    '''\n",
    "    Calculate percentage revision from the period tuple\n",
    "    EPS_df: EPS dataframe\n",
    "    period_tuple: (start,length), start is the offset and return includes that day, length is number of days\n",
    "    return calculation assumes enters on the price of the prior day to day when the count starts\n",
    "    '''\n",
    "    if type(date)==float:\n",
    "        return None\n",
    "    elif type(date)==pd.Timestamp:\n",
    "        date=date.strftime(\"%d/%b/%Y\")\n",
    "    elif type(date)==str:\n",
    "        date=date\n",
    "    \n",
    "    \n",
    "    date_series=EPS_df.columns.tolist()\n",
    "    \n",
    "    \n",
    "    if date in date_series and ticker in EPS_df.index: \n",
    "        eps_series=EPS_df.loc[ticker]\n",
    "        day0=date_series.index(date)\n",
    "        \n",
    "        if len(eps_series.iloc[:day0+period_tuple[0]].dropna())>0:\n",
    "        \n",
    "            if period_tuple[0]<0 and len(eps_series.loc[:date])-1<=abs(period_tuple[0]):\n",
    "                start=eps_series.dropna().iloc[0]\n",
    "            else:\n",
    "                start=eps_series.iloc[:day0+period_tuple[0]].dropna().iloc[-1]        \n",
    "\n",
    "            end=eps_series.iloc[:day0+period_tuple[0]+period_tuple[1]].dropna().iloc[-1]\n",
    "\n",
    "            if start!=0:\n",
    "                revision=(end-start)/abs(start)\n",
    "            else:\n",
    "                revision=None\n",
    "\n",
    "            target_revision=revision\n",
    "            return target_revision\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EPS_avg(ticker,date,EPS_df,period_tuple):\n",
    "    '''\n",
    "    Calculate average EPS for the period tuple\n",
    "    EPS_df: EPS dataframe\n",
    "    period_tuple: (start,length), start is the offset and return includes that day, length is number of days\n",
    "    return calculation assumes enters on the price of the prior day to day when the count starts\n",
    "    '''\n",
    "    if type(date)==float:\n",
    "        return None\n",
    "    elif type(date)==pd.Timestamp:\n",
    "        date=date.strftime(\"%d/%b/%Y\")\n",
    "    elif type(date)==str:\n",
    "        date=date\n",
    "    \n",
    "    \n",
    "    date_series=EPS_df.columns.tolist()\n",
    "    \n",
    "    \n",
    "    if date in date_series and ticker in EPS_df.index: \n",
    "        eps_series=EPS_df.loc[ticker]\n",
    "        day0=date_series.index(date)\n",
    "        \n",
    "        try:\n",
    "            if day0-1>-period_tuple[0] and not pd.isna(eps_series.iloc[day0+period_tuple[0]-1]) \\\n",
    "            and not pd.isna(eps_series.iloc[day0+period_tuple[0]+period_tuple[1]-1]) and \\\n",
    "            eps_series.iloc[day0+period_tuple[0]-1]!=0:#need to have value on both end\n",
    "                target_series=eps_series.iloc[day0+period_tuple[0]-1:day0+period_tuple[0]+period_tuple[1]]\n",
    "\n",
    "                eps_avg=target_series.mean()\n",
    "            else:\n",
    "                eps_avg=None\n",
    "        except IndexError:#in case the end of the target return is beyond the end of the series\n",
    "            eps_avg=None\n",
    "    else:\n",
    "        eps_avg=None\n",
    "    return eps_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_calc(ticker,date,price_df,index_series,period_tuple,abs_rel):\n",
    "    '''\n",
    "    Calculate perccentage return from the period tuple\n",
    "    abs_rel: bool, abs_return if assigned abs\n",
    "    period_tuple: (start,length), start is the offset and return includes that day, length is number of days\n",
    "    return calculation assumes enters on the price of the prior day to day when the count starts\n",
    "    '''\n",
    "    if type(date)==float:\n",
    "        return None\n",
    "    elif type(date)==pd.Timestamp:\n",
    "        date=date.strftime(\"%d/%b/%Y\")\n",
    "    elif type(date)==str:\n",
    "        date=date\n",
    "    \n",
    "    date_series=price_df.columns.tolist()\n",
    "    index_date_series=index_series.index.tolist()\n",
    "    \n",
    "    if date in date_series and ticker in price_df.index:\n",
    "        \n",
    "        price_series=price_df.loc[ticker]\n",
    "        \n",
    "        day0=date_series.index(date)\n",
    "        \n",
    "        try:\n",
    "            if day0-1>-period_tuple[0] and not pd.isna(price_series.iloc[day0+period_tuple[0]-1]) \\\n",
    "            and not pd.isna(price_series.iloc[day0+period_tuple[0]+period_tuple[1]-1]) and \\\n",
    "            price_series.iloc[day0+period_tuple[0]-1]!=0:#need to have value on both end\n",
    "                target_series=price_series.iloc[day0+period_tuple[0]-1:day0+period_tuple[0]+period_tuple[1]]\n",
    "\n",
    "                abs_return=target_series[-1]/target_series[0]-1\n",
    "\n",
    "                if abs_rel==\"abs\":\n",
    "                    target_return=abs_return\n",
    "                else:\n",
    "                    day0_index=index_date_series.index(date)\n",
    "\n",
    "                    if not pd.isna(index_series.iloc[day0_index+period_tuple[0]-1]) \\\n",
    "                    and not pd.isna(index_series.iloc[day0_index+period_tuple[0]-1])\\\n",
    "                    and index_series.iloc[day0_index+period_tuple[0]-1]!=0:\n",
    "                        target_index_series=index_series.iloc[day0_index+period_tuple[0]-1:\\\n",
    "                                                                     day0_index+period_tuple[0]+period_tuple[1]]\n",
    "                        index_return=target_index_series.iloc[-1]/target_index_series.iloc[0]-1\n",
    "                        target_return=abs_return-index_return\n",
    "                    else:\n",
    "                        target_return=None\n",
    "            else:\n",
    "                target_return=None\n",
    "        except IndexError:#in case the end of the target return is beyond the end of the series\n",
    "            target_return=None\n",
    "    else:\n",
    "        target_return=None\n",
    "    \n",
    "    return target_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_calc_log(ticker,date,price_df,index_series,period_tuple,abs_rel):\n",
    "    '''\n",
    "    Calculate log return from the period tuple\n",
    "    abs_rel: bool, abs_return if assigned abs\n",
    "    period_tuple: (start,length), start is the offset and return includes that day, length is number of days\n",
    "    return calculation assumes enters on the price of the prior day to day when the count starts\n",
    "    '''\n",
    "    if type(date)==float:\n",
    "        return None\n",
    "    elif type(date)==pd.Timestamp:\n",
    "        date=date.strftime(\"%d/%b/%Y\")\n",
    "    elif type(date)==str:\n",
    "        date=date\n",
    "    \n",
    "    date_series=price_df.columns.tolist()\n",
    "    index_date_series=index_series.index.tolist()\n",
    "    \n",
    "    if date in date_series and ticker in price_df.index:\n",
    "        \n",
    "        price_series=price_df.loc[ticker]\n",
    "        \n",
    "        day0=date_series.index(date)\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            if day0-1>-period_tuple[0] and not pd.isna(price_series.iloc[day0+period_tuple[0]-1]) \\\n",
    "            and not pd.isna(price_series.iloc[day0+period_tuple[0]+period_tuple[1]-1]): #need to have value on both end\n",
    "                target_series=np.log(price_series.iloc[day0+period_tuple[0]-1:day0+period_tuple[0]+period_tuple[1]])\n",
    "                abs_return=target_series[-1]-target_series[0]\n",
    "\n",
    "                if abs_rel==\"abs\":\n",
    "                    target_return=abs_return\n",
    "                else:\n",
    "                    day0_index=index_date_series.index(date)\n",
    "\n",
    "                    if not pd.isna(index_series.iloc[day0_index+period_tuple[0]-1]) \\\n",
    "                    and not pd.isna(index_series.iloc[day0_index+period_tuple[0]-1]):\n",
    "                        target_index_series=np.log(index_series.iloc[day0_index+period_tuple[0]-1:\\\n",
    "                                                                     day0_index+period_tuple[0]+period_tuple[1]])\n",
    "                        index_return=target_index_series.iloc[-1]-target_index_series.iloc[0]\n",
    "                        target_return=abs_return-index_return\n",
    "                    else:\n",
    "                        target_return=None\n",
    "            else:\n",
    "                target_return=None\n",
    "        except IndexError: #in case the end of the target return is beyond the end of the series\n",
    "            target_return=None\n",
    "    else:\n",
    "        target_return=None\n",
    "    \n",
    "    return target_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revision_vol(ticker,date,EPS_df,period):\n",
    "    '''\n",
    "    calculate vol of revision\n",
    "    '''\n",
    "    if type(date)==float:\n",
    "        return None\n",
    "    elif type(date)==pd.Timestamp:\n",
    "        date=date.strftime(\"%d/%b/%Y\")\n",
    "    elif type(date)==str:\n",
    "        date=date\n",
    "    \n",
    "    revision_df=EPS_df.diff(1,axis=1)/EPS_df.shift(1,axis=1)\n",
    "    revision_series=revision_df.loc[ticker].dropna()\n",
    "    date_series=revision_series.index.tolist()\n",
    "    \n",
    "    if date in date_series:\n",
    "        day0=date_series.index(date)\n",
    "\n",
    "        if len(revision_series.iloc[:day0].dropna())<period:\n",
    "            target_revision=revision_series.iloc[:day0]\n",
    "        else:\n",
    "            target_revision=revision_series.iloc[day0-period:day0]       \n",
    "        return target_revision.std()\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quarter_transform(quarter):\n",
    "    '''\n",
    "    Take the raw quarter to Q1 to Q4\n",
    "    '''\n",
    "    if type(quarter)==float:\n",
    "        adj_quarter=None\n",
    "    else:\n",
    "        \n",
    "        if quarter[-2:]=='Q4' or quarter[-2:]==':A':\n",
    "            adj_quarter='Q4'\n",
    "        elif quarter[-2:]=='Q3' or quarter[-2:]=='C3':\n",
    "            adj_quarter='Q3'\n",
    "        elif quarter[-2:]=='Q2' or quarter[-2:]=='C2' or quarter[-2:]=='S1':\n",
    "            adj_quarter='Q2'\n",
    "        elif quarter[-2:]=='Q1' or quarter[-2:]=='C1':\n",
    "            adj_quarter='Q1'\n",
    "        else:\n",
    "            adj_quarter=None\n",
    "    return adj_quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_beta(ticker,date,return_df,index_df,lookback,offset):\n",
    "    '''\n",
    "    calculate beta for individual stocks\n",
    "    index_df: index return \n",
    "    the end index is -1 as we need to exclude the return on the earning day\n",
    "    offset adjust the final date for calculation\n",
    "    if history is less than lookback, return None\n",
    "    '''\n",
    "    if type(date)==float:\n",
    "        return None\n",
    "    elif type(date)==pd.Timestamp:\n",
    "        date=date.strftime(\"%d/%b/%Y\")\n",
    "    elif type(date)==str:\n",
    "        date=date    \n",
    "    \n",
    "    date_series=return_df.columns.tolist()\n",
    "    index_date_series=index_df.index.tolist()\n",
    "    \n",
    "    if date in date_series and date in index_date_series and ticker in return_df.index:     \n",
    "    \n",
    "        return_series=return_df.loc[ticker]\n",
    "        day0=date_series.index(date)  \n",
    "        day0_index=index_date_series.index(date)\n",
    "        \n",
    "        if day0+1>=lookback and day0_index+1>=lookback and len(return_series.iloc[:day0+offset+1].dropna())>0 and \\\n",
    "        len(return_series.iloc[day0-lookback+offset+1:day0+offset+1].dropna())==\\\n",
    "        len(index_df.iloc[day0_index-lookback+offset+1:day0_index+offset+1].dropna()):\n",
    "            cov_matrix=np.cov(return_series.iloc[day0-lookback+offset+1:day0+offset+1],\\\n",
    "                              index_df.iloc[day0_index-lookback+offset+1:day0_index+offset+1])\n",
    "            \n",
    "\n",
    "            beta=cov_matrix[0][1]/cov_matrix[1][1]\n",
    "        else:\n",
    "            beta=None\n",
    "    else:\n",
    "        beta=None\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MA_n_deviation(ticker,date,price_df,lookback,offset,vol):\n",
    "    ''' \n",
    "    The function calculate the number of daily deviation from certain moving average\n",
    "    date - transform to the string below\n",
    "    price_df - price dataframe used to calculate the moving average\n",
    "    lookback: business days\n",
    "    vol - last 30 day volatility in daily form\n",
    "    offset:int, use it as an addition to the date variable to base the target price\n",
    "    if history is less than the lookback, return None\n",
    "    '''\n",
    "    \n",
    "    if type(date)==float:\n",
    "        return None\n",
    "    elif type(date)==pd.Timestamp:\n",
    "        date=date.strftime(\"%d/%b/%Y\")\n",
    "    elif type(date)==str:\n",
    "        date=date    \n",
    "        \n",
    "    date_series=price_df.columns.tolist()\n",
    "\n",
    "    if date in date_series and ticker in price_df.index: \n",
    "        \n",
    "        price_series=price_df.loc[ticker]\n",
    "        day0=date_series.index(date)\n",
    "        \n",
    "        if day0+1>=lookback and len(price_series.iloc[:day0+offset+1].dropna())>0 and not pd.isna(price_series.iloc[day0+offset]):\n",
    "            MA=price_series.iloc[day0-lookback+offset+1:day0+offset+1] \n",
    "            MA_n=(MA.iloc[-1]/MA.mean()-1)/vol \n",
    "        else:\n",
    "            MA_n=None\n",
    "    else:\n",
    "        MA_n=None\n",
    "    return MA_n\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MA_deviation(ticker,date,price_df,lookback,offset):\n",
    "    ''' \n",
    "    The function calculate the percentage deviation from certain moving average\n",
    "    date - transform to the string below\n",
    "    price_df - price dataframe used to calculate the moving average\n",
    "    lookback: business days\n",
    "    vol - last 30 day volatility in daily form\n",
    "    offset:int, use it as an addition to the date variable to base the target price\n",
    "    if history is less than the lookback, return None\n",
    "    '''\n",
    "    \n",
    "    if type(date)==float:\n",
    "        return None\n",
    "    elif type(date)==pd.Timestamp:\n",
    "        date=date.strftime(\"%d/%b/%Y\")\n",
    "    elif type(date)==str:\n",
    "        date=date    \n",
    "        \n",
    "    date_series=price_df.columns.tolist()\n",
    "\n",
    "    if date in date_series and ticker in price_df.index: \n",
    "        \n",
    "        price_series=price_df.loc[ticker]\n",
    "        day0=date_series.index(date)\n",
    "        \n",
    "        if day0+1>=lookback and len(price_series.iloc[:day0+offset+1].dropna())>0 and not pd.isna(price_series.iloc[day0+offset]):\n",
    "            MA=price_series.iloc[day0-lookback+offset+1:day0+offset+1] \n",
    "            MA_ret=MA.iloc[-1]/MA.mean()-1\n",
    "        else:\n",
    "            MA_ret=None\n",
    "    else:\n",
    "        MA_ret=None\n",
    "    return MA_ret\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chart_position(ticker,date,price_df,lookback,offset):\n",
    "    '''\n",
    "    The function calculate the position in a range, 100% being high, 0% being low\n",
    "    date - transform to the string below\n",
    "    price_df - price dataframe used to calculate the range\n",
    "    lookback: business days\n",
    "    offset:int, use it as an addition to the date variable to base the target price    \n",
    "    if history is less than the lookback, return None\n",
    "    '''\n",
    "    if type(date)==float:\n",
    "        return None\n",
    "    elif type(date)==pd.Timestamp:\n",
    "        date=date.strftime(\"%d/%b/%Y\")\n",
    "    elif type(date)==str:\n",
    "        date=date    \n",
    "        \n",
    "    date_series=price_df.columns.tolist()\n",
    "\n",
    "    if date in date_series and ticker in price_df.index: \n",
    "        \n",
    "        price_series=price_df.loc[ticker]\n",
    "        day0=date_series.index(date)\n",
    "        \n",
    "        if day0+1>=lookback and len(price_series.iloc[:day0+offset+1].dropna())>0 and not pd.isna(price_series.iloc[day0+offset]):\n",
    "            target_range=price_series.iloc[day0-lookback+offset+1:day0+offset+1] \n",
    "            position=(target_range.iloc[-1]-target_range.min())/(target_range.max()-target_range.min())\n",
    "        else:\n",
    "            position=None\n",
    "    else:\n",
    "        position=None\n",
    "    return position    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EAR_calc(ticker,date,return_df,EAR_period,vol_lookback):\n",
    "    '''\n",
    "    Calculate EAR from ticker and reference date\n",
    "    '''\n",
    "    if type(date)==float:\n",
    "        return None\n",
    "    elif type(date)==pd.Timestamp:\n",
    "        date=date.strftime(\"%d/%b/%Y\")\n",
    "    elif type(date)==str:\n",
    "        date=date\n",
    "    return_series=return_df.loc[ticker].dropna()\n",
    "    date_series=return_series.index.tolist()\n",
    "    if date in date_series:\n",
    "        day0=date_series.index(date)\n",
    "        post_series=return_series.iloc[day0:]\n",
    "        pre_series=return_series.iloc[:day0]\n",
    "        vol= return_series.iloc[day0-min(len(pre_series),vol_lookback+1):day0].std()\n",
    "        ret=(return_series.iloc[day0:day0+EAR_period]+1).prod()-1\n",
    "        nmove=ret/vol\n",
    "        return nmove\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_vol(signal_column,return_df,vol_lookback):\n",
    "    '''\n",
    "    Calculate simple vol from signal tuple\n",
    "    '''\n",
    "    signal_series=return_df.loc[signal_column.name[0]]\n",
    "    location=signal_series.index.tolist().index(signal_column.name[1])\n",
    "    vol_range=min(vol_lookback,len(signal_series[:location]))\n",
    "    signal_vol=signal_series[location-vol_range-1:location].std()\n",
    "    return signal_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_vol(date,index_df,vol_lookback):\n",
    "    '''\n",
    "    Calculate simple vol from signal tuple\n",
    "    '''\n",
    "    location=index_df.index.tolist().index(date)\n",
    "    vol_range=min(vol_lookback,len(index_df.iloc[:location]))\n",
    "    signal_vol=index_df.iloc[location-vol_range-1:location].std()\n",
    "    return signal_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_universe(signal_df,start_datetime,end_datetime,old_position):\n",
    "    '''\n",
    "    Slice the signal_df, both the index and entry date have to be \n",
    "    '''\n",
    "    \n",
    "\n",
    "    signal_df=signal_df.loc[start_datetime:end_datetime]\n",
    "    \n",
    "    if old_position is True:  \n",
    "        adj_signal_df=signal_df\n",
    "    else:\n",
    "        entry=signal_df.apply(lambda x:datetime.strptime(x.name[1],\"%d/%b/%Y\"),axis=0)\n",
    "        period_evaluate=(entry>=start_datetime)&(entry<=end_datetime)\n",
    "        adj_signal_df=signal_df.loc[:,period_evaluate]\n",
    "    \n",
    "    \n",
    "    zero_index=pd.Series(1,index=pd.date_range(start_datetime,end_datetime,freq='B')).to_frame()\n",
    "    adj_signal_df=pd.concat([adj_signal_df.drop_duplicates(),zero_index],axis=1).iloc[:,:-1]\n",
    "    return adj_signal_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_filter_stop(signal_df,stop_level,return_df,vol_lookback,stop_type,index_df):\n",
    "    '''\n",
    "    Input - signal_df\n",
    "    Get the updated signal df after the stop loss\n",
    "    stop_type:abs,rel\n",
    "    \n",
    "    '''\n",
    "    if stop_type=='abs':\n",
    "        vol_row=signal_df.apply(lambda column:signal_vol(column,return_df,vol_lookback),axis=0)\n",
    "        signal_cum_nmove=((1+signal_df).cumprod()-1).ffill()/vol_row\n",
    "        signal_df_stop=signal_df[-(signal_cum_nmove.expanding().min().shift(1,axis=0)<-stop_level)]\n",
    "    elif stop_type=='rel':\n",
    "        if index_df.shape[1]==1:\n",
    "            signal_count=signal_df.copy()\n",
    "            signal_count[((signal_count)>0) | ((signal_count)<0)]=1.0\n",
    "            signal_hedge=signal_count.apply(lambda x:x.multiply(index_df.iloc[:,0],axis=0))\n",
    "            \n",
    "            vol_row=signal_df.apply(lambda x:signal_vol(x,return_df,vol_lookback),axis=0)\n",
    "            rel_signal_cum_nmove=((1+signal_df).cumprod()-(1+signal_hedge).cumprod()).ffill()/vol_row\n",
    "            signal_df_stop=signal_df[-(rel_signal_cum_nmove.expanding().min().shift(1,axis=0)<-stop_level)]\n",
    "        else:\n",
    "            signal_count=signal_df.copy()\n",
    "            signal_count[((signal_count)>0) | ((signal_count)<0)]=1.0\n",
    "            signal_hedge=signal_count.apply(lambda x:x.multiply(index_df[Asia_mapping.loc[x.name[0][-2:]].iloc[0]],axis=0))\n",
    "            \n",
    "            vol_row=signal_df.apply(lambda x:signal_vol(x,return_df,vol_lookback),axis=0)\n",
    "            rel_signal_cum_nmove=((1+signal_df).cumprod()-(1+signal_hedge).cumprod()).ffill()/vol_row\n",
    "            signal_df_stop=signal_df[-(rel_signal_cum_nmove.expanding().min().shift(1,axis=0)<-stop_level)]            \n",
    "            \n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    return signal_df_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revision_adjusted_size(reference_signal_df,lower_revision,higher_revision,size_multiple,revision_row,revision_row_reference,\\\n",
    "                           gross,long):\n",
    "    \n",
    "    ''' \n",
    "    Use positive size\n",
    "    '''\n",
    "    \n",
    "    lower_size=0.01\n",
    "    higher_size=lower_size*size_multiple\n",
    "\n",
    "    if long is True:\n",
    "        size_row_reference=revision_row_reference.to_frame().copy().apply(lambda x: lower_size+(higher_size-lower_size)\\\n",
    "                                                      *(np.abs(x.iloc[0]-lower_revision))/np.abs(higher_revision-lower_revision) \\\n",
    "                                                      if np.abs(x.iloc[0])<=np.abs(higher_revision) else higher_size,axis=1)\n",
    "\n",
    "        size_df_reference=(1+reference_signal_df).cumprod()*size_row_reference\n",
    "\n",
    "        trial_gross=np.abs(size_df_reference.sum(axis=1).mean())\n",
    "        new_lower_size=lower_size/(trial_gross*100/gross)\n",
    "        new_higher_size=higher_size/(trial_gross*100/gross)\n",
    "\n",
    "        size_row=revision_row.to_frame().copy().apply(lambda x: new_lower_size+(new_higher_size-new_lower_size)\\\n",
    "                                                      *(np.abs(x.iloc[0]-lower_revision))/np.abs(higher_revision-lower_revision) \\\n",
    "                                                      if np.abs(x.iloc[0])<=np.abs(higher_revision) else new_higher_size,axis=1)\n",
    "    else:\n",
    "        size_row_reference=revision_row_reference.to_frame().copy().apply(lambda x: lower_size+(higher_size-lower_size)\\\n",
    "                                                      *(np.abs(x.iloc[0]-lower_revision))/np.abs(higher_revision-lower_revision) \\\n",
    "                                                      if np.abs(x.iloc[0])<=np.abs(lower_revision) else higher_size,axis=1)\n",
    "\n",
    "        size_df_reference=(1-reference_signal_df).cumprod()*size_row_reference\n",
    "\n",
    "        trial_gross=np.abs(size_df_reference.sum(axis=1).mean())\n",
    "        new_lower_size=lower_size/(trial_gross*100/gross)\n",
    "        new_higher_size=higher_size/(trial_gross*100/gross)\n",
    "\n",
    "        size_row=revision_row.to_frame().copy().apply(lambda x: new_lower_size+(new_higher_size-new_lower_size)\\\n",
    "                                                      *(np.abs(x.iloc[0]-lower_revision))/np.abs(higher_revision-lower_revision) \\\n",
    "                                                      if np.abs(x.iloc[0])<=np.abs(lower_revision) else new_higher_size,axis=1)\n",
    "\n",
    "    return size_row, new_lower_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sizing(signal_df,reference_signal_df,gross,fundamental_df,new_signal,return_df,risk_parity,liquidity,capital,\\\n",
    "           revision_adjust,long):\n",
    "    '''\n",
    "    Use historical signal_df range to calculate the size row for the current signal_df range\n",
    "    Idea is to use historical as a benchmark for future sizing\n",
    "    '''\n",
    "    \n",
    "    fundamental_df=fundamental_df.copy().sort_index()\n",
    "    vol_reference=reference_signal_df.apply(lambda x:signal_vol(x,return_df,30),axis=0).mean()\n",
    "    vol_row=signal_df.apply(lambda x:signal_vol(x,return_df,30),axis=0)\n",
    "    \n",
    "    '''\n",
    "    Revision row needs to be updated using reference_signal\n",
    "    new sizing scheme is a linear function\n",
    "    revision_adjust=(True/False,lower_revision,higher_revision,size_multiple)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    if revision_adjust[0] is True:\n",
    "        \n",
    "        if new_signal is True:     \n",
    "            revision_row_reference=fundamental_df.loc[reference_signal_df.columns][\"Revision_real\"]\n",
    "            revision_row=fundamental_df.loc[signal_df.columns][\"Revision_real\"]\n",
    "        else:\n",
    "            revision_row_reference=fundamental_df.loc[reference_signal_df.columns][\"Revision_20\"]\n",
    "            revision_row=fundamental_df.loc[signal_df.columns][\"Revision_20\"]           \n",
    "        \n",
    "        if long is True and revision_adjust[1] is not None:\n",
    "            lower_revision=revision_adjust[1][0]\n",
    "            higher_revision=revision_adjust[1][1]\n",
    "            size_multiple=revision_adjust[3]\n",
    "        \n",
    "            base_size,low_size=revision_adjusted_size(reference_signal_df,lower_revision,higher_revision,size_multiple,\\\n",
    "                                                      revision_row,revision_row_reference,gross,True)\n",
    "            if risk_parity is True:\n",
    "                size_row=signal_df.apply(lambda x: min(base_size[x.name]/(vol_row[x.name]/vol_reference),\\\n",
    "                                                       fundamental_df.loc[x.name[0],x.name[1]][\"Volume\"].iloc[0]*\\\n",
    "                                                       liquidity/capital),axis=0)\n",
    "            else:\n",
    "                size_row=signal_df.apply(lambda x: min(base_size[x.name], fundamental_df.loc[x.name[0],x.name[1]][\"Volume\"].iloc[0]\\\n",
    "                                                       *liquidity/capital),axis=0)            \n",
    "\n",
    "        elif long is False and revision_adjust[2] is not None:\n",
    "            lower_revision=revision_adjust[2][0]\n",
    "            higher_revision=revision_adjust[2][1]\n",
    "            size_multiple=revision_adjust[3]\n",
    "        \n",
    "            base_size,low_size=revision_adjusted_size(reference_signal_df,lower_revision,higher_revision,size_multiple,revision_row,\\\n",
    "                                             revision_row_reference,gross,False) \n",
    "\n",
    "            if risk_parity is True:\n",
    "                size_row=signal_df.apply(lambda x: min(base_size[x.name]/(vol_row[x.name]/vol_reference),\\\n",
    "                                                       fundamental_df.loc[x.name[0],x.name[1]][\"Volume\"].iloc[0]*\\\n",
    "                                                       liquidity/capital),axis=0)\n",
    "            else:\n",
    "                size_row=signal_df.apply(lambda x: min(base_size[x.name], fundamental_df.loc[x.name[0],x.name[1]][\"Volume\"].iloc[0]\\\n",
    "                                                       *liquidity/capital),axis=0)\n",
    "\n",
    "        else:\n",
    "            size_row=None\n",
    "            low_size=None\n",
    "\n",
    "    elif revision_adjust[0]=='constant':\n",
    "        if risk_parity is True:\n",
    "            size_row=signal_df.apply(lambda x: min(revision_adjust[1]/(vol_row[x.name]/vol_reference),\\\n",
    "                                                   fundamental_df.loc[x.name[0],x.name[1]][\"Volume\"].iloc[0]*\\\n",
    "                                                   liquidity/capital),axis=0)\n",
    "        else:\n",
    "            size_row=signal_df.apply(lambda x: min(revision_adjust[1], fundamental_df.loc[x.name[0],x.name[1]][\"Volume\"].iloc[0]\\\n",
    "                                                   *liquidity/capital),axis=0)\n",
    "        low_size=None\n",
    "\n",
    "    else:\n",
    "        number=reference_signal_df.count(axis=1).mean()\n",
    "        avg_size=gross/100/number\n",
    "\n",
    "        if risk_parity is True:\n",
    "            size_row=signal_df.apply(lambda x: min(avg_size/(vol_row[x.name]/vol_reference),\\\n",
    "                                                   fundamental_df.loc[x.name[0],x.name[1]][\"Volume\"].iloc[0]*\\\n",
    "                                                   liquidity/capital),axis=0)\n",
    "        else:\n",
    "            size_row=signal_df.apply(lambda x: min(avg_size, fundamental_df.loc[x.name[0],x.name[1]][\"Volume\"].iloc[0]\\\n",
    "                                                   *liquidity/capital),axis=0)\n",
    "        low_size=None\n",
    "    return size_row,low_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trading_analytics_date(portfolio_cache):\n",
    "    '''\n",
    "    Key portfolio metrics from portfolio cache\n",
    "    Feed into plot function\n",
    "    '''\n",
    "    \n",
    "    ind_return=portfolio_cache[3]\n",
    "    signal_count=len(ind_return)\n",
    "    account_curve=portfolio_cache[1]\n",
    "    \n",
    "    if signal_count==0:\n",
    "        return None,None,None,None,None,None,None\n",
    "    else:\n",
    "        mean_return=ind_return.mean()\n",
    "        hit_rate=len(ind_return[ind_return>0])/len(ind_return)*1.0\n",
    "        payoff_ratio=ind_return[ind_return>0].mean()/ind_return[ind_return<0].mean()*-1.0\n",
    "        \n",
    "        account_price=account_curve+1\n",
    "        ann_vol=account_curve.diff(1).std()*(260**0.5)\n",
    "        ann_ret=account_curve.diff(1).mean()*260 #we only make money on constant capital\n",
    "        ann_sharpe=ann_ret/ann_vol\n",
    "        \n",
    "        max_dd=-((1+account_curve)-(1+account_curve).cummax(axis=0)).expanding().min().min()\n",
    "        \n",
    "        #low_date=(np.maximum.accumulate(account_curve)-account_curve).idxmax()\n",
    "        #high_date=account_curve[:low_date].idxmax()\n",
    "        #max_dd=1-(1+account_curve[low_date])/(1+account_curve[high_date])\n",
    "        \n",
    "        return signal_count,hit_rate,payoff_ratio,ann_ret,ann_vol,ann_sharpe,max_dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trading_analytics_simp(account_curve):\n",
    "    '''\n",
    "    Key portfolio metrics from portfolio account curve\n",
    "    Only sharpe and drawdown\n",
    "    '''\n",
    "\n",
    "\n",
    "    account_price=account_curve+1\n",
    "    ann_vol=acciybt_curve.diff(1).std()*(260**0.5)\n",
    "    ann_ret=account_curve.diff(1).mean()*260 #we only make money on constant capital\n",
    "    ann_sharpe=ann_ret/ann_vol\n",
    "\n",
    "    max_dd=-((1+account_curve)-(1+account_curve).cummax(axis=0)).expanding().min().min()\n",
    "\n",
    "    #low_date=(np.maximum.accumulate(account_curve)-account_curve).idxmax()\n",
    "    #high_date=account_curve[:low_date].idxmax()\n",
    "    #max_dd=1-(1+account_curve[low_date])/(1+account_curve[high_date])\n",
    "\n",
    "    return ann_sharpe,max_dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_signal(title,figsize,portfolio_cache):\n",
    "\n",
    "    account_curve=portfolio_cache[1]\n",
    "    avg_size=np.abs(portfolio_cache[2]).mean(axis=0).mean()\n",
    "    ind_return=portfolio_cache[3]\n",
    "    gross=portfolio_cache[4]\n",
    "    turnover=portfolio_cache[5]\n",
    "\n",
    "    fig=plt.figure(figsize=figsize)\n",
    "    ax1=fig.add_subplot(1,1,1)\n",
    "    ln1=ax1.plot(account_curve,label='signal',color='b')\n",
    "\n",
    "    val1=ax1.get_yticks()\n",
    "    start=val1[0]\n",
    "    end=val1[-1]\n",
    "    ax1.set_yticks(np.arange(start,end,0.1))  \n",
    "    adj_val1=ax1.get_yticks()\n",
    "    ax1.set_yticklabels([\"{:.1%}\".format(x) for x in adj_val1])\n",
    "\n",
    "    ax2=ax1.twinx()\n",
    "    ln2=ax2.plot(gross,label='gross',color='silver')\n",
    "\n",
    "    val2=ax2.get_yticks()\n",
    "    start=val2[0]\n",
    "    end=val2[-1]\n",
    "    ax2.set_yticks(np.arange(start,end,0.3))  \n",
    "    adj_val2=ax2.get_yticks()\n",
    "    ax2.set_yticklabels([\"{:.0%}\".format(x) for x in adj_val2])\n",
    "\n",
    "    count,hit,payoff,ret,vol,sharpe,max_dd=trading_analytics_date(portfolio_cache)\n",
    "\n",
    "    plt.title(\"\\n\".join(textwrap.wrap('count='+str(count)+\n",
    "                             ',avg_size='+str(\"{:.1%}\".format(avg_size))+\n",
    "                             ',hit_rate='+str(\"{:.0%}\".format(hit))+\n",
    "                             ',payoff='+str(round(payoff,1))+\n",
    "                             ',return='+str(\"{:.1%}\".format(ret))+\n",
    "                             ',vol='+str(\"{:.1%}\".format(vol))+\n",
    "                             ',sharpe='+str(round(sharpe,1))+\n",
    "                             ',turnover='+str(round(turnover,1))+'x'+                             \n",
    "                             ',max_drawdown='+str(\"{:.1%}\".format(max_dd)))),fontsize=10)\n",
    "\n",
    "    ax1.set_xlabel('Year')\n",
    "    ax1.set_ylabel('Return')\n",
    "    ax2.set_ylabel('Exposure')\n",
    "    plt.suptitle(title,y=1.05,fontsize=16)\n",
    "    plt.grid(linestyle='dashed')\n",
    "    plt.legend(ln1+ln2,[l.get_label() for l in ln1+ln2],loc=2)\n",
    "    ax1.axhline(y=0,color='k')\n",
    "\n",
    "    plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_base(fundamental_df,long_criteria_dict,short_criteria_dict,EAR_tuple):\n",
    "    '''\n",
    "    class to produce long and short signals to trade\n",
    "    fundamental_df - dataframe to conduct the filtering\n",
    "    long_criteria_dict - {variable_name;condition tuple}\n",
    "    EAR_tuple - if EAR_n is available, then (price_df,index_series,start,length,abs_rel)\n",
    "    '''\n",
    "    long_base=fundamental_df.copy()\n",
    "    short_base=fundamental_df.copy()\n",
    "    \n",
    "    if long_criteria_dict is not None:\n",
    "        for i in long_criteria_dict.items():\n",
    "            if i[0]==\"EAR_n\":\n",
    "                EAR=long_base.swifter.\\\n",
    "                apply(lambda x:return_calc(x.name[0],x.name[1],EAR_tuple[0],EAR_tuple[1],(EAR_tuple[2],EAR_tuple[3]),\\\n",
    "                                           EAR_tuple[4])if x.name[0] in EAR_tuple[0].index \\\n",
    "                      else None,axis=1)\n",
    "                \n",
    "                EAR_n=long_base.swifter.\\\n",
    "                apply(lambda x:EAR.loc[x.name]/x[\"30d_vol\"] if x[\"30d_vol\"]!=0 and EAR.loc[x.name] is not None \\\n",
    "                      else None,axis=1)\n",
    "                \n",
    "                long_base=long_base[(EAR_n>=i[1][0]) & (EAR_n<=i[1][1])]\n",
    "            else:\n",
    "                long_base=long_base[(long_base[i[0]]>=i[1][0]) & (long_base[i[0]]<=i[1][1])]\n",
    "                \n",
    "    else:\n",
    "        long_base=None\n",
    "        \n",
    "    if short_criteria_dict is not None:\n",
    "        for i in short_criteria_dict.items():\n",
    "            if i[0]==\"EAR_n\":\n",
    "                EAR_short=short_base.swifter.\\\n",
    "                apply(lambda x:return_calc(x.name[0],x.name[1],EAR_tuple[0],EAR_tuple[1],(EAR_tuple[2],EAR_tuple[3]),\\\n",
    "                                           EAR_tuple[4])if x.name[0] in EAR_tuple[0].index \\\n",
    "                      else None,axis=1)\n",
    "                \n",
    "                EAR_n_short=short_base.swifter.\\\n",
    "                apply(lambda x:EAR_short.loc[x.name]/x[\"30d_vol\"] if x[\"30d_vol\"]!=0 and EAR_short.loc[x.name] is not None \\\n",
    "                      else None,axis=1)\n",
    "                                \n",
    "                short_base=short_base[(EAR_n_short>=i[1][0]) & (EAR_n_short<=i[1][1])]\n",
    "            else:\n",
    "                short_base=short_base[(short_base[i[0]]>=i[1][0]) & (short_base[i[0]]<=i[1][1])]\n",
    "                \n",
    "    else:\n",
    "        short_base=None    \n",
    "    \n",
    "    return long_base,short_base\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class signal(object):\n",
    "    '''\n",
    "    Signal class is built to initialize the signal_df and account curve from base parameters \n",
    "    '''\n",
    "    def __init__(self,fundamental_df,price_df,close_return_df,entry,base,holding,start,end,\\\n",
    "                 old_position,new_signal,revision_adjust,early_exit):\n",
    "        '''\n",
    "        Define the key free parameters of the signal\n",
    "        Criteria:(EAR,revision,size)\n",
    "        revision_adjust:(True/False,(long lower revision,long higher revision),(short low abs revision,short high abs_revision)\\\n",
    "        ,size_multiple)\n",
    "        Entry: 1 means enter on the earning day, 2 means enters on whatever price given on the day after earning\n",
    "        '''\n",
    "        self.base=base\n",
    "        self.long_base=base[0]\n",
    "        self.short_base=base[1]\n",
    "        self.fundamental_df=fundamental_df\n",
    "        self.price_df=price_df\n",
    "        self.abs_return=price_df.diff(1,axis=1)/price_df.shift(1,axis=1)\n",
    "        self.close_return_df=close_return_df\n",
    "        #self.EAR_period=EAR_period\n",
    "        self.entry=entry\n",
    "#         self.long_criteria=long_criteria\n",
    "#         self.short_criteria=short_criteria\n",
    "        self.holding=holding\n",
    "        self.start=start\n",
    "        self.end=end\n",
    "        self.old_position=old_position\n",
    "        self.new_signal=new_signal\n",
    "        self.revision_adjust=revision_adjust\n",
    "        self.early_exit=early_exit\n",
    "        \n",
    "#     def signal_base(self):\n",
    "#         '''\n",
    "#         Filter the signal criteria like EAR and revision, get the target signal list\n",
    "#         From the fundamental information\n",
    "#         For both long and short side\n",
    "#         '''\n",
    "        \n",
    "#         long_base=self.fundamental_df.copy()\n",
    "#         short_base=self.fundamental_df.copy()\n",
    "        \n",
    "\n",
    "#         long_base[\"EAR\"]=long_base.apply(lambda x:EAR_calc(x.name[0],x.name[1],self.close_return_df,self.EAR_period,30)\\\n",
    "#                                      if x.name[0] in self.close_return_df.index else None,axis=1)\n",
    "\n",
    "#         short_base[\"EAR\"]=short_base.apply(lambda x:EAR_calc(x.name[0],x.name[1],self.close_return_df,self.EAR_period,30)\\\n",
    "#                                      if x.name[0] in self.close_return_df.index else None,axis=1)\n",
    "\n",
    "\n",
    "#         if self.long_criteria is None:\n",
    "#             long_base=None\n",
    "#         else:\n",
    "            \n",
    "#             if self.long_criteria[1] is None:\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 if self.new_signal is True:\n",
    "#                     long_base=long_base[(long_base[\"Revision_real\"]>self.long_criteria[1][0])\\\n",
    "#                                                     &(long_base[\"Revision_real\"]<self.long_criteria[1][1])]\n",
    "#                 else:\n",
    "#                     long_base=long_base[(long_base[\"Revision_20\"]>self.long_criteria[1][0])\\\n",
    "#                                                     &(long_base[\"Revision_20\"]<self.long_criteria[1][1])]                    \n",
    "                \n",
    "#             if self.long_criteria[2] is None:\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 long_base=long_base[(long_base[\"Market cap\"]>self.long_criteria[2][0])&\\\n",
    "#                                         (long_base[\"Market cap\"]<self.long_criteria[2][1])]\n",
    "            \n",
    "#             if self.long_criteria[0] is None:\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 long_base=long_base[(long_base[\"EAR\"]>self.long_criteria[0][0])&(long_base[\"EAR\"]<self.long_criteria[0][1])]\n",
    "                \n",
    "                \n",
    "#         if self.short_criteria is None:\n",
    "#             short_base=None\n",
    "#         else:\n",
    "            \n",
    "#             if self.short_criteria[1] is None:\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 if self.new_signal is True:\n",
    "#                     short_base=short_base[(short_base[\"Revision_real\"]>self.short_criteria[1][0])\\\n",
    "#                                                     &(short_base[\"Revision_real\"]<self.short_criteria[1][1])]\n",
    "#                 else:\n",
    "#                     short_base=short_base[(short_base[\"Revision_20\"]>self.short_criteria[1][0])\\\n",
    "#                                                     &(short_base[\"Revision_20\"]<self.short_criteria[1][1])]                    \n",
    "                \n",
    "#             if self.short_criteria[2] is None:\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 short_base=short_base[(short_base[\"Market cap\"]>self.short_criteria[2][0])&\\\n",
    "#                                         (short_base[\"Market cap\"]<self.short_criteria[2][1])]\n",
    "            \n",
    "#             if self.short_criteria[0] is None:\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 short_base=short_base[(short_base[\"EAR\"]>self.short_criteria[0][0])&\\\n",
    "#                                       (short_base[\"EAR\"]<self.short_criteria[0][1])]\n",
    "        \n",
    "#         self.long_base=long_base\n",
    "#         self.short_base=short_base\n",
    "        \n",
    "#         return long_base,short_base\n",
    "    \n",
    "    def signal_df_date(self):#if we hold them through next earning\n",
    "        '''\n",
    "        Obtain the signal_df function over the whole time period from the target signal list\n",
    "        '''\n",
    "        \n",
    "        if self.long_base is None:\n",
    "            long_df=None\n",
    "            self.long_df=long_df\n",
    "        \n",
    "        else:\n",
    "            long_base=self.long_base.copy()\n",
    "            long_df=pd.DataFrame(index=self.price_df.columns)\n",
    "\n",
    "            for s in long_base.index:\n",
    "                return_series=self.abs_return.loc[s[0]]\n",
    "                if s[1] in return_series.index:\n",
    "                    if not np.isnan(return_series.loc[s[1]]): \n",
    "                        day0=return_series.index.tolist().index(s[1])\n",
    "                        \n",
    "                        if self.early_exit is True and datetime.strptime(s[1],\"%d/%b/%Y\").date()>=\\\n",
    "                        datetime.strptime(s[3],\"%d/%b/%Y\").date():\n",
    "                            period=None\n",
    "                        \n",
    "                        elif self.early_exit is True and type(s[2])!=float and datetime.strptime(s[1],\"%d/%b/%Y\").date()<\\\n",
    "                        datetime.strptime(s[3],\"%d/%b/%Y\").date():\n",
    "                            period=min(self.holding,np.busday_count(datetime.strptime(s[1],\"%d/%b/%Y\").date(),\\\n",
    "                                                                    datetime.strptime(s[2],\"%d/%b/%Y\").date())-self.entry,\n",
    "                                      np.busday_count(datetime.strptime(s[1],\"%d/%b/%Y\").date(),\\\n",
    "                                                                    datetime.strptime(s[3],\"%d/%b/%Y\").date())-self.entry+1)                            \n",
    "                        \n",
    "                        elif self.early_exit is True and type(s[2])==float and datetime.strptime(s[1],\"%d/%b/%Y\").date()<\\\n",
    "                        datetime.strptime(s[3],\"%d/%b/%Y\").date():\n",
    "                            period=min(self.holding,np.busday_count(datetime.strptime(s[1],\"%d/%b/%Y\").date(),\\\n",
    "                                                                    datetime.strptime(s[3],\"%d/%b/%Y\").date())-self.entry+1)\n",
    "                            \n",
    "                        elif type(s[2])==float:##basically np.nan has type float\n",
    "                            period=self.holding+1\n",
    "                        \n",
    "                        else: ##assume that we are not holding through numbers\n",
    "                            period=min(self.holding,np.busday_count(datetime.strptime(s[1],\"%d/%b/%Y\").date(),\\\n",
    "                                                                    datetime.strptime(s[2],\"%d/%b/%Y\").date())-self.entry)\n",
    "                            \n",
    "                        if period is None:\n",
    "                            pass\n",
    "                        else:\n",
    "                            target_series=return_series.iloc[day0+self.entry-1:day0+min(period+self.entry, \\\n",
    "                                                                                           len(return_series[day0:]))].dropna()\n",
    "                            \n",
    "                            if len(target_series)==0:\n",
    "                                pass\n",
    "                            else:\n",
    "                                target_series.iloc[0]=0.0\n",
    "                                long_df[s]=target_series                        \n",
    "\n",
    "                    else:\n",
    "                        pass\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "            long_df=long_df.reindex(datetime.strptime(i,\"%d/%b/%Y\") for i in long_df.index)\n",
    "            long_df=long_df.sort_index()\n",
    "\n",
    "            if self.start is not None:\n",
    "                long_df=slice_universe(long_df,self.start,self.end,self.old_position)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            long_df=long_df.dropna(how=\"all\",axis=1)\n",
    "            long_df.columns=pd.MultiIndex.from_tuples(pd.Series(list(long_df.columns)))\n",
    "            self.long_df=long_df\n",
    "            \n",
    "            \n",
    "        \n",
    "        if self.short_base is None:\n",
    "            short_df=None\n",
    "            self.short_df=short_df\n",
    "        \n",
    "        else:\n",
    "            short_base=self.short_base.copy()\n",
    "            short_df=pd.DataFrame(index=self.price_df.columns)\n",
    "\n",
    "            for s in short_base.index:\n",
    "                return_series=self.abs_return.loc[s[0]]\n",
    "                if s[1] in return_series.index:\n",
    "                    if not np.isnan(return_series.loc[s[1]]): \n",
    "                        day0=return_series.index.tolist().index(s[1])\n",
    "                        \n",
    "                        if self.early_exit is True and datetime.strptime(s[1],\"%d/%b/%Y\").date()>=\\\n",
    "                        datetime.strptime(s[3],\"%d/%b/%Y\").date():\n",
    "                            period=None\n",
    "                            \n",
    "                        elif self.early_exit is True and type(s[2])!=float and datetime.strptime(s[1],\"%d/%b/%Y\").date()<\\\n",
    "                        datetime.strptime(s[3],\"%d/%b/%Y\").date():\n",
    "                            period=min(self.holding,np.busday_count(datetime.strptime(s[1],\"%d/%b/%Y\").date(),\\\n",
    "                                                                    datetime.strptime(s[2],\"%d/%b/%Y\").date())-self.entry,\n",
    "                                      np.busday_count(datetime.strptime(s[1],\"%d/%b/%Y\").date(),\\\n",
    "                                                                    datetime.strptime(s[3],\"%d/%b/%Y\").date())-self.entry+1)  \n",
    "                            \n",
    "                        elif self.early_exit is True and type(s[2])==float and datetime.strptime(s[1],\"%d/%b/%Y\").date()<\\\n",
    "                        datetime.strptime(s[3],\"%d/%b/%Y\").date():\n",
    "                            period=min(self.holding,np.busday_count(datetime.strptime(s[1],\"%d/%b/%Y\").date(),\\\n",
    "                                                                    datetime.strptime(s[3],\"%d/%b/%Y\").date())-self.entry+1)\n",
    "                        elif type(s[2])==float:##basically np.nan has type float\n",
    "                            period=self.holding\n",
    "                    \n",
    "                        else: ##assume that we are not holding through numbers\n",
    "                            period=min(self.holding,np.busday_count(datetime.strptime(s[1],\"%d/%b/%Y\").date(),\\\n",
    "                                                                    datetime.strptime(s[2],\"%d/%b/%Y\").date())-self.entry)\\\n",
    "\n",
    "\n",
    "                        if period is None:\n",
    "                            pass\n",
    "                        else:\n",
    "                            target_series=return_series.iloc[day0+self.entry-1:day0+min(period+self.entry, \\\n",
    "                                                                                           len(return_series[day0:]))].dropna()\n",
    "                            \n",
    "                            if len(target_series)==0:\n",
    "                                pass\n",
    "                            else:\n",
    "                                target_series.iloc[0]=0.0\n",
    "                                short_df[s]=target_series         \n",
    "                            \n",
    "                    else:\n",
    "                        pass\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "            short_df=short_df.reindex(datetime.strptime(i,\"%d/%b/%Y\") for i in short_df.index)\n",
    "            short_df=short_df.sort_index()\n",
    "\n",
    "            if self.start is not None:\n",
    "                short_df=slice_universe(short_df,self.start,self.end,self.old_position)\n",
    "            else:\n",
    "                pass  \n",
    "            \n",
    "            short_df=short_df.dropna(how=\"all\",axis=1)\n",
    "            short_df.columns=pd.MultiIndex.from_tuples(pd.Series(list(short_df.columns)))\n",
    "            self.short_df=short_df\n",
    "        \n",
    "        return long_df,short_df\n",
    "\n",
    "    def signal_account(self,stop,gross,index_df,net_level,risk_parity,liquidity,capital):\n",
    "        '''\n",
    "        Build the account curve with signal_df\n",
    "        Assume quarterly rebalancing that's why the period list has quarter as the key\n",
    "        Take extra care when building the account curve, the logic is: work out the size_df, then shift by 1 and * signal_df\n",
    "        Stop=(long_stop,short_stop,type)\n",
    "        index_df has to be a dataframe with a name\n",
    "        '''\n",
    "        \n",
    "        try:\n",
    "            long_df=self.long_df.copy()\n",
    "            short_df=self.short_df.copy()\n",
    "            \n",
    "        except:\n",
    "            long_df,short_df=self.signal_df_date()\n",
    "                   \n",
    "        '''\n",
    "        Assign values for later use\n",
    "        '''\n",
    "        \n",
    "        self.capital=capital\n",
    "        \n",
    "        self.index_df=index_df\n",
    "        self.index_df.index=[datetime.strptime(i,\"%d/%b/%Y\") for i in self.index_df.index]\n",
    "       \n",
    "        \n",
    "        '''\n",
    "        Define rebalance period first\n",
    "        '''\n",
    "        if long_df is None:\n",
    "            period=short_df.apply(lambda x:str(x.name.year)+\" \"+str(x.name.quarter),axis=1)\n",
    "            period_list=list(set(period))\n",
    "            period_list.sort()    \n",
    "            \n",
    "        else:\n",
    "            period=long_df.apply(lambda x:str(x.name.year)+\" \"+str(x.name.quarter),axis=1)\n",
    "            period_list=list(set(period))\n",
    "            period_list.sort()    \n",
    "                \n",
    "        '''\n",
    "        Separate out long and short\n",
    "        '''\n",
    "        if long_df is None:\n",
    "            long_cache=(None,None,None,None)\n",
    "        else:\n",
    "            if stop is None:\n",
    "                pass\n",
    "            else:\n",
    "                long_df=signal_filter_stop(long_df,stop[0],self.abs_return,30,stop[2],self.index_df)   \n",
    "                self.long_df=long_df\n",
    "\n",
    "            long_sub_signal={}\n",
    "            long_sub_size_row={}\n",
    "            long_sub_size_df={}\n",
    "            long_sub_pnl={}\n",
    "        \n",
    "            for s in period_list:\n",
    "            \n",
    "                long_sub_signal[s]=long_df[period==s].dropna(how='all',axis=1)\n",
    "                \n",
    "                if long_sub_signal[s].shape[1]==0:\n",
    "                    long_sub_size_df[s]=long_sub_signal[s]\n",
    "                    long_sub_pnl[s]=long_sub_signal[s]\n",
    "                    \n",
    "                else:\n",
    "                    if period_list.index(s)<4:##use last quarter's sizing as reference\n",
    "                        long_sub_size_row[s]=sizing(long_sub_signal[s],long_sub_signal[s],gross[0],self.fundamental_df,\\\n",
    "                                                       self.new_signal,self.close_return_df,risk_parity,liquidity,capital,\\\n",
    "                                                    self.revision_adjust,True)[0]\n",
    "                        \n",
    "                    else:\n",
    "                        try:\n",
    "                            long_sub_size_row[s]=sizing(long_sub_signal[s],long_sub_signal\\\n",
    "                                                        [period_list[period_list.index(s)-1]],\\\n",
    "                                                       gross[0],self.fundamental_df,self.new_signal,self.close_return_df,\\\n",
    "                                                        risk_parity,liquidity,capital,self.revision_adjust,True)[0]\n",
    "                        except:\n",
    "                            long_sub_size_row[s]=sizing(long_sub_signal[s],long_sub_signal[s],\\\n",
    "                                                       gross[0],self.fundamental_df,self.new_signal,self.close_return_df,\\\n",
    "                                                        risk_parity,liquidity,capital,self.revision_adjust,True)[0] \n",
    "\n",
    "                    long_sub_size_df[s]=(1+long_sub_signal[s]).cumprod()*long_sub_size_row[s]\n",
    "                    long_sub_pnl[s]=(long_sub_size_df[s].shift(1))*long_sub_signal[s] \n",
    "                    # need to shift by 1 as the size is end of the day\n",
    "        \n",
    "            long_daily_pnl=pd.concat(list(long_sub_pnl.values()),axis=0)\n",
    "            long_acct_curve=long_daily_pnl.cumsum().ffill().sum(axis=1)\n",
    "            long_size_df=pd.concat(list(long_sub_size_df.values()),axis=0)\n",
    "            \n",
    "            long_ind_return=long_daily_pnl.cumsum().ffill().iloc[-1].dropna()\n",
    "            long_cache=(long_daily_pnl,long_acct_curve,long_size_df,long_ind_return)\n",
    "            \n",
    "            self.long_cache=long_cache\n",
    "\n",
    "            \n",
    "        if short_df is None:\n",
    "            short_cache=(None,None,None,None)\n",
    "        else:\n",
    "            if stop is None:\n",
    "                pass\n",
    "            else:\n",
    "                short_df=-signal_filter_stop(-short_df,stop[1],self.abs_return,30,stop[2],self.index_df)   \n",
    "                self.short_df=short_df\n",
    "\n",
    "            short_sub_signal={}\n",
    "            short_sub_size_row={}\n",
    "            short_sub_size_df={}\n",
    "            short_sub_pnl={}\n",
    "        \n",
    "            for s in period_list:\n",
    "                short_sub_signal[s]=short_df[period==s].dropna(how='all',axis=1)\n",
    "\n",
    "                if short_sub_signal[s].shape[1]==0:\n",
    "                    short_sub_size_df[s]=short_sub_signal[s]\n",
    "                    short_sub_pnl[s]=short_sub_signal[s]\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    if period_list.index(s)<4:##use last quarter's sizing as reference\n",
    "                        short_sub_size_row[s]=-sizing(short_sub_signal[s],short_sub_signal[s],gross[1],self.fundamental_df,\\\n",
    "                                                       self.new_signal,self.close_return_df,risk_parity,liquidity,capital,\\\n",
    "                                                      self.revision_adjust,False)[0]\n",
    "                    else:\n",
    "                        try:\n",
    "                            short_sub_size_row[s]=-sizing(short_sub_signal[s],short_sub_signal\\\n",
    "                                                          [period_list[period_list.index(s)-1]],\\\n",
    "                                                           gross[1],self.fundamental_df,self.new_signal,self.close_return_df,\\\n",
    "                                                          risk_parity,liquidity,\\\n",
    "                                                          capital,self.revision_adjust,False)[0]\n",
    "                        except:\n",
    "                            short_sub_size_row[s]=-sizing(short_sub_signal[s],short_sub_signal[s],\\\n",
    "                                                           gross[1],self.fundamental_df,self.new_signal,self.close_return_df,\\\n",
    "                                                          risk_parity,liquidity,\\\n",
    "                                                          capital,self.revision_adjust,False)[0]\n",
    "\n",
    "                    short_sub_size_df[s]=(1+short_sub_signal[s]).cumprod()*short_sub_size_row[s]\n",
    "                    short_sub_pnl[s]=(short_sub_size_df[s].shift(1))*short_sub_signal[s] \n",
    "                # need to shift by 1 as the size is end of the day\n",
    "        \n",
    "            short_daily_pnl=pd.concat(list(short_sub_pnl.values()),axis=0)\n",
    "            short_acct_curve=short_daily_pnl.cumsum().ffill().sum(axis=1)\n",
    "            short_size_df=pd.concat(list(short_sub_size_df.values()),axis=0)\n",
    "            short_ind_return=short_daily_pnl.cumsum().ffill().iloc[-1].dropna()\n",
    "            \n",
    "            short_cache=(short_daily_pnl,short_acct_curve,short_size_df,short_ind_return)\n",
    "            self.short_cache=short_cache\n",
    "    \n",
    "        '''Put alpha positions together to form the alpha part'''\n",
    "        alpha_df=pd.concat([long_df,short_df],axis=1)\n",
    "        self.alpha_df=alpha_df\n",
    "        \n",
    "        alpha_daily_pnl=pd.concat([long_cache[0],short_cache[0]],axis=1)\n",
    "        alpha_acct_curve=alpha_daily_pnl.cumsum().ffill().sum(axis=1)\n",
    "        alpha_size_df=pd.concat([long_cache[2],short_cache[2]],axis=1)\n",
    "        alpha_ind_return=pd.concat([long_cache[3],short_cache[3]],axis=0)\n",
    "        \n",
    "        alpha_cache=(alpha_daily_pnl,alpha_acct_curve,alpha_size_df,alpha_ind_return)\n",
    "        \n",
    "\n",
    "        \n",
    "        if self.index_df is not None:\n",
    "            if self.index_df.shape[1]==1:\n",
    "                index_df=self.index_df.copy().reindex(alpha_df.index) \n",
    "                if net_level=='beta':\n",
    "                    long_beta=long_size_df.apply(lambda x:calc_beta(x.name[0],x.name[1],self.close_return_df,\\\n",
    "                                                                    self.index_df.iloc[:,0],260),axis=0)\n",
    "                    short_beta=short_size_df.apply(lambda x:calc_beta(x.name[0],x.name[1],self.close_return_df,\\\n",
    "                                                                      self.index_df.iloc[:,0],260),axis=0)\n",
    "                    beta_adj_alpha_size_df=pd.concat([long_size_df*long_beta,short_size_df*short_beta],axis=1)\n",
    "                    index_size_df=(0-beta_adj_alpha_size_df.sum(axis=1)).to_frame(index_df.columns[0])\n",
    "                elif net_level=='vol':\n",
    "                    \n",
    "                    \n",
    "                    long_vol=long_size_df.apply(lambda x:signal_vol(x,self.close_return_df,260)/\\\n",
    "                                                index_vol(datetime.strptime(x.name[1], \"%d/%b/%Y\"),self.index_df.iloc[:,0],30)\\\n",
    "                                                if datetime.strptime(x.name[1], \"%d/%b/%Y\") in self.index_df.index\\\n",
    "                                                else signal_vol(x,self.close_return_df,260), axis=0)\n",
    "                    short_vol=short_size_df.apply(lambda x:signal_vol(x,self.close_return_df,260)/\\\n",
    "                                                index_vol(datetime.strptime(x.name[1], \"%d/%b/%Y\"),self.index_df.iloc[:,0],30)\\\n",
    "                                                  if datetime.strptime(x.name[1], \"%d/%b/%Y\") in self.index_df.index\\\n",
    "                                                else signal_vol(x,self.close_return_df,260), axis=0)\n",
    "                    self.long_vol=long_vol\n",
    "                    self.short_vol=short_vol\n",
    "                    vol_adj_alpha_size_df=pd.concat([long_size_df*long_vol,short_size_df*short_vol],axis=1)\n",
    "                    index_size_df=(0-vol_adj_alpha_size_df.sum(axis=1)).to_frame(index_df.columns[0])\n",
    "                else:\n",
    "                    index_size_df=(net_level-alpha_size_df.sum(axis=1)).to_frame(index_df.columns[0])\n",
    "                index_daily_pnl=index_size_df.shift(1)*index_df\n",
    "                index_acct_curve=index_daily_pnl.cumsum()\n",
    "                index_ind_return=index_acct_curve.iloc[-1]\n",
    "                index_cache=(index_daily_pnl,index_acct_curve,index_size_df,index_ind_return)\n",
    "            else:\n",
    "                index_df=self.index_df.copy().reindex(alpha_df.index) \n",
    "                alpha_size_temp=alpha_cache[2].copy().T\n",
    "                if net_level=='beta':\n",
    "                    long_beta=long_size_df.apply(lambda x:calc_beta(x.name[0],x.name[1],self.close_return_df,\\\n",
    "                                                                    self.index_df.loc[:,Asia_mapping.loc[x.name[0][-2:]].\\\n",
    "                                                                                      iloc[0]],260),axis=0)\n",
    "                    alpha_size_temp=(alpha_cache[2].copy()*long_beta).T\n",
    "                    alpha_size_temp[\"index\"]=alpha_size_temp.apply(lambda x:Asia_mapping.loc[x.name[0][-2:]].iloc[0],axis=1)\n",
    "                    index_size_df=0-alpha_size_temp.groupby(\"index\").apply(sum).T.iloc[:-1]\n",
    "                    \n",
    "                    self.long_beta=long_beta\n",
    "                else:\n",
    "                    \n",
    "                    alpha_size_temp=alpha_cache[2].copy().T\n",
    "                    alpha_size_temp[\"index\"]=alpha_size_temp.apply(lambda x:Asia_mapping.loc[x.name[0][-2:]].iloc[0],axis=1)\n",
    "                    index_size_df=net_level-alpha_size_temp.groupby(\"index\").apply(sum).T.iloc[:-1]\n",
    "                \n",
    "                index_daily_pnl=index_size_df.shift(1)*index_df\n",
    "                index_acct_curve=index_daily_pnl.cumsum()\n",
    "                index_ind_return=index_acct_curve.iloc[-1]\n",
    "                index_cache=(index_daily_pnl,index_acct_curve,index_size_df,index_ind_return)\n",
    "        else:\n",
    "            index_cache=(None,None,None,None)\n",
    "            \n",
    "        '''Finally put everything together'''    \n",
    "        portfolio_df=pd.concat([alpha_df,index_df],axis=1)\n",
    "            \n",
    "        portfolio_size_df=pd.concat([alpha_cache[2],index_cache[2]],axis=1).sort_index()\n",
    "  \n",
    "        portfolio_daily_pnl=pd.concat([alpha_cache[0],index_cache[0]],axis=1).sort_index()\n",
    "\n",
    "        portfolio_acct_curve=portfolio_daily_pnl.cumsum().ffill().sum(axis=1)\n",
    "        portfolio_ind_return=alpha_cache[3].copy()\n",
    "        \n",
    "        portfolio_gross=np.abs(portfolio_size_df).sum(axis=1).sort_index()\n",
    "        portfolio_turnover=(np.abs(alpha_size_df.fillna(0.0).diff(1)).sum().sum())/(portfolio_size_df.shape[0]/260)\n",
    "        \n",
    "        portfolio_cache=(portfolio_daily_pnl,portfolio_acct_curve,portfolio_size_df,portfolio_ind_return,portfolio_gross,\\\n",
    "                         portfolio_turnover,portfolio_df)\n",
    "        \n",
    "        self.portfolio_account=portfolio_cache #save for later use\n",
    "        \n",
    "        \n",
    "        return long_cache,short_cache,alpha_cache,portfolio_cache\n",
    "    \n",
    "    def plot_account(self,title,figsize=[10,4],portfolio=None):\n",
    "        '''\n",
    "        Plot the account curve\n",
    "        '''\n",
    "        if portfolio is None:\n",
    "            try:\n",
    "                portfolio_cache=self.portfolio_account\n",
    "\n",
    "            except AttributeError:\n",
    "                print(\"Execute the signal_account first!\")  \n",
    "                return None\n",
    "        else:\n",
    "            portfolio_cache=portfolio\n",
    "        \n",
    "        plot_signal(title,figsize,portfolio_cache)\n",
    "        \n",
    "    def review(self):\n",
    "        ''' \n",
    "        Export the data for review\n",
    "        '''\n",
    "        if self.index_df.shape[1]==1:\n",
    "            long_df_adj=self.long_df.copy().sub(self.index_df.loc[self.long_df.index].iloc[:,0],axis=0)\n",
    "            short_df_adj=(-self.short_df.copy()).add(self.index_df.loc[self.long_df.index].iloc[:,0],axis=0)\n",
    "            \n",
    "            short_review=((1+short_df_adj).cumprod()-1).ffill().iloc[-1].to_frame()\n",
    "            short_review[\"size\"]=self.short_cache[2].apply(lambda x:x.dropna().iloc[0],axis=0)*(self.capital)*1000\n",
    "            \n",
    "        else:\n",
    "            long_df_adj=self.long_df.copy().apply(lambda x: x.sub(self.index_df.\\\n",
    "                                                                  loc[self.long_df.index,\\\n",
    "                                                                      Asia_mapping.loc[x.name[0][-2:]].iloc[0]]),axis=0)\n",
    "            short_review=None\n",
    "                                                  \n",
    "                                                  \n",
    "        \n",
    "        long_review=((1+long_df_adj).cumprod()-1).ffill().iloc[-1].to_frame()\n",
    "        long_review[\"size\"]=self.long_cache[2].apply(lambda x:x.dropna().iloc[0],axis=0)*(self.capital)*1000\n",
    "\n",
    "        return long_review,short_review\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_signal(portfolio_list,capital_list):\n",
    "    \n",
    "    \n",
    "    daily_pnl_list=[]\n",
    "    signal_df_list=[]\n",
    "    portfolio_size_list=[]\n",
    "    for i in range(len(portfolio_list)):\n",
    "\n",
    "        daily_pnl_list.append(portfolio_list[i][0]*capital_list[i]/np.sum(capital_list))\n",
    "        signal_df_list.append(portfolio_list[i][-1]*capital_list[i]/np.sum(capital_list))\n",
    "        portfolio_size_list.append(portfolio_list[i][2]*capital_list[i]/np.sum(capital_list))\n",
    "    \n",
    "    daily_pnl=pd.concat(daily_pnl_list,axis=1)\n",
    "    signal_df=pd.concat(signal_df_list,axis=1)\n",
    "    size_df=pd.concat(portfolio_size_list,axis=1)\n",
    "    \n",
    "    account_curve=daily_pnl.cumsum().ffill().sum(axis=1)\n",
    "    ind_return=daily_pnl.cumsum().ffill().iloc[-1]\n",
    "\n",
    "    gross=np.abs(size_df).sum(axis=1)\n",
    "    turnover=(np.abs(size_df.fillna(0.0).diff(1)).sum().sum())/(size_df.shape[0]/260)\n",
    "    \n",
    "    portfolio_cache=(daily_pnl,account_curve,size_df,ind_return,gross,turnover,signal_df)\n",
    "    return portfolio_cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpe_by_year(account_curve,year_list):\n",
    "    result_dict={}\n",
    "    for i in year_list:\n",
    "        sub_curve=account_curve.iloc[(account_curve.index>=pd.Timestamp(i,1,1))&(account_curve.index<=pd.Timestamp(i,12,31))]\n",
    "        result_dict[i]=trading_analytics_simp(sub_curve)[0]\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dd_by_year(account_curve,year_list):\n",
    "    result_dict={}\n",
    "    for i in year_list:\n",
    "        sub_curve=account_curve.iloc[(account_curve.index>=pd.Timestamp(i,1,1))&(account_curve.index<=pd.Timestamp(i,12,31))]\n",
    "        result_dict[i]=trading_analytics_simp(sub_curve)[1]\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quarter_pnl_df(pnl_series):\n",
    "    \n",
    "    pnl=pnl_series.to_frame()\n",
    "    pnl.columns=[\"pnl\"]  \n",
    "    \n",
    "    def quarter(month):\n",
    "        if month in [1,2,3]:\n",
    "            return \"Q1\"\n",
    "        elif month in [4,5,6]:\n",
    "            return \"Q2\"\n",
    "        elif month in [7,8,9]:\n",
    "            return \"Q3\"\n",
    "        else:\n",
    "            return \"Q4\"\n",
    "    \n",
    "    pnl[\"Quarter\"]=pnl.apply(lambda x:str((x.name).year)+quarter((x.name).month),axis=1)\n",
    "    \n",
    "    quarter_first_dict={}\n",
    "    quarter_second_dict={}\n",
    "    for i in pnl[\"Quarter\"].unique():\n",
    "        first=pnl[pnl[\"Quarter\"]==i].iloc[:30][\"pnl\"]\n",
    "        second=pnl[pnl[\"Quarter\"]==i].iloc[30:][\"pnl\"]\n",
    "        quarter_first_dict[i]=first.iloc[-1]-first.iloc[0]\n",
    "        quarter_second_dict[i]=second.iloc[-1]-second.iloc[0]\n",
    "\n",
    "    quarter_df=pd.Series(quarter_first_dict).to_frame()\n",
    "    quarter_df.columns=[\"first_half\"]\n",
    "    quarter_df[\"second_half\"]=quarter_second_dict.values()\n",
    "    \n",
    "    return quarter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Asia_split_portfolio(portfolio_cache):\n",
    "    ''' \n",
    "    country separation analysis\n",
    "    '''\n",
    "    country_dict={}\n",
    "    \n",
    "    pnl_df=portfolio_cache[0].iloc[:,:-6].copy()\n",
    "    size_df=portfolio_cache[2].iloc[:,:-6].copy()\n",
    "    portfolio_df=portfolio_cache[-1].iloc[:,:-6].copy()\n",
    "    \n",
    "    country_row=pnl_df.apply(lambda x:x.name[0][-2:],axis=0)\n",
    "    unique_country=country_row.unique()\n",
    "    \n",
    "    for i in unique_country:\n",
    "        sub_pnl_df=pd.concat([pnl_df.loc[:,country_row==i],portfolio_cache[0][Asia_mapping.loc[i].iloc[0]]],axis=1)\n",
    "        sub_acct_curve=sub_pnl_df.cumsum().ffill().sum(axis=1)\n",
    "    \n",
    "        sub_size_df=pd.concat([size_df.loc[:,country_row==i],portfolio_cache[2][Asia_mapping.loc[i].iloc[0]]],axis=1)\n",
    "        sub_ind_return=pnl_df.loc[:,country_row==i].cumsum().ffill().iloc[-1].dropna()\n",
    "        \n",
    "        sub_gross=np.abs(sub_size_df).sum(axis=1).sort_index()\n",
    "        sub_turnover=(np.abs(size_df.loc[:,country_row==i].fillna(0.0).diff(1)).sum().sum())/\\\n",
    "        (size_df.loc[:,country_row==i].shape[0]/260)        \n",
    "        \n",
    "        sub_portfolio_df=pd.concat([portfolio_df.loc[:,country_row==i],portfolio_cache[-1][Asia_mapping.loc[i].iloc[0]]],axis=1)\n",
    "        \n",
    "        country_dict[i]=[sub_pnl_df,sub_acct_curve,sub_size_df,sub_ind_return,sub_gross,sub_turnover,sub_portfolio_df]\n",
    "    return country_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
