{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import textwrap\n",
    "import swifter\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_date_transform(CSV_date,index=False):\n",
    "    '''\n",
    "    Transform the CSV price style string into dateframe string style\n",
    "    The CSV date follows US style which is MM/DD/YYYY\n",
    "    '''\n",
    "    if index==False:\n",
    "        timestamp=pd.Timestamp(int(CSV_date[CSV_date.find(\"/\",3)+1:]),\n",
    "                            int(CSV_date[:CSV_date.find(\"/\")]),\n",
    "                            int(CSV_date[CSV_date.find(\"/\",1)+1:CSV_date.find(\"/\",3)]))\n",
    "        return timestamp.strftime(\"%d/%b/%Y\")\n",
    "    else:\n",
    "        timestamp=pd.Timestamp(int(CSV_date[-4:]),int(CSV_date[3:5]),int(CSV_date[:2]))\n",
    "        return timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fundamental_date_transform(CSV_date):\n",
    "    '''\n",
    "    Transform the fundamental style string into dateframe string style\n",
    "    CSV date follow following style yyyy-mm-dd or MM/DD/YYYY\n",
    "    '''\n",
    "    if '-' in CSV_date:\n",
    "        timestamp=pd.Timestamp(int(CSV_date[:4]),\n",
    "                            int(CSV_date[5:7]),\n",
    "                            int(CSV_date[8:]))\n",
    "    else:\n",
    "        timestamp=pd.Timestamp(int(CSV_date[CSV_date.find(\"/\",3)+1:]),\n",
    "                               int(CSV_date[CSV_date.find(\"/\",1)+1:CSV_date.find(\"/\",3)]),\n",
    "                               int(CSV_date[:CSV_date.find(\"/\")]))        \n",
    "    return timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CSV_price(region,start,end,VWAP):\n",
    "    '''\n",
    "    Grab the pricing data from CSV\n",
    "    region= US, Europe,Asia,or Canada\n",
    "    start,end are in year\n",
    "    VWAP: boolean to determine if we read price_df or VWAP_df\n",
    "    key is the reference to search\n",
    "    return the target price dataframe with timestamp on the column; also index price and return dataframe\n",
    "    '''\n",
    "    \n",
    "    # price dataframe\n",
    "    mylist=[]\n",
    "    for year in range(start,end+1):\n",
    "        if VWAP==False:\n",
    "            csv=pd.read_csv(r\"C:\\Users\\Eric.Li\\OneDrive\\Post result data\\{0} CSV\\{0}_price_{1}.csv\".format(region,year)).dropna\\\n",
    "    (how='all',axis=0).dropna(how='all',axis=1)\n",
    "        else:\n",
    "            csv=pd.read_csv(r\"C:\\Users\\Eric.Li\\OneDrive\\Post result data\\{0} CSV\\{0}_VWAP_{1}.csv\".format(region,year)).dropna\\\n",
    "    (how='all',axis=0).dropna(how='all',axis=1)\n",
    "        data=csv.set_index(\"Ticker\")\n",
    "        adj_data=data.loc[[x for x in data.index if type(x)==str]].replace('#N/A N/A','').replace(' #N/A N/A ','').\\\n",
    "        replace('#N/A Invalid Security','')\n",
    "        adj_data=adj_data.loc[[x for x in adj_data.index if len(x)>0]]\n",
    "        mylist.append(adj_data)\n",
    "\n",
    "    price=pd.concat(mylist,axis=1,sort=True)\n",
    "    price=price.apply(lambda x:pd.to_numeric(x),axis=1)\n",
    "    price.columns=[price_date_transform(i) for i in price.columns]\n",
    "    \n",
    "    # index price dataframe\n",
    "    csv_index=pd.read_csv(r\"C:\\Users\\Eric.Li\\OneDrive\\Post result data\\{0} CSV\\{0}_price_index.csv\".format(region)).dropna\\\n",
    "    (how='all',axis=0)\n",
    "    data_index=csv_index.set_index(\"Ticker\").T\n",
    "    price_index=data_index.replace('#N/A N/A','')\n",
    "    price_index.columns=[price_date_transform(i) for i in price_index.columns]\n",
    "    \n",
    "    # return data\n",
    "    abs_return=price.diff(1,axis=1)/price.shift(1,axis=1)\n",
    "    abs_return_index=price_index.diff(1,axis=1)/price_index.shift(1,axis=1)\n",
    "    return price,abs_return,price_index,abs_return_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CSV_EPS(region,start,end):\n",
    "    '''\n",
    "    Grab the EPS data from CSV database\n",
    "    region= US, Europe,Asia,or Canada\n",
    "    start,end are in year\n",
    "    key is the reference to search\n",
    "    return the target EPS dataframe with timestamp on the column\n",
    "    '''\n",
    "    mylist=[]\n",
    "    for year in range(start,end+1):\n",
    "        csv=pd.read_csv(r\"C:\\Users\\Eric.Li\\OneDrive\\Post result data\\{0} CSV\\{0}_EPS_{1}.csv\".format(region,year))\n",
    "        data=csv.set_index(\"Ticker\")\n",
    "        adj_data=data.loc[[x for x in data.index if type(x)==str]].replace('#N/A N/A','').replace(\" #N/A N/A \",\"\").dropna\\\n",
    "        (how='all',axis=0).dropna(how='all',axis=1)\n",
    "        adj_data=adj_data.loc[[x for x in adj_data.index if len(x)>0]]\n",
    "        mylist.append(adj_data)\n",
    "\n",
    "    EPS=pd.concat(mylist,axis=1,sort=True)\n",
    "    EPS=EPS.apply(lambda x:pd.to_numeric(x),axis=1)\n",
    "    EPS.columns=[price_date_transform(i) for i in EPS.columns]\n",
    "    return EPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CSV_fundamentals(region,price,EPS_df,revision_period,min_history,min_vol,use_cache):\n",
    "    '''\n",
    "    Grab the fundamental data from the spreadsheet\n",
    "    region= US, Europe,Asia,or Canada\n",
    "    return the post result fundamental dataframe\n",
    "    use_cache: boolean, if yes we just read the last cache of fundamental_df\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if use_cache is True:\n",
    "        try:\n",
    "            data=pd.read_csv(r'C:\\Users\\Eric.Li\\OneDrive\\Post result data cache\\{0}{1}_fundamental_df.csv'.format(region,\\\n",
    "                                                                                                        str(revision_period)))\n",
    "            new_index=pd.MultiIndex.from_tuples(list(zip(data.iloc[:,0],data.iloc[:,1],data.iloc[:,2],data.iloc[:,3])))\n",
    "            data.index=new_index\n",
    "            target_data=data.iloc[:,4:]\n",
    "        except:\n",
    "            print(\"No such file!\")\n",
    "    else:\n",
    "        # import the raw fundamental_df and clean up all the nonsense\n",
    "        csv=pd.read_csv(r'C:\\Users\\Eric.Li\\OneDrive\\Post result data cache\\{0}_raw_fundamental_df.csv'.format(region))\n",
    "        data=csv.set_index(\"Ticker\").drop_duplicates().replace('#N/A Invalid Security','').\\\n",
    "        replace('#N/A Requesting Data...','')\n",
    "        data=data[data.index!='']\n",
    "        data=data.dropna(how=\"all\")\n",
    "\n",
    "        # Manipulate the data to get the next earning date, quarter end date, finally generate multi-index for the dataframe \n",
    "        data[\"date_copy\"]=[fundamental_date_transform(i) for i in data[\"Date\"].copy()]\n",
    "        data[\"ticker_copy\"]=data.index\n",
    "        data=data.copy().sort_values(by=[\"ticker_copy\",\"date_copy\"])\n",
    "        data[\"next_date\"]=data[\"date_copy\"].shift(-1)\n",
    "        data[\"ticker_copy\"]=data[\"ticker_copy\"].shift(-1)\n",
    "        print(datetime.now())\n",
    "        data[\"Date\"]=data[\"date_copy\"].copy().swifter.apply(lambda x: x.strftime(\"%d/%b/%Y\") if x!='' else np.nan)\n",
    "        print(datetime.now())\n",
    "        data[\"Orig date\"]=data[\"Orig date\"].copy().swifter.apply(lambda x: pd.Timestamp(x).strftime(\"%d/%b/%Y\")\\\n",
    "                                                          if x!='' else np.nan)\n",
    "        print(datetime.now())\n",
    "        data[\"Next\"]=data.swifter.apply(lambda x: x[\"next_date\"].strftime(\"%d/%b/%Y\") \\\n",
    "                                        if type(x[\"next_date\"])==pd.Timestamp and \\\n",
    "                                x.name==x[\"ticker_copy\"] else np.nan,axis=1)\n",
    "        print(datetime.now())\n",
    "        \n",
    "#         data[\"period\"]=data.apply(lambda x:str(pd.Timestamp(datetime.strptime(x[\"Date\"],\"%d/%b/%Y\")).year)\\\n",
    "#                                             +\" \"+str(pd.Timestamp(datetime.strptime(x[\"Date\"],\"%d/%b/%Y\")).quarter),\\\n",
    "#                                             axis=1)\n",
    "\n",
    "        data[\"end_period\"]=data.swifter.apply(lambda x: pd.offsets.BQuarterEnd().rollforward(x[\"date_copy\"])\\\n",
    "                                              .strftime(\"%d/%b/%Y\"),\\\n",
    "                                      axis=1)\n",
    "\n",
    "        data.index=pd.MultiIndex.from_tuples(list(zip(data.index,data[\"Date\"],data[\"Next\"],data[\"end_period\"])))\n",
    "\n",
    "        del data[\"ticker_copy\"]\n",
    "        del data[\"date_copy\"]\n",
    "        del data[\"next_date\"]\n",
    "        del data[\"end_period\"]\n",
    "        del data[\"Next\"]\n",
    "\n",
    "        for s in [\"Market cap\",\"Volume\"]:\n",
    "            try:\n",
    "                data[s]=pd.to_numeric(data[s])\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "        '''\n",
    "        Add more forward look and realistic versions of earning revision\n",
    "        '''\n",
    "        print(datetime.now())\n",
    "        data[\"Revision_real\"]=data.swifter.apply(lambda x: revision_calc(x.name[0],x.name[1],EPS_df,(0,revision_period)),axis=1)\n",
    "        print(datetime.now())\n",
    "        data[\"Revision_20\"]=data.swifter.apply(lambda x: revision_calc(x.name[0],x.name[1],EPS_df,(0,20)),axis=1)\n",
    "        print(datetime.now())\n",
    "\n",
    "        '''\n",
    "        take out data with zero or none revision/market cap\n",
    "        '''\n",
    "        data=data[(data[\"Market cap\"]>=500)] #universe above 500mn\n",
    "        data=data[(data[\"Revision_20\"]>=0)|(data[\"Revision_20\"]<0)]\n",
    "        \n",
    "        '''\n",
    "        take out cases where there is a short history\n",
    "        '''\n",
    "        count_history=data.swifter.apply(lambda x: price.loc[x.name[0],:x.name[1]][-2*min_history:].count() if x.name[1] in \\\n",
    "                                   price.columns and x.name[0] in price.index else None,axis=1)\n",
    "        \n",
    "        data=data.copy()[count_history>=min_history]\n",
    "         \n",
    "\n",
    "        '''\n",
    "        Add historic volatility\n",
    "        '''\n",
    "        \n",
    "        abs_return=price.diff(1,axis=1)/price.shift(1,axis=1)\n",
    "        print(datetime.now())\n",
    "        data[\"30d_vol\"]=data.swifter.apply(lambda x: abs_return.loc[x.name[0],:x.name[1]][-31:-1].std() \\\n",
    "                                           if x.name[0] in abs_return.index\\\n",
    "                                   and abs_return.loc[x.name[0],:x.name[1]][-31:-1].dropna().shape[0]!=0 else None,axis=1)      \n",
    "        print(datetime.now())\n",
    "        data=data[data[\"30d_vol\"]>=min_vol]\n",
    "        \n",
    "        '''\n",
    "        Final cleaning and export the data\n",
    "        '''\n",
    "        target_data=data.drop_duplicates()\n",
    "        target_data.to_csv(r'C:\\Users\\Eric.Li\\OneDrive\\Post result data cache\\{0}{1}_fundamental_df.csv'.format(region,\\\n",
    "                                                                                                      str(revision_period)))\n",
    "    return target_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Util function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revision_calc(ticker,date,EPS_df,period):\n",
    "    '''\n",
    "    Calculate revision from ticker and reference date\n",
    "    EPS_df: EPS dataframe with all the historical data\n",
    "    '''\n",
    "    if type(date)==pd.NaT:\n",
    "        return None\n",
    "    elif type(date)==pd.Timestamp:\n",
    "        date=date.strftime(\"%d/%b/%Y\")\n",
    "    elif type(date)==str:\n",
    "        date=date\n",
    "    \n",
    "    if ticker in EPS_df.index:\n",
    "        eps_series=EPS_df.loc[ticker]\n",
    "        date_series=eps_series.index.tolist()\n",
    "        if date in date_series:\n",
    "            day0=date_series.index(date)\n",
    "            post_series=eps_series.iloc[day0+period-1:day0+period+10]\n",
    "            pre_series=eps_series.iloc[day0-10:day0]\n",
    "            if len(post_series.dropna())==0 or len(pre_series.dropna())==0 or pre_series.dropna().iloc[-1]==0:\n",
    "                revision=None\n",
    "            else:\n",
    "                try:\n",
    "                    revision=np.divide(post_series.dropna().iloc[0],pre_series.dropna().iloc[-1])-1\n",
    "                except:\n",
    "                    revision=None\n",
    "            return revision\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revision_calc(ticker,date,EPS_df,period_tuple):\n",
    "    '''\n",
    "    Calculate percentage revision from the period tuple\n",
    "    Day starts from zero, so 1 means start the return calculation one day after result, second element is the number of days\n",
    "    return calculation assumes enters on the price of the prior day to day when the count starts\n",
    "    '''\n",
    "    if type(date)==float:\n",
    "        return None\n",
    "    elif type(date)==pd.Timestamp:\n",
    "        date=date.strftime(\"%d/%b/%Y\")\n",
    "    elif type(date)==str:\n",
    "        date=date\n",
    "    \n",
    "    eps_series=EPS_df.loc[ticker].dropna()\n",
    "    date_series=eps_series.index.tolist()\n",
    "    \n",
    "    if date in date_series:\n",
    "        day0=date_series.index(date)\n",
    "\n",
    "        if period_tuple[0]<0 and len(eps_series.loc[:date].dropna())-1<=abs(period_tuple[0]):\n",
    "            start=eps_series.dropna().iloc[0]\n",
    "        else:\n",
    "            start=eps_series.iloc[:day0+period_tuple[0]].dropna().iloc[-1]        \n",
    "        \n",
    "        end=eps_series.iloc[:day0+period_tuple[0]+period_tuple[1]].dropna().iloc[-1]\n",
    "                \n",
    "        if start!=0:\n",
    "            revision=(end-start)/abs(start)\n",
    "        else:\n",
    "            revision=None\n",
    "        \n",
    "        target_revision=revision\n",
    "        return target_revision\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_calc(ticker,date,price_df,index_df,period_tuple,abs_rel):\n",
    "    '''\n",
    "    Calculate percentage return from the period tuple\n",
    "    abs_rel: bool, abs_return if assigned abs\n",
    "    Day starts from zero, so 1 means start the return calculation one day after result, second element is the number of days\n",
    "    return calculation assumes enters on the close price of the prior day to day when the count starts\n",
    "    '''\n",
    "    if type(date)==float:\n",
    "        return None\n",
    "    elif type(date)==pd.Timestamp:\n",
    "        date=date.strftime(\"%d/%b/%Y\")\n",
    "    elif type(date)==str:\n",
    "        date=date\n",
    "    price_series=price_df.loc[ticker].dropna()\n",
    "    date_series=price_series.index.tolist()\n",
    "    \n",
    "    index_data_series=index_df.index.tolist()\n",
    "    \n",
    "    if date in date_series:\n",
    "        day0=date_series.index(date)\n",
    "        day0_index=index_data_series.index(date)\n",
    "\n",
    "        if period_tuple[0]<0 and len(price_series.loc[:date].dropna())-1<=abs(period_tuple[0]):\n",
    "            start_price=price_series.dropna().iloc[0]\n",
    "        else:\n",
    "            start_price=price_series.iloc[:day0+period_tuple[0]].dropna().iloc[-1]        \n",
    "        end_price=price_series.iloc[:day0+period_tuple[0]+period_tuple[1]].dropna().iloc[-1]\n",
    "        \n",
    "        #target_series=price_series.iloc[day0+period_tuple[0]-2:day0+period_tuple[0]+period_tuple[1]]\n",
    "        \n",
    "        if start_price!=0:\n",
    "            abs_return=end_price/start_price-1\n",
    "        else:\n",
    "            abs_return=None\n",
    "        \n",
    "        if abs_rel=='abs':\n",
    "            target_return=abs_return\n",
    "        else:\n",
    "\n",
    "            if period_tuple[0]<0 and len(index_df.loc[:date].dropna())-1<=abs(period_tuple[0]):\n",
    "                start_index=index_df.dropna().iloc[0]\n",
    "            else:\n",
    "                start_index=index_df.iloc[:day0_index+period_tuple[0]].dropna().iloc[-1]\n",
    "                end_index=index_df.iloc[:day0_index+period_tuple[0]+period_tuple[1]].dropna().iloc[-1]\n",
    "                \n",
    "            if start_index!=0:\n",
    "                index_return=end_index/start_index-1\n",
    "            else:\n",
    "                index_return=None\n",
    "\n",
    "            if abs_return is None or index_return is None:\n",
    "                target_return=None\n",
    "            else:\n",
    "                target_return=abs_return-index_return\n",
    "        return target_return\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_calc_log(ticker,date,price_df,index_df,period_tuple,abs_rel):\n",
    "    '''\n",
    "    Calculate log return from the period tuple\n",
    "    abs_rel: bool, abs_return if assigned abs\n",
    "    Day starts from zero, so 1 means start the return calculation one day after result, second element is the number of days\n",
    "    return calculation assumes enters on the close price of the prior day to day when the count starts\n",
    "    '''\n",
    "    if type(date)==float:\n",
    "        return None\n",
    "    elif type(date)==pd.Timestamp:\n",
    "        date=date.strftime(\"%d/%b/%Y\")\n",
    "    elif type(date)==str:\n",
    "        date=date\n",
    "    price_series=np.log(price_df.loc[ticker].dropna())\n",
    "    date_series=price_series.index.tolist()\n",
    "    \n",
    "    index_data_series=index_df.index.tolist()\n",
    "    \n",
    "    if date in date_series:\n",
    "        day0=date_series.index(date)\n",
    "        day0_index=index_data_series.index(date)\n",
    "        \n",
    "        if period_tuple[0]<0 and len(price_series.loc[:date].dropna())-1<=abs(period_tuple[0]):\n",
    "            start_price=price_series.dropna().iloc[0]\n",
    "        else:\n",
    "            start_price=price_series.iloc[:day0+period_tuple[0]].dropna().iloc[-1]\n",
    "        end_price=price_series.iloc[:day0+period_tuple[0]+period_tuple[1]].dropna().iloc[-1]\n",
    "        \n",
    "        #target_series=price_series.iloc[day0+period_tuple[0]-2:day0+period_tuple[0]+period_tuple[1]]\n",
    "        \n",
    "        if start_price!=0:\n",
    "            abs_return=end_price-start_price\n",
    "        else:\n",
    "            abs_return=None\n",
    "        \n",
    "\n",
    "        \n",
    "        if abs_rel=='abs':\n",
    "            target_return=abs_return\n",
    "        else:\n",
    "\n",
    "            if period_tuple[0]<0 and len(index_df.loc[:date].dropna())-1<=abs(period_tuple[0]):\n",
    "                start_index=np.log(index_df).dropna().iloc[0]\n",
    "            else:\n",
    "                start_index=np.log(index_df).iloc[:day0_index+period_tuple[0]].dropna().iloc[-1]\n",
    "                end_index=np.log(index_df).iloc[:day0_index+period_tuple[0]+period_tuple[1]].dropna().iloc[-1]\n",
    "                if start_index!=0:\n",
    "                    index_return=end_index-start_index\n",
    "                else:\n",
    "                    index_return=None\n",
    "\n",
    "            if abs_return is None or index_return is None:\n",
    "                target_return=None\n",
    "            else:\n",
    "                target_return=abs_return-index_return\n",
    "        return target_return\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quarter_transform(quarter):\n",
    "    '''\n",
    "    Take the raw quarter to Q1 to Q4\n",
    "    '''\n",
    "    if type(quarter)==float:\n",
    "        adj_quarter=None\n",
    "    else:\n",
    "        \n",
    "        if quarter[-2:]=='Q4' or quarter[-2:]==':A':\n",
    "            adj_quarter='Q4'\n",
    "        elif quarter[-2:]=='Q3' or quarter[-2:]=='C3':\n",
    "            adj_quarter='Q3'\n",
    "        elif quarter[-2:]=='Q2' or quarter[-2:]=='C2' or quarter[-2:]=='S1':\n",
    "            adj_quarter='Q2'\n",
    "        elif quarter[-2:]=='Q1' or quarter[-2:]=='C1':\n",
    "            adj_quarter='Q1'\n",
    "        else:\n",
    "            adj_quarter=None\n",
    "    return adj_quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_beta(ticker,date,return_df,index_df,length):\n",
    "    '''\n",
    "    calculate beta for individual stocks\n",
    "    '''\n",
    "    cov_matrix=np.cov(return_df.loc[ticker,:date].iloc[-length-1:-1],index_df.loc[:date].iloc[-length-1:-1])\n",
    "    \n",
    "    beta=cov_matrix[0][1]/cov_matrix[1][1]\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_vol(signal_column,return_df,vol_lookback):\n",
    "    '''\n",
    "    Calculate simple vol from signal tuple\n",
    "    '''\n",
    "    signal_series=return_df.loc[signal_column.name[0]]\n",
    "    location=signal_series.index.tolist().index(signal_column.name[1])\n",
    "    vol_range=min(vol_lookback,len(signal_series[:location]))\n",
    "    signal_vol=signal_series[location-vol_range-1:location].std()\n",
    "    return signal_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_vol(date,index_df,vol_lookback):\n",
    "    '''\n",
    "    Calculate simple vol from signal tuple\n",
    "    '''\n",
    "    location=index_df.index.tolist().index(date)\n",
    "    vol_range=min(vol_lookback,len(index_df.iloc[:location]))\n",
    "    signal_vol=index_df.iloc[location-vol_range-1:location].std()\n",
    "    return signal_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_universe(signal_df,start_datetime,end_datetime,old_position):\n",
    "    '''\n",
    "    Slice the signal_df, both the index and entry date have to be \n",
    "    '''\n",
    "    \n",
    "\n",
    "    signal_df=signal_df.loc[start_datetime:end_datetime]\n",
    "    \n",
    "    if old_position is True:  \n",
    "        adj_signal_df=signal_df\n",
    "    else:\n",
    "        entry=signal_df.apply(lambda x:datetime.strptime(x.name[1],\"%d/%b/%Y\"),axis=0)\n",
    "        period_evaluate=(entry>=start_datetime)&(entry<=end_datetime)\n",
    "        adj_signal_df=signal_df.loc[:,period_evaluate]\n",
    "    \n",
    "    \n",
    "    zero_index=pd.Series(1,index=pd.date_range(start_datetime,end_datetime,freq='B')).to_frame()\n",
    "    adj_signal_df=pd.concat([adj_signal_df.drop_duplicates(),zero_index],axis=1).iloc[:,:-1]\n",
    "    return adj_signal_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_filter_stop(signal_df,stop_level,return_df,vol_lookback,stop_type,index_df):\n",
    "    '''\n",
    "    Input - signal_df\n",
    "    Get the updated signal df after the stop loss\n",
    "    stop_type:abs,rel\n",
    "    \n",
    "    '''\n",
    "    if stop_type=='abs':\n",
    "        vol_row=signal_df.apply(lambda column:signal_vol(column,return_df,vol_lookback),axis=0)\n",
    "        signal_cum_nmove=((1+signal_df).cumprod()-1).ffill()/vol_row\n",
    "        signal_df_stop=signal_df[-(signal_cum_nmove.expanding().min().shift(1,axis=0)<-stop_level)]\n",
    "    elif stop_type=='rel':\n",
    "        if index_df.shape[1]==1:\n",
    "            signal_count=signal_df.copy()\n",
    "            signal_count[((signal_count)>0) | ((signal_count)<0)]=1.0\n",
    "            signal_hedge=signal_count.apply(lambda x:x.multiply(index_df.iloc[:,0],axis=0))\n",
    "            \n",
    "            vol_row=signal_df.apply(lambda x:signal_vol(x,return_df,vol_lookback),axis=0)\n",
    "            rel_signal_cum_nmove=((1+signal_df).cumprod()-(1+signal_hedge).cumprod()).ffill()/vol_row\n",
    "            signal_df_stop=signal_df[-(rel_signal_cum_nmove.expanding().min().shift(1,axis=0)<-stop_level)]\n",
    "        else:\n",
    "            signal_count=signal_df.copy()\n",
    "            signal_count[((signal_count)>0) | ((signal_count)<0)]=1.0\n",
    "            signal_hedge=signal_count.apply(lambda x:x.multiply(index_df[Asia_mapping.loc[x.name[0][-2:]].iloc[0]],axis=0))\n",
    "            \n",
    "            vol_row=signal_df.apply(lambda x:signal_vol(x,return_df,vol_lookback),axis=0)\n",
    "            rel_signal_cum_nmove=((1+signal_df).cumprod()-(1+signal_hedge).cumprod()).ffill()/vol_row\n",
    "            signal_df_stop=signal_df[-(rel_signal_cum_nmove.expanding().min().shift(1,axis=0)<-stop_level)]            \n",
    "            \n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    return signal_df_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revision_adjusted_size(reference_signal_df,lower_revision,higher_revision,size_multiple,revision_row,revision_row_reference,\\\n",
    "                           gross,long):\n",
    "    \n",
    "    ''' \n",
    "    Use positive size\n",
    "    '''\n",
    "    \n",
    "    lower_size=0.01\n",
    "    higher_size=lower_size*size_multiple\n",
    "\n",
    "    if long is True:\n",
    "        size_row_reference=revision_row_reference.to_frame().copy().apply(lambda x: lower_size+(higher_size-lower_size)\\\n",
    "                                                      *(np.abs(x.iloc[0]-lower_revision))/np.abs(higher_revision-lower_revision) \\\n",
    "                                                      if np.abs(x.iloc[0])<=np.abs(higher_revision) else higher_size,axis=1)\n",
    "\n",
    "        size_df_reference=(1+reference_signal_df).cumprod()*size_row_reference\n",
    "\n",
    "        trial_gross=np.abs(size_df_reference.sum(axis=1).mean())\n",
    "        new_lower_size=lower_size/(trial_gross*100/gross)\n",
    "        new_higher_size=higher_size/(trial_gross*100/gross)\n",
    "\n",
    "        size_row=revision_row.to_frame().copy().apply(lambda x: new_lower_size+(new_higher_size-new_lower_size)\\\n",
    "                                                      *(np.abs(x.iloc[0]-lower_revision))/np.abs(higher_revision-lower_revision) \\\n",
    "                                                      if np.abs(x.iloc[0])<=np.abs(higher_revision) else new_higher_size,axis=1)\n",
    "    else:\n",
    "        size_row_reference=revision_row_reference.to_frame().copy().apply(lambda x: lower_size+(higher_size-lower_size)\\\n",
    "                                                      *(np.abs(x.iloc[0]-lower_revision))/np.abs(higher_revision-lower_revision) \\\n",
    "                                                      if np.abs(x.iloc[0])<=np.abs(lower_revision) else higher_size,axis=1)\n",
    "\n",
    "        size_df_reference=(1-reference_signal_df).cumprod()*size_row_reference\n",
    "\n",
    "        trial_gross=np.abs(size_df_reference.sum(axis=1).mean())\n",
    "        new_lower_size=lower_size/(trial_gross*100/gross)\n",
    "        new_higher_size=higher_size/(trial_gross*100/gross)\n",
    "\n",
    "        size_row=revision_row.to_frame().copy().apply(lambda x: new_lower_size+(new_higher_size-new_lower_size)\\\n",
    "                                                      *(np.abs(x.iloc[0]-lower_revision))/np.abs(higher_revision-lower_revision) \\\n",
    "                                                      if np.abs(x.iloc[0])<=np.abs(lower_revision) else new_higher_size,axis=1)\n",
    "\n",
    "    return size_row, new_lower_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sizing(signal_df,reference_signal_df,gross,fundamental_df,new_signal,return_df,risk_parity,liquidity,capital,\\\n",
    "           revision_adjust,long):\n",
    "    '''\n",
    "    Use historical signal_df range to calculate the size row for the current signal_df range\n",
    "    Idea is to use historical as a benchmark for future sizing\n",
    "    '''\n",
    "    \n",
    "    fundamental_df=fundamental_df.copy().sort_index()\n",
    "    vol_reference=reference_signal_df.apply(lambda x:signal_vol(x,return_df,30),axis=0).mean()\n",
    "    vol_row=signal_df.apply(lambda x:signal_vol(x,return_df,30),axis=0)\n",
    "    \n",
    "    '''\n",
    "    Revision row needs to be updated using reference_signal\n",
    "    new sizing scheme is a linear function\n",
    "    revision_adjust=(True/False,lower_revision,higher_revision,size_multiple)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    if revision_adjust[0] is True:\n",
    "        \n",
    "        if new_signal is True:     \n",
    "            revision_row_reference=fundamental_df.loc[reference_signal_df.columns][\"Revision_real\"]\n",
    "            revision_row=fundamental_df.loc[signal_df.columns][\"Revision_real\"]\n",
    "        else:\n",
    "            revision_row_reference=fundamental_df.loc[reference_signal_df.columns][\"Revision_20\"]\n",
    "            revision_row=fundamental_df.loc[signal_df.columns][\"Revision_20\"]           \n",
    "        \n",
    "        if long is True and revision_adjust[1] is not None:\n",
    "            lower_revision=revision_adjust[1][0]\n",
    "            higher_revision=revision_adjust[1][1]\n",
    "            size_multiple=revision_adjust[3]\n",
    "        \n",
    "            base_size,low_size=revision_adjusted_size(reference_signal_df,lower_revision,higher_revision,size_multiple,\\\n",
    "                                                      revision_row,revision_row_reference,gross,True)\n",
    "            if risk_parity is True:\n",
    "                size_row=signal_df.apply(lambda x: min(base_size[x.name]/(vol_row[x.name]/vol_reference),\\\n",
    "                                                       fundamental_df.loc[x.name[0],x.name[1]][\"Volume\"].iloc[0]*\\\n",
    "                                                       liquidity/capital),axis=0)\n",
    "            else:\n",
    "                size_row=signal_df.apply(lambda x: min(base_size[x.name], fundamental_df.loc[x.name[0],x.name[1]][\"Volume\"].iloc[0]\\\n",
    "                                                       *liquidity/capital),axis=0)            \n",
    "\n",
    "        elif long is False and revision_adjust[2] is not None:\n",
    "            lower_revision=revision_adjust[2][0]\n",
    "            higher_revision=revision_adjust[2][1]\n",
    "            size_multiple=revision_adjust[3]\n",
    "        \n",
    "            base_size,low_size=revision_adjusted_size(reference_signal_df,lower_revision,higher_revision,size_multiple,revision_row,\\\n",
    "                                             revision_row_reference,gross,False) \n",
    "\n",
    "            if risk_parity is True:\n",
    "                size_row=signal_df.apply(lambda x: min(base_size[x.name]/(vol_row[x.name]/vol_reference),\\\n",
    "                                                       fundamental_df.loc[x.name[0],x.name[1]][\"Volume\"].iloc[0]*\\\n",
    "                                                       liquidity/capital),axis=0)\n",
    "            else:\n",
    "                size_row=signal_df.apply(lambda x: min(base_size[x.name], fundamental_df.loc[x.name[0],x.name[1]][\"Volume\"].iloc[0]\\\n",
    "                                                       *liquidity/capital),axis=0)\n",
    "\n",
    "        else:\n",
    "            size_row=None\n",
    "            low_size=None\n",
    "\n",
    "    elif revision_adjust[0]=='constant':\n",
    "        if risk_parity is True:\n",
    "            size_row=signal_df.apply(lambda x: min(revision_adjust[1]/(vol_row[x.name]/vol_reference),\\\n",
    "                                                   fundamental_df.loc[x.name[0],x.name[1]][\"Volume\"].iloc[0]*\\\n",
    "                                                   liquidity/capital),axis=0)\n",
    "        else:\n",
    "            size_row=signal_df.apply(lambda x: min(revision_adjust[1], fundamental_df.loc[x.name[0],x.name[1]][\"Volume\"].iloc[0]\\\n",
    "                                                   *liquidity/capital),axis=0)\n",
    "        low_size=None\n",
    "\n",
    "    else:\n",
    "        number=reference_signal_df.count(axis=1).mean()\n",
    "        avg_size=gross/100/number\n",
    "\n",
    "        if risk_parity is True:\n",
    "            size_row=signal_df.apply(lambda x: min(avg_size/(vol_row[x.name]/vol_reference),\\\n",
    "                                                   fundamental_df.loc[x.name[0],x.name[1]][\"Volume\"].iloc[0]*\\\n",
    "                                                   liquidity/capital),axis=0)\n",
    "        else:\n",
    "            size_row=signal_df.apply(lambda x: min(avg_size, fundamental_df.loc[x.name[0],x.name[1]][\"Volume\"].iloc[0]\\\n",
    "                                                   *liquidity/capital),axis=0)\n",
    "        low_size=None\n",
    "    return size_row,low_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trading_analytics_date(portfolio_cache):\n",
    "    '''\n",
    "    Key portfolio metrics from portfolio cache\n",
    "    Feed into plot function\n",
    "    '''\n",
    "    \n",
    "    ind_return=portfolio_cache[3]\n",
    "    signal_count=len(ind_return)\n",
    "    account_curve=portfolio_cache[1]\n",
    "    \n",
    "    if signal_count==0:\n",
    "        return None,None,None,None,None,None,None\n",
    "    else:\n",
    "        mean_return=ind_return.mean()\n",
    "        hit_rate=len(ind_return[ind_return>0])/len(ind_return)*1.0\n",
    "        payoff_ratio=ind_return[ind_return>0].mean()/ind_return[ind_return<0].mean()*-1.0\n",
    "        \n",
    "        account_price=account_curve+1\n",
    "        ann_vol=np.std(account_price.diff()/account_price.shift(1))*(260**0.5)\n",
    "        ann_ret=(account_price.iloc[-1]**(1/len(account_price)))**260-1\n",
    "        ann_sharpe=ann_ret/ann_vol\n",
    "        \n",
    "        max_dd=-((1+account_curve)-(1+account_curve).cummax(axis=0)).expanding().min().min()\n",
    "        \n",
    "        #low_date=(np.maximum.accumulate(account_curve)-account_curve).idxmax()\n",
    "        #high_date=account_curve[:low_date].idxmax()\n",
    "        #max_dd=1-(1+account_curve[low_date])/(1+account_curve[high_date])\n",
    "        \n",
    "        return signal_count,hit_rate,payoff_ratio,ann_ret,ann_vol,ann_sharpe,max_dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trading_analytics_simp(account_curve):\n",
    "    '''\n",
    "    Key portfolio metrics from portfolio account curve\n",
    "    Only sharpe and drawdown\n",
    "    '''\n",
    "\n",
    "\n",
    "    account_price=account_curve+1\n",
    "    ann_vol=np.std(account_price.diff()/account_price.shift(1))*(260**0.5)\n",
    "    ann_ret=(account_price.iloc[-1]**(1/len(account_price)))**260-1\n",
    "    ann_sharpe=ann_ret/ann_vol\n",
    "\n",
    "    max_dd=-((1+account_curve)-(1+account_curve).cummax(axis=0)).expanding().min().min()\n",
    "\n",
    "    #low_date=(np.maximum.accumulate(account_curve)-account_curve).idxmax()\n",
    "    #high_date=account_curve[:low_date].idxmax()\n",
    "    #max_dd=1-(1+account_curve[low_date])/(1+account_curve[high_date])\n",
    "\n",
    "    return ann_sharpe,max_dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_signal(title,figsize,portfolio_cache):\n",
    "\n",
    "    account_curve=portfolio_cache[1]\n",
    "    avg_size=np.abs(portfolio_cache[2]).mean(axis=0).mean()\n",
    "    ind_return=portfolio_cache[3]\n",
    "    gross=portfolio_cache[4]\n",
    "    turnover=portfolio_cache[5]\n",
    "\n",
    "    fig=plt.figure(figsize=figsize)\n",
    "    ax1=fig.add_subplot(1,1,1)\n",
    "    ln1=ax1.plot(account_curve,label='signal',color='b')\n",
    "\n",
    "    val1=ax1.get_yticks()\n",
    "    start=val1[0]\n",
    "    end=val1[-1]\n",
    "    ax1.set_yticks(np.arange(start,end,0.1))  \n",
    "    adj_val1=ax1.get_yticks()\n",
    "    ax1.set_yticklabels([\"{:.1%}\".format(x) for x in adj_val1])\n",
    "\n",
    "    ax2=ax1.twinx()\n",
    "    ln2=ax2.plot(gross,label='gross',color='silver')\n",
    "\n",
    "    val2=ax2.get_yticks()\n",
    "    start=val2[0]\n",
    "    end=val2[-1]\n",
    "    ax2.set_yticks(np.arange(start,end,0.3))  \n",
    "    adj_val2=ax2.get_yticks()\n",
    "    ax2.set_yticklabels([\"{:.0%}\".format(x) for x in adj_val2])\n",
    "\n",
    "    count,hit,payoff,ret,vol,sharpe,max_dd=trading_analytics_date(portfolio_cache)\n",
    "\n",
    "    plt.title(\"\\n\".join(textwrap.wrap('count='+str(count)+\n",
    "                             ',avg_size='+str(\"{:.1%}\".format(avg_size))+\n",
    "                             ',hit_rate='+str(\"{:.0%}\".format(hit))+\n",
    "                             ',payoff='+str(round(payoff,1))+\n",
    "                             ',return='+str(\"{:.1%}\".format(ret))+\n",
    "                             ',vol='+str(\"{:.1%}\".format(vol))+\n",
    "                             ',sharpe='+str(round(sharpe,1))+\n",
    "                             ',turnover='+str(round(turnover,1))+'x'+                             \n",
    "                             ',max_drawdown='+str(\"{:.1%}\".format(max_dd)))),fontsize=10)\n",
    "\n",
    "    ax1.set_xlabel('Year')\n",
    "    ax1.set_ylabel('Return')\n",
    "    ax2.set_ylabel('Exposure')\n",
    "    plt.suptitle(title,y=1.05,fontsize=16)\n",
    "    plt.grid(linestyle='dashed')\n",
    "    plt.legend(ln1+ln2,[l.get_label() for l in ln1+ln2],loc=2)\n",
    "    ax1.axhline(y=0,color='k')\n",
    "\n",
    "    plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
