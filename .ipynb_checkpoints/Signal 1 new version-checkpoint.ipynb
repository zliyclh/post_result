{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Packages import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from textwrap import wrap\n",
    "import seaborn as sns\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import pyodbc \n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Connect to post result SQL server'''\n",
    "cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER=dsa-ln-WS015\\SQLEXPRESS;DATABASE=Post_result;Trusted_Connection=yes')   \n",
    "cursor = cnxn.cursor() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SQL_fundamentals(region):\n",
    "    '''\n",
    "    Grab the fundamental data from SQL database\n",
    "    region= US, Europe,Asia,or Canada\n",
    "    return the post result fundamental dataframe\n",
    "    '''\n",
    "    sql=\"select * from {0}\".format(region)\n",
    "    data=pd.read_sql(sql,cnxn).set_index(\"Ticker\").drop_duplicates().replace('#N/A Invalid Security','')\n",
    "    \n",
    "    def SQL_fundamental_date_transform(SQL_date):\n",
    "        '''\n",
    "        Transform the SQL fundamental style string into dateframe string style\n",
    "        '''\n",
    "        timestamp=pd.Timestamp(int(SQL_date[:4]),\n",
    "                            int(SQL_date[5:7]),\n",
    "                            int(SQL_date[8:]))\n",
    "        return timestamp\n",
    "    \n",
    "    data=data[data.index!='']\n",
    "    data[\"Date\"]=[SQL_fundamental_date_transform(i) for i in data[\"Date\"]]\n",
    "    data[\"ticker_copy\"]=data.index\n",
    "    data=data.sort_values(by=[\"ticker_copy\",\"Date\"])\n",
    "    data[\"next_date\"]=data[\"Date\"].shift(-1)\n",
    "    data[\"ticker_copy\"]=data[\"ticker_copy\"].shift(-1)\n",
    "    data[\"Date\"]=data[\"Date\"].apply(lambda x: x.strftime(\"%d/%b/%Y\") if x!='' else np.nan)\n",
    "    data[\"Next\"]=data.apply(lambda x: x[\"next_date\"].strftime(\"%d/%b/%Y\") if x.name==x[\"ticker_copy\"] else np.nan,axis=1)\n",
    "\n",
    "    del data[\"ticker_copy\"]\n",
    "    del data[\"next_date\"]\n",
    "     \n",
    "    data.index=pd.MultiIndex.from_tuples(data.apply(lambda x:(x.name,x[\"Date\"],x[\"Next\"]),axis=1))\n",
    "    \n",
    "    for s in [\"Revision\",\"Market cap\",\"Volume\",\"EPS\",\"Broker\"]:\n",
    "        data[s]=pd.to_numeric(data[s])\n",
    "    \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SQL_price(region,start,end):\n",
    "    '''\n",
    "    Grab the pricing data from SQL database\n",
    "    region= US, Europe,Asia,or Canada\n",
    "    start,end are in year\n",
    "    key is the reference to search\n",
    "    return the target price dataframe with timestamp on the column\n",
    "    '''\n",
    "    mylist=[]\n",
    "    for year in range(start,end+1):\n",
    "        sql=\"select * from {0}_price_{1}\".format(region,year)\n",
    "        data=pd.read_sql(sql,cnxn).set_index(\"Ticker\").drop_duplicates()\n",
    "        adj_data=data.loc[[x for x in data.index if len(x)>0]].replace('#N/A N/A','')\n",
    "        mylist.append(adj_data)\n",
    "\n",
    "    price=pd.concat(mylist,axis=1)\n",
    "    price=price.apply(lambda x:pd.to_numeric(x),axis=1)\n",
    "    \n",
    "    \n",
    "    def SQL_price_date_transform(SQL_date):\n",
    "        '''\n",
    "        Transform the SQL price style string into dateframe string style\n",
    "        '''\n",
    "        timestamp=pd.Timestamp(int(SQL_date[SQL_date.find(\" \",3):]),\n",
    "                            int(SQL_date[:SQL_date.find(\" \")]),\n",
    "                            int(SQL_date[SQL_date.find(\" \",1):SQL_date.find(\" \",3)]))\n",
    "        return timestamp.strftime(\"%d/%b/%Y\")\n",
    "    \n",
    "    price.columns=[SQL_price_date_transform(i) for i in price.columns]\n",
    "    return price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "US=SQL_fundamentals(\"US\")\n",
    "US_price=SQL_price('US',2006,2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EAR_calc(ticker,date,return_df,EAR_period,vol_lookback):\n",
    "    '''\n",
    "    Calculate EAR from ticker and reference date\n",
    "    '''\n",
    "    if type(date)==pd._libs.tslib.NaTType:\n",
    "        return None\n",
    "    elif type(date)==pd._libs.tslib.Timestamp:\n",
    "        date=date.strftime(\"%d/%b/%Y\")\n",
    "    elif type(date)==str:\n",
    "        date=date\n",
    "    return_series=return_df.loc[ticker].dropna()\n",
    "    date_series=return_series.index.tolist()\n",
    "    if date in date_series:\n",
    "        day0=date_series.index(date)\n",
    "        post_series=return_series.iloc[day0:]\n",
    "        pre_series=return_series.iloc[:day0]\n",
    "        vol= return_series.iloc[day0-min(len(pre_series),vol_lookback+1):day0].std()\n",
    "        ret=(return_series.iloc[day0:day0+EAR_period]+1).prod()-1\n",
    "        nmove=ret/vol\n",
    "        return nmove\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_vol(signal_column,return_df,vol_lookback):\n",
    "    '''\n",
    "    Calculate simple vol from signal tuple\n",
    "    '''\n",
    "    signal_series=return_df.loc[signal_column.name[0]]\n",
    "    location=signal_series.index.tolist().index(signal_column.name[1])\n",
    "    vol_range=min(vol_lookback,len(signal_series[:location]))\n",
    "    signal_vol=signal_series[location-vol_range-1:location].std()\n",
    "    return signal_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_universe(signal_df,start_datetime,end_datetime):\n",
    "    '''\n",
    "    Slice the signal_df, both the index and entry date have to be \n",
    "    '''\n",
    "    \n",
    "    entry=signal_df.apply(lambda x:datetime.strptime(x.name[1],\"%d/%b/%Y\"),axis=0)\n",
    "    period_evaluate=(entry>=start_datetime)&(entry<=end_datetime)\n",
    "    adj_signal_df=signal_df.loc[:,period_evaluate]\n",
    "    adj_signal_df=adj_signal_df.loc[start_datetime:end_datetime]\n",
    "    \n",
    "    zero_index=pd.date_range(start_datetime,end_datetime,freq='B').to_frame()\n",
    "    adj_signal_df=pd.concat([adj_signal_df,zero_index],axis=1).iloc[:,:-1]\n",
    "    return adj_signal_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_filter_stop(signal_df,stop_level,return_df,vol_lookback):\n",
    "    '''\n",
    "    Input - signal_df\n",
    "    Get the updated signal df after the stop loss\n",
    "    '''\n",
    "    \n",
    "    vol_row=signal_df.apply(lambda column:signal_vol(column,return_df,vol_lookback),axis=0)\n",
    "    signal_cum_nmove=((1+signal_df).cumprod()-1).ffill()/vol_row\n",
    "    signal_df_stop=signal_df[-(signal_cum_nmove.expanding().min().shift(1,axis=0)<-stop_level)]\n",
    "    return signal_df_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sizing(signal_df,reference,gross,fundamental_df,return_df,risk_parity,liquidity,capital):\n",
    "    '''\n",
    "    Use historical signal_df range to calculate the size row for the current signal_df range\n",
    "    Idea is to use historical as a benchmark for future sizing\n",
    "    '''\n",
    "    number=reference.count(axis=1).mean()\n",
    "    avg_size=gross/100/number\n",
    "    vol_reference=reference.apply(lambda column:signal_vol(column,return_df,30),axis=0).mean()\n",
    "    vol_row=signal_df.apply(lambda column:signal_vol(column,return_df,30),axis=0)\n",
    "    \n",
    "    fundamental_df=fundamental_df.sort_index()\n",
    "    if risk_parity is True:\n",
    "        size_row=signal_df.apply(lambda x: min(avg_size/(vol_row[x.name]/vol_reference),\\\n",
    "                                               fundamental_df.loc[x.name[0],x.name[1]][\"Volume\"].iloc[0]*\\\n",
    "                                               liquidity/capital),axis=0)\n",
    "    else:\n",
    "        size_row=signal_df.apply(lambda x: min(avg_size, fundamental_df.loc[x.name[0],x.name[1]][\"Volume\"].iloc[0]\\\n",
    "                                               *liquidity/capital),axis=0)\n",
    "    return size_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trading_analytics_date(portfolio_cache):\n",
    "    '''\n",
    "    Key portfolio metrics from portfolio cache\n",
    "    Feed into plot function\n",
    "    '''\n",
    "    \n",
    "    ind_return=portfolio_cache[3]\n",
    "    signal_count=len(ind_return)\n",
    "    account_curve=portfolio_cache[1]\n",
    "    \n",
    "    if signal_count==0:\n",
    "        return None,None,None,None,None,None,None\n",
    "    else:\n",
    "        mean_return=ind_return.mean()\n",
    "        hit_rate=len(ind_return[ind_return>0])/len(ind_return)*1.0\n",
    "        payoff_ratio=ind_return[ind_return>0].mean()/ind_return[ind_return<0].mean()*-1.0\n",
    "        \n",
    "        account_price=account_curve+1\n",
    "        ann_vol=np.std(account_price.diff()/account_price.shift(1))*(260**0.5)\n",
    "        ann_ret=np.mean(account_price.diff()/account_price.shift(1))*260\n",
    "        ann_sharpe=ann_ret/ann_vol\n",
    "        \n",
    "        max_dd=-((1+account_curve)/(1+account_curve).cummax(axis=0)-1).expanding().min().min()\n",
    "        \n",
    "        #low_date=(np.maximum.accumulate(account_curve)-account_curve).idxmax()\n",
    "        #high_date=account_curve[:low_date].idxmax()\n",
    "        #max_dd=1-(1+account_curve[low_date])/(1+account_curve[high_date])\n",
    "        \n",
    "        return signal_count,hit_rate,payoff_ratio,ann_ret,ann_vol,ann_sharpe,max_dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Signal class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class signal(object):\n",
    "    '''\n",
    "    Signal class is built to initialize the signal_df and account curve from base parameters \n",
    "    '''\n",
    "    def __init__(self,fundamental_df,price_df,EAR_period,long_criteria,short_criteria,holding,start,end,\\\n",
    "                long_size=None,short_size=None):\n",
    "        '''\n",
    "        Define the key free parameters of the signal\n",
    "        Criteria:(EAR_low,EAR_high,revision_low,revision_high,size_low,size_high)\n",
    "        '''\n",
    "        self.fundamental_df=fundamental_df\n",
    "        self.price_df=price_df\n",
    "        self.abs_return=price_df.diff(1,axis=1)/price_df.shift(1,axis=1)\n",
    "        self.EAR_period=EAR_period\n",
    "        self.long_criteria=long_criteria\n",
    "        self.short_criteria=short_criteria\n",
    "        self.holding=holding\n",
    "        self.start=start\n",
    "        self.end=end\n",
    "        self.long_size=long_size\n",
    "        self.short_size=short_size\n",
    "        \n",
    "    def signal_base(self):\n",
    "        '''\n",
    "        Filter the EAR and revision, get the target signal list\n",
    "        For both long and short side\n",
    "        '''\n",
    "        \n",
    "        if self.long_criteria is None:\n",
    "            long_base=None\n",
    "        else:\n",
    "            \n",
    "            target_long=self.fundamental_df[(self.fundamental_df[\"Revision\"]>self.long_criteria[2])\\\n",
    "                                                &(self.fundamental_df[\"Revision\"]<self.long_criteria[3])]\n",
    "\n",
    "            if self.long_size is None:\n",
    "                pass\n",
    "            else:\n",
    "                target_long=target_long[(target_long[\"Market cap\"]>self.long_size[0])&\\\n",
    "                                        (target_long[\"Market cap\"]<self.long_size[1])]\n",
    "\n",
    "            target_long[\"EAR\"]=target_long.apply(lambda x:EAR_calc(x.name[0],x.name[1],self.abs_return,self.EAR_period,30)\\\n",
    "                                         if x.name[0] in self.abs_return.index else None,axis=1)\n",
    "            \n",
    "            long_base=target_long[(target_long[\"EAR\"]>self.long_criteria[0])&(target_long[\"EAR\"]<self.long_criteria[1])]\n",
    "            \n",
    "        if self.short_criteria is None:\n",
    "            short_base=None\n",
    "        else:\n",
    "            \n",
    "            target_short=self.fundamental_df[(self.fundamental_df[\"Revision\"]>self.short_criteria[2])\\\n",
    "                                                &(self.fundamental_df[\"Revision\"]<self.short_criteria[3])]\n",
    "\n",
    "            if self.short_size is None:\n",
    "                pass\n",
    "            else:\n",
    "                target_short=target_short[(target_short[\"Market cap\"]>self.short_size[0])&\\\n",
    "                                          (target_short[\"Market cap\"]<self.short_size[1])]\n",
    "\n",
    "            target_short[\"EAR\"]=target_short.apply(lambda x:EAR_calc(x.name[0],x.name[1],self.abs_return,self.EAR_period,30)\\\n",
    "                                         if x.name[0] in self.abs_return.index else None,axis=1)\n",
    "            \n",
    "            short_base=target_short[(target_short[\"EAR\"]>self.short_criteria[0])&(target_short[\"EAR\"]<self.short_criteria[1])]\n",
    "        return long_base,short_base\n",
    "    \n",
    "    def signal_df_date(self):#if we hold them through next earning\n",
    "        '''\n",
    "        Obtain the signal_df function over the whole time period\n",
    "        '''\n",
    "        long_base,short_base=signal.signal_base(self)\n",
    "        \n",
    "        if long_base is None:\n",
    "            long_df=None\n",
    "        \n",
    "        else:\n",
    "            long_df=pd.DataFrame(index=self.price_df.columns)\n",
    "\n",
    "            for s in long_base.index:\n",
    "                return_series=self.abs_return.loc[s[0]]\n",
    "                if s[1] in return_series.index:\n",
    "                    if not np.isnan(return_series.loc[s[1]]): \n",
    "                        day0=return_series.index.tolist().index(s[1])\n",
    "                        if type(s[2])==float:##basically np.nan has type float\n",
    "                            period=self.holding\n",
    "                        else: ##assume that we are not holding through numbers\n",
    "                            period=min(self.holding,np.busday_count(datetime.strptime(s[1],\"%d/%b/%Y\"),\\\n",
    "                                                                    datetime.strptime(s[2],\"%d/%b/%Y\"))-self.EAR_period)\\\n",
    "                            #EAR period as entry\n",
    "                        target_series=return_series.iloc[day0+self.EAR_period-1:day0+min(period+self.EAR_period, \\\n",
    "                                                                                       len(return_series[day0:]))].dropna()\n",
    "                        if len(target_series)==0:\n",
    "                            pass\n",
    "                        else:\n",
    "                            target_series.iloc[0]=0.0\n",
    "                            long_df[s]=target_series                        \n",
    "\n",
    "            long_df=long_df.reindex(datetime.strptime(i,\"%d/%b/%Y\") for i in long_df.index)\n",
    "\n",
    "            if self.start is not None:\n",
    "                long_df=slice_universe(long_df,self.start,self.end)\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        if short_base is None:\n",
    "            short_df=None\n",
    "        \n",
    "        else:\n",
    "            short_df=pd.DataFrame(index=self.price_df.columns)\n",
    "\n",
    "            for s in short_base.index:\n",
    "                return_series=self.abs_return.loc[s[0]]\n",
    "                if s[1] in return_series.index:\n",
    "                    if not np.isnan(return_series.loc[s[1]]): \n",
    "                        day0=return_series.index.tolist().index(s[1])\n",
    "                        if type(s[2])==float:##basically np.nan has type float\n",
    "                            period=self.holding\n",
    "                        else: ##assume that we are not holding through numbers\n",
    "                            period=min(self.holding,np.busday_count(datetime.strptime(s[1],\"%d/%b/%Y\"),\\\n",
    "                                                                    datetime.strptime(s[2],\"%d/%b/%Y\"))-self.EAR_period)\\\n",
    "                            #EAR period as entry\n",
    "                        target_series=return_series.iloc[day0+self.EAR_period-1:day0+min(period+self.EAR_period, \\\n",
    "                                                                                       len(return_series[day0:]))].dropna()\n",
    "                        if len(target_series)==0:\n",
    "                            pass\n",
    "                        else:\n",
    "                            target_series.iloc[0]=0.0\n",
    "                            short_df[s]=target_series                        \n",
    "\n",
    "            short_df=short_df.reindex(datetime.strptime(i,\"%d/%b/%Y\") for i in short_df.index)\n",
    "\n",
    "            if self.start is not None:\n",
    "                short_df=slice_universe(short_df,self.start,self.end)\n",
    "            else:\n",
    "                pass        \n",
    "        \n",
    "        long_df=long_df.dropna(how=\"all\",axis=1)\n",
    "        short_df=short_df.dropna(how=\"all\",axis=1)\n",
    "        \n",
    "        self.long_df=long_df\n",
    "        self.short_df=short_df\n",
    "        \n",
    "        return long_df,short_df\n",
    "\n",
    "    def signal_account(self,stop,gross,index_df,net_level,risk_parity,liquidity,capital):\n",
    "        '''\n",
    "        Build the account curve with signal_df\n",
    "        Assume quarterly rebalancing that's why the period list has quarter as the key\n",
    "        Take extra care when building the account curve, the logic is: work out the size_df, then shift by 1 and * signal_df\n",
    "        Stop=(long_stop,short_stop)\n",
    "        '''\n",
    "        \n",
    "        try:\n",
    "            long_df=self.long_df\n",
    "            short_df=self.short_df\n",
    "            \n",
    "        except AttributeError:\n",
    "            long_df,short_df=signal.signal_df_date(self)\n",
    "                   \n",
    "        '''\n",
    "        Assign values for later use\n",
    "        '''\n",
    "        self.stop=stop\n",
    "        self.gross=gross\n",
    "        self.index_df=index_df\n",
    "        self.net_level=net_level\n",
    "        self.risk_parity=risk_parity\n",
    "        self.liquidity=liquidity\n",
    "        self.capital=capital\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Define rebalance period first\n",
    "        '''\n",
    "        period=long_df.apply(lambda x:str(x.name.year)+\" \"+str(x.name.quarter),axis=1)\n",
    "        period_list=list(set(period))\n",
    "        period_list.sort()    \n",
    "        \n",
    "        '''\n",
    "        Separate out long and short\n",
    "        '''\n",
    "        if long_df is None:\n",
    "            long_cache=(None,None,None,None)\n",
    "        else:\n",
    "            if stop is None:\n",
    "                pass\n",
    "            else:\n",
    "                long_df=signal_filter_stop(long_df,stop[0],self.abs_return,30)   \n",
    "\n",
    "            long_sub_signal={}\n",
    "            long_sub_size_row={}\n",
    "            long_sub_size_df={}\n",
    "            long_sub_pnl={}\n",
    "        \n",
    "            for s in period_list:\n",
    "                long_sub_signal[s]=long_df[period==s].dropna(how='all',axis=1)\n",
    "                if period_list.index(s)<4:##use last quarter's sizing as reference\n",
    "                    long_sub_size_row[s]=sizing(long_sub_signal[s],long_sub_signal[s],gross[0],self.fundamental_df,\\\n",
    "                                                   self.abs_return,risk_parity,liquidity,capital)\n",
    "                else:\n",
    "                    long_sub_size_row[s]=sizing(long_sub_signal[s],long_sub_signal[period_list[period_list.index(s)-1]],\\\n",
    "                                                   gross[0],self.fundamental_df,self.abs_return,risk_parity,liquidity,capital)\n",
    "                long_sub_size_df[s]=(1+long_sub_signal[s]).cumprod()*long_sub_size_row[s]\n",
    "                long_sub_pnl[s]=(long_sub_size_df[s].shift(1))*long_sub_signal[s] \n",
    "                # need to shift by 1 as the size is end of the day\n",
    "        \n",
    "            long_daily_pnl=pd.concat(list(long_sub_pnl.values()),axis=0)\n",
    "            long_acct_curve=long_daily_pnl.cumsum().ffill().sum(axis=1)\n",
    "            long_size_df=pd.concat(list(long_sub_size_df.values()),axis=0)\n",
    "            long_ind_return=long_daily_pnl.cumsum().ffill().iloc[-1].dropna()\n",
    "            long_cache=(long_daily_pnl,long_acct_curve,long_size_df,long_ind_return)\n",
    "\n",
    "            \n",
    "        if short_df is None:\n",
    "            short_cache=(None,None,None,None)\n",
    "        else:\n",
    "            if stop is None:\n",
    "                pass\n",
    "            else:\n",
    "                short_df=-signal_filter_stop(-short_df,stop[1],self.abs_return,30)   \n",
    "\n",
    "            short_sub_signal={}\n",
    "            short_sub_size_row={}\n",
    "            short_sub_size_df={}\n",
    "            short_sub_pnl={}\n",
    "        \n",
    "            for s in period_list:\n",
    "                short_sub_signal[s]=short_df[period==s].dropna(how='all',axis=1)\n",
    "                if period_list.index(s)<4:##use last quarter's sizing as reference\n",
    "                    short_sub_size_row[s]=-sizing(short_sub_signal[s],short_sub_signal[s],gross[1],self.fundamental_df,\\\n",
    "                                                   self.abs_return,risk_parity,liquidity,capital)\n",
    "                else:\n",
    "                    short_sub_size_row[s]=-sizing(short_sub_signal[s],short_sub_signal[period_list[period_list.index(s)-1]],\\\n",
    "                                                   gross[1],self.fundamental_df,self.abs_return,risk_parity,liquidity,capital)\n",
    "                short_sub_size_df[s]=(1+short_sub_signal[s]).cumprod()*short_sub_size_row[s]\n",
    "                short_sub_pnl[s]=(short_sub_size_df[s].shift(1))*short_sub_signal[s] \n",
    "                # need to shift by 1 as the size is end of the day\n",
    "        \n",
    "            short_daily_pnl=pd.concat(list(short_sub_pnl.values()),axis=0)\n",
    "            short_acct_curve=short_daily_pnl.cumsum().ffill().sum(axis=1)\n",
    "            short_size_df=pd.concat(list(short_sub_size_df.values()),axis=0)\n",
    "            short_ind_return=short_daily_pnl.cumsum().ffill().iloc[-1].dropna()\n",
    "            \n",
    "            short_cache=(short_daily_pnl,short_acct_curve,short_size_df,short_ind_return)\n",
    "    \n",
    "        '''Put alpha positions together to form the alpha part'''\n",
    "        alpha_df=pd.concat([long_df,short_df],axis=1)\n",
    "        self.alpha_df=alpha_df\n",
    "        \n",
    "        alpha_daily_pnl=pd.concat([long_cache[0],short_cache[0]],axis=1)\n",
    "        alpha_acct_curve=alpha_daily_pnl.cumsum().ffill().sum(axis=1)\n",
    "        alpha_size_df=pd.concat([long_cache[2],short_cache[2]],axis=1)\n",
    "        alpha_ind_return=pd.concat([long_cache[3],short_cache[3]],axis=0)\n",
    "        \n",
    "        alpha_cache=(alpha_daily_pnl,alpha_acct_curve,alpha_size_df,alpha_ind_return)\n",
    "        \n",
    "        \n",
    "        if index_df is not None:\n",
    "            index_df=alpha_df.join(index_df).iloc[:,-1]\n",
    "            \n",
    "            index_size_df=net_level-alpha_size_df.sum(axis=1)\n",
    "            index_daily_pnl=index_size_df.shift(1)*index_df\n",
    "            index_acct_curve=index_daily_pnl.cumsum()\n",
    "            index_ind_return=index_acct_curve.iloc[-1]\n",
    "            index_cache=(index_daily_pnl,index_acct_curve,index_size_df,index_ind_return)\n",
    "        else:\n",
    "            index_cache=(None,None,None,None)\n",
    "            \n",
    "        '''Finally put everything together'''    \n",
    "        portfolio_df=alpha_df.join(index_df)\n",
    "        \n",
    "        portfolio_size_df=pd.concat([alpha_cache[2],index_cache[2]],axis=1)\n",
    "        portfolio_daily_pnl=pd.concat([alpha_cache[0],index_cache[0]],axis=1)\n",
    "        portfolio_acct_curve=portfolio_daily_pnl.cumsum().ffill().sum(axis=1)\n",
    "        portfolio_ind_return=alpha_cache[3]\n",
    "        \n",
    "        portfolio_gross=np.abs(portfolio_size_df).sum(axis=1)\n",
    "        portfolio_turnover=(np.abs(alpha_size_df.fillna(0.0).diff(1)).sum().sum())/(portfolio_size_df.shape[0]/260)\n",
    "        \n",
    "        portfolio_cache=(portfolio_daily_pnl,portfolio_acct_curve,portfolio_size_df,portfolio_ind_return,portfolio_gross,\\\n",
    "                         portfolio_turnover)\n",
    "        \n",
    "        self.portfolio_account=portfolio_cache #save for later use\n",
    "        \n",
    "        return long_cache,short_cache,alpha_cache,portfolio_cache\n",
    "    \n",
    "    def plot_account(self,title,figsize=[6,4]):\n",
    "        '''\n",
    "        Plot the account curve\n",
    "        '''\n",
    "        try:\n",
    "            portfolio_cache=self.portfolio_account\n",
    "            \n",
    "        except AttributeError:\n",
    "            print(\"Execute the signal_account first!\")  \n",
    "            return None        \n",
    "        \n",
    "        account_curve=portfolio_cache[1]\n",
    "        avg_size=np.abs(portfolio_cache[2]).mean(axis=0).mean()\n",
    "        ind_return=portfolio_cache[3]\n",
    "        gross=portfolio_cache[4]\n",
    "        turnover=portfolio_cache[5]\n",
    "        \n",
    "        fig=plt.figure(figsize=figsize)\n",
    "        ax1=fig.add_subplot(1,1,1)\n",
    "        ln1=ax1.plot(account_curve,label='signal',color='b')\n",
    "\n",
    "        val1=ax1.get_yticks()\n",
    "        start=val1[0]\n",
    "        end=val1[-1]\n",
    "        ax1.set_yticks(np.arange(start,end,0.1))  \n",
    "        adj_val1=ax1.get_yticks()\n",
    "        ax1.set_yticklabels([\"{:.0%}\".format(x) for x in adj_val1])\n",
    "\n",
    "        ax2=ax1.twinx()\n",
    "        ln2=ax2.plot(gross,label='gross',color='silver')\n",
    "\n",
    "        val2=ax2.get_yticks()\n",
    "        start=val2[0]\n",
    "        end=val2[-1]\n",
    "        ax2.set_yticks(np.arange(start,end,0.1))  \n",
    "        adj_val2=ax2.get_yticks()\n",
    "        ax2.set_yticklabels([\"{:.0%}\".format(x) for x in adj_val2])\n",
    "\n",
    "        count,hit,payoff,ret,vol,sharpe,max_dd=trading_analytics_date(portfoliio_cache)\n",
    "\n",
    "        plt.title(\"\\n\".join(wrap('count='+str(count)+\n",
    "                                 ',avg_size='+str(\"{:.1%}\".format(avg_size))+\n",
    "                                 ',hit_rate='+str(\"{:.0%}\".format(hit))+\n",
    "                                 ',payoff='+str(round(payoff,1))+\n",
    "                                 ',return='+str(\"{:.1%}\".format(ret))+\n",
    "                                 ',vol='+str(\"{:.1%}\".format(vol))+\n",
    "                                 ',sharpe='+str(round(sharpe,1))+\n",
    "                                 ',turnover='+str(round(turnover,1))+'x'+                             \n",
    "                                 ',max_drawdown='+str(\"{:.1%}\".format(max_dd)))),fontsize=10)\n",
    "\n",
    "        ax1.set_xlabel('Year')\n",
    "        ax1.set_ylabel('Return')\n",
    "        ax2.set_ylabel('Exposure')\n",
    "        plt.suptitle(title,y=1.05,fontsize=16)\n",
    "        plt.grid(linestyle='dashed')\n",
    "        plt.legend(ln1+ln2,[l.get_label() for l in ln1+ln2])\n",
    "        ax1.axhline(y=0,color='k')\n",
    "\n",
    "        plt.show()        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_day2_30d=signal(fundamental_df=US,\n",
    "                   price_df=US_price,\n",
    "                   EAR_period=2,\n",
    "                   long_criteria=(1,1000,0.05,1000),\n",
    "                   short_criteria=(-1000,-2,-1000,-0.03),\n",
    "                   holding=30,\n",
    "                   start=pd.Timestamp(2007,1,1),\n",
    "                   end=pd.Timestamp(2017,12,31),\n",
    "                   long_size=None,\n",
    "                   short_size=(5000,200000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execute the signal_account first\n"
     ]
    }
   ],
   "source": [
    "US_day2_30d.plot_account(\"trial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
