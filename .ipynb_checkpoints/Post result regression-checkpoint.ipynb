{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Packages import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sheet(dataframe):\n",
    "    '''\n",
    "    cleaning function for raw CSV import\n",
    "    '''\n",
    "    adj=dataframe.dropna(axis=1,how='all')\n",
    "    adj=dataframe.set_index(adj.columns[0]) #use stock tickers as the index\n",
    "    return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_date_transform(CSV_date,index=False):\n",
    "    '''\n",
    "    Transform the CSV price style string into dateframe string style\n",
    "    The CSV date follows US style which is MM/DD/YYYY\n",
    "    '''\n",
    "    if index==False:\n",
    "        timestamp=pd.Timestamp(int(CSV_date[CSV_date.find(\"/\",3)+1:]),\n",
    "                            int(CSV_date[:CSV_date.find(\"/\")]),\n",
    "                            int(CSV_date[CSV_date.find(\"/\",1)+1:CSV_date.find(\"/\",3)]))\n",
    "        return timestamp.strftime(\"%d/%b/%Y\")\n",
    "    else:\n",
    "        timestamp=pd.Timestamp(int(CSV_date[-4:]),int(CSV_date[3:5]),int(CSV_date[:2]))\n",
    "        return timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fundamental_date_transform(CSV_date):\n",
    "    '''\n",
    "    Transform the fundamental style string into dateframe string style\n",
    "    CSV date follow following style yyyy-mm-dd or MM/DD/YYYY\n",
    "    '''\n",
    "    if '-' in CSV_date:\n",
    "        timestamp=pd.Timestamp(int(CSV_date[:4]),\n",
    "                            int(CSV_date[5:7]),\n",
    "                            int(CSV_date[8:]))\n",
    "    else:\n",
    "        timestamp=pd.Timestamp(int(CSV_date[CSV_date.find(\"/\",3)+1:]),\n",
    "                               int(CSV_date[CSV_date.find(\"/\",1)+1:CSV_date.find(\"/\",3)]),\n",
    "                               int(CSV_date[:CSV_date.find(\"/\")]))        \n",
    "    return timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CSV_price(region,start,end,VWAP):\n",
    "    '''\n",
    "    Grab the pricing data from CSV\n",
    "    region= US, Europe,Asia,or Canada\n",
    "    start,end are in year\n",
    "    key is the reference to search\n",
    "    return the target price dataframe with timestamp on the column\n",
    "    '''\n",
    "    mylist=[]\n",
    "    for year in range(start,end+1):\n",
    "        if VWAP==False:\n",
    "            csv=pd.read_csv(r\"C:\\Users\\Eric.Li\\Documents\\Post result data\\{0} CSV\\{0}_price_{1}.csv\".format(region,year))\n",
    "        else:\n",
    "            csv=pd.read_csv(r\"C:\\Users\\Eric.Li\\Documents\\Post result data\\{0} CSV\\{0}_VWAP_{1}.csv\".format(region,year))\n",
    "        data=csv.set_index(\"Ticker\")\n",
    "        adj_data=data.loc[[x for x in data.index if type(x)==str]].replace('#N/A N/A','').replace(' #N/A N/A ','')\n",
    "        adj_data=adj_data.loc[[x for x in adj_data.index if len(x)>0]]\n",
    "        mylist.append(adj_data)\n",
    "\n",
    "    price=pd.concat(mylist,axis=1)\n",
    "    price=price.apply(lambda x:pd.to_numeric(x),axis=1)\n",
    "    \n",
    "    csv_index=pd.read_csv(r\"C:\\Users\\Eric.Li\\Documents\\Post result data\\{0} CSV\\{0}_price_index.csv\".format(region))\n",
    "    data_index=csv_index.set_index(\"Ticker\").T\n",
    "    price_index=data_index.replace('#N/A N/A','')\n",
    "    #price_index=data_index.apply(lambda x:pd.to_numeric(x),axis=1)\n",
    "    \n",
    "    price.columns=[price_date_transform(i) for i in price.columns]\n",
    "    '''\n",
    "    Need to sort the columns for index price, and then transform to date string\n",
    "    '''\n",
    "    price_index.columns=[price_date_transform(i) for i in price_index.columns]\n",
    "    #price_index=price_index.reindex(sorted(price_index.columns))\n",
    "    #price_index.columns=[i.strftime(\"%d/%b/%Y\") for i  in price_index.columns]\n",
    "    \n",
    "    abs_return=price.diff(1,axis=1)/price.shift(1,axis=1)\n",
    "    abs_return_index=price_index.diff(1,axis=1)/price_index.shift(1,axis=1)\n",
    "    return price,abs_return,price_index,abs_return_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CSV_EPS(region,start,end):\n",
    "    '''\n",
    "    Grab the pricing data from CSV database\n",
    "    region= US, Europe,Asia,or Canada\n",
    "    start,end are in year\n",
    "    key is the reference to search\n",
    "    return the target EPS dataframe with timestamp on the column\n",
    "    '''\n",
    "    mylist=[]\n",
    "    for year in range(start,end+1):\n",
    "        csv=pd.read_csv(r\"C:\\Users\\Eric.Li\\Documents\\Post result data\\{0} CSV\\{0}_EPS_{1}.csv\".format(region,year))\n",
    "        data=csv.set_index(\"Ticker\")\n",
    "        adj_data=data.loc[[x for x in data.index if type(x)==str]].replace('#N/A N/A','').replace(\" #N/A N/A \",\"\")\n",
    "        adj_data=adj_data.loc[[x for x in adj_data.index if len(x)>0]]\n",
    "        mylist.append(adj_data)\n",
    "\n",
    "    EPS=pd.concat(mylist,axis=1)\n",
    "    EPS=EPS.apply(lambda x:pd.to_numeric(x),axis=1)\n",
    "    \n",
    "    EPS.columns=[price_date_transform(i) for i in EPS.columns]\n",
    "    return EPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revision_calc(ticker,date,EPS_df,period):\n",
    "    '''\n",
    "    Calculate revision from ticker and reference date\n",
    "    '''\n",
    "    if type(date)==pd.lib.tslib.NaTType:\n",
    "        return None\n",
    "    elif type(date)==pd.lib.tslib.Timestamp:\n",
    "        date=date.strftime(\"%d/%b/%Y\")\n",
    "    elif type(date)==str:\n",
    "        date=date\n",
    "    \n",
    "    if ticker in EPS_df.index:\n",
    "        eps_series=EPS_df.loc[ticker]\n",
    "        date_series=eps_series.index.tolist()\n",
    "        if date in date_series:\n",
    "            day0=date_series.index(date)\n",
    "            post_series=eps_series.iloc[day0+period-1:day0+period+10]\n",
    "            pre_series=eps_series.iloc[day0-10:day0]\n",
    "            if len(post_series.dropna())==0 or len(pre_series.dropna())==0 or pre_series.dropna().iloc[-1]==0:\n",
    "                revision=None\n",
    "            else:\n",
    "                try:\n",
    "                    revision=np.divide(post_series.dropna().iloc[0],pre_series.dropna().iloc[-1])-1\n",
    "                except:\n",
    "                    revision=None\n",
    "            return revision\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CSV_fundamentals(region,price,EPS_df,revision_period,min_history,min_vol,use_cache):\n",
    "    '''\n",
    "    Grab the fundamental data from the spreadsheet\n",
    "    region= US, Europe,Asia,or Canada\n",
    "    return the post result fundamental dataframe\n",
    "    '''\n",
    "    \n",
    "    if use_cache is True:\n",
    "        try:\n",
    "            data=pd.read_csv(r'C:\\Users\\Eric.Li\\Documents\\Post result code\\{0}{1}_clean_reg.csv'.format(region,\\\n",
    "                                                                                                        str(revision_period)))\n",
    "            new_index=pd.MultiIndex.from_tuples(list(zip(data.iloc[:,0],data.iloc[:,1],data.iloc[:,2],data.iloc[:,3])))\n",
    "            data.index=new_index\n",
    "            target_data=data.iloc[:,4:]\n",
    "        except:\n",
    "            print(\"No such file!\")\n",
    "    else:\n",
    "        csv=pd.read_csv(r'C:\\Users\\Eric.Li\\Documents\\Post result code\\{0}.csv'.format(region))\n",
    "        data=csv.set_index(\"Ticker\").drop_duplicates().replace('#N/A Invalid Security','').\\\n",
    "        replace('#N/A Requesting Data...','')\n",
    "\n",
    "        data=data[data.index!='']\n",
    "        data=data.dropna(how=\"all\")\n",
    "\n",
    "        data[\"date_copy\"]=[fundamental_date_transform(i) for i in data[\"Date\"].copy()]\n",
    "        data[\"ticker_copy\"]=data.index\n",
    "        data=data.copy().sort_values(by=[\"ticker_copy\",\"date_copy\"])\n",
    "        data[\"next_date\"]=data[\"date_copy\"].shift(-1)\n",
    "        data[\"ticker_copy\"]=data[\"ticker_copy\"].shift(-1)\n",
    "        data[\"Date\"]=data[\"date_copy\"].copy().apply(lambda x: x.strftime(\"%d/%b/%Y\") if x!='' else np.nan)\n",
    "        data[\"Next\"]=data.apply(lambda x: x[\"next_date\"].strftime(\"%d/%b/%Y\") if type(x[\"next_date\"])==pd.Timestamp and \\\n",
    "                                x.name==x[\"ticker_copy\"] else np.nan,axis=1)\n",
    "\n",
    "#         data[\"period\"]=data.apply(lambda x:str(pd.Timestamp(datetime.strptime(x[\"Date\"],\"%d/%b/%Y\")).year)\\\n",
    "#                                             +\" \"+str(pd.Timestamp(datetime.strptime(x[\"Date\"],\"%d/%b/%Y\")).quarter),\\\n",
    "#                                             axis=1)\n",
    "\n",
    "        data[\"end_period\"]=data.apply(lambda x: pd.offsets.BQuarterEnd().rollforward(x[\"date_copy\"]).strftime(\"%d/%b/%Y\"),\\\n",
    "                                      axis=1)\n",
    "\n",
    "        data.index=pd.MultiIndex.from_tuples(list(zip(data.index,data[\"Date\"],data[\"Next\"],data[\"end_period\"])))\n",
    "\n",
    "        del data[\"ticker_copy\"]\n",
    "        del data[\"date_copy\"]\n",
    "        del data[\"next_date\"]\n",
    "        del data[\"Revision\"]\n",
    "        del data[\"end_period\"]\n",
    "        del data[\"Next\"]\n",
    "\n",
    "        for s in [\"Market cap\",\"Volume\",\"SI\",\"Broker\"]:\n",
    "            try:\n",
    "                data[s]=pd.to_numeric(data[s])\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "        '''\n",
    "        Add more realistic version of revision\n",
    "        '''\n",
    "        data[\"Revision_real\"]=data.apply(lambda x: revision_calc(x.name[0],x.name[1],EPS_df,revision_period),axis=1)\n",
    "        \n",
    "        data[\"Revision_20\"]=data.apply(lambda x: revision_calc(x.name[0],x.name[1],EPS_df,20),axis=1)\n",
    "        data=data[(data[\"Revision_20\"]>0)|(data[\"Revision_20\"]<0)]\n",
    "\n",
    "        '''\n",
    "        take out data with zero or none revision/market cap\n",
    "        '''\n",
    "        data=data[(data[\"Revision_real\"]>0)|(data[\"Revision_real\"]<0)]\n",
    "        data=data[(data[\"Market cap\"]>500)] #universe above 500mn\n",
    "        \n",
    "        '''\n",
    "        take out cases where there is a short history\n",
    "        '''\n",
    "        count_history=data.apply(lambda x: price.loc[x.name[0],:x.name[1]][-2*min_history:].count() if x.name[1] in \\\n",
    "                                   price.columns else None,axis=1)\n",
    "        \n",
    "        data=data.copy()[count_history>min_history]\n",
    "        \n",
    "        '''\n",
    "        Add momentum\n",
    "        '''\n",
    "        data[\"mom\"]=data.apply(lambda x: price.loc[x.name[0],:x.name[1]][-260:-23].dropna()[-1]/\\\n",
    "                               price.loc[x.name[0],:x.name[1]][-260:-23].dropna()[0] if x.name[1] in \\\n",
    "                               price.columns else None, axis=1)\n",
    "        \n",
    "        data=data[(data[\"mom\"]>0)|(data[\"mom\"]<0)]\n",
    "        \n",
    "        data[\"mom_short\"]=data.apply(lambda x: price.loc[x.name[0],:x.name[1]][-24:-1].dropna()[-1]/\\\n",
    "                               price.loc[x.name[0],:x.name[1]][-24:-1].dropna()[0] if x.name[1] in \\\n",
    "                               price.columns else None, axis=1)      \n",
    "\n",
    "        '''\n",
    "        Add historic volatility\n",
    "        '''\n",
    "        \n",
    "        abs_return=price.diff(1,axis=1)/price.shift(1,axis=1)\n",
    "        \n",
    "        data[\"30d_vol\"]=data.apply(lambda x: abs_return.loc[x.name[0],:x.name[1]][-31:-1].std() if x.name[1] in \\\n",
    "                                   abs_return.columns else None,axis=1)      \n",
    "        \n",
    "        data=data[data[\"30d_vol\"]>=min_vol]\n",
    "        \n",
    "        '''\n",
    "        Final cleaning and export the data\n",
    "        '''\n",
    "        target_data=data.drop_duplicates()\n",
    "        target_data.to_csv(r'C:\\Users\\Eric.Li\\Documents\\Post result code\\{0}{1}_clean_reg.csv'.format(region,\\\n",
    "                                                                                                      str(revision_period)))\n",
    "    return target_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - US data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_price,abs_return_US,US_index_price,abs_return_index_US=CSV_price('US',2006,2019,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_VWAP,abs_return_VWAP_US,US_index_VWAP,abs_return_VWAP_index_US=CSV_price('US',2006,2019,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_EPS =CSV_EPS(\"US\",2006,2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eric.Li\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: pandas.lib.tslib is deprecated and will be removed in a future version.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "US1=CSV_fundamentals(\"US\",US_price,US_EPS,1,120,0.005,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Asian data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asia index mmapping\n",
    "Asia_mapping=clean_sheet(pd.read_csv(\"Asia mapping.csv\")).dropna(how=\"all\").iloc[:,:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia_price,abs_return_Asia,Asia_index_price,abs_return_index_Asia=CSV_price('Asia',2010,2019,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia_EPS=CSV_EPS(\"Asia\",2010,2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia2_old=CSV_fundamentals(\"Asia\",abs_return_Asia,Asia_EPS,2,\"old\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia2_new=CSV_fundamentals(\"Asia\",abs_return_Asia,Asia_EPS,2,\"new\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - European data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Europe_price,abs_return_Europe,Europe_index_price,abs_return_index_Europe=CSV_price('Europe',2010,2019,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Europe_VWAP,abs_return_VWAP_Europe,Europe_index_VWAP,abs_return_index_Europe=CSV_price('Europe',2010,2019,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Europe_EPS=CSV_EPS(\"Europe\",2010,2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Europe2_old=CSV_fundamentals(\"Europe\",abs_return_Europe,Europe_EPS,2,'old')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Europe2_new=CSV_fundamentals(\"Europe\",abs_return_Europe,Europe_EPS,2,'new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_calc(ticker,date,price_df,index_df,period_tuple,abs_rel):\n",
    "    '''\n",
    "    Calculate return from the period tuple, abs_return if assigned abs\n",
    "    Day starts from zero, so 1 means start the return calculation one day after result, second element is the number of days\n",
    "    '''\n",
    "    if type(date)==pd.tslib.NaTType:\n",
    "        return None\n",
    "    elif type(date)==pd.tslib.Timestamp:\n",
    "        date=date.strftime(\"%d/%b/%Y\")\n",
    "    elif type(date)==str:\n",
    "        date=date\n",
    "    price_series=price_df.loc[ticker].dropna()\n",
    "    date_series=price_series.index.tolist()\n",
    "    \n",
    "    index_data_series=index_df.index.tolist()\n",
    "    \n",
    "    if date in date_series:\n",
    "        day0=date_series.index(date)\n",
    "        day0_index=index_data_series.index(date)\n",
    "        \n",
    "        start_price=price_series.iloc[:day0+period_tuple[0]].dropna().iloc[-1]\n",
    "        end_price=price_series.iloc[:day0+period_tuple[0]+period_tuple[1]].dropna().iloc[-1]\n",
    "        \n",
    "        target_series=price_series.iloc[day0+period_tuple[0]-2:day0+period_tuple[0]+period_tuple[1]]\n",
    "        \n",
    "        if start_price!=0:\n",
    "            abs_return=end_price/start_price-1\n",
    "        else:\n",
    "            abs_return=None\n",
    "        \n",
    "\n",
    "        \n",
    "        if abs_rel=='abs':\n",
    "            target_return=abs_return\n",
    "        else:\n",
    "\n",
    "            start_index=index_df.iloc[:day0_index+period_tuple[0]].dropna().iloc[-1]\n",
    "            end_index=index_df.iloc[:day0_index+period_tuple[0]+period_tuple[1]].dropna().iloc[-1]\n",
    "            if start_index!=0:\n",
    "                index_return=end_index/start_index-1\n",
    "            else:\n",
    "                index_return=None\n",
    "\n",
    "            if abs_return is None or index_return is None:\n",
    "                target_return=None\n",
    "            else:\n",
    "                target_return=abs_return-index_return\n",
    "        return target_return\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quarter_transform(quarter):\n",
    "    '''\n",
    "    Take the raw quarter to Q1 to Q4\n",
    "    '''\n",
    "    if type(quarter)==float:\n",
    "        adj_quarter=None\n",
    "    else:\n",
    "        \n",
    "        if quarter[-2:]=='Q4' or quarter[-2:]==':A':\n",
    "            adj_quarter='Q4'\n",
    "        elif quarter[-2:]=='Q3' or quarter[-2:]=='C3':\n",
    "            adj_quarter='Q3'\n",
    "        elif quarter[-2:]=='Q2' or quarter[-2:]=='C2' or quarter[-2:]=='S1':\n",
    "            adj_quarter='Q2'\n",
    "        elif quarter[-2:]=='Q1' or quarter[-2:]=='C1':\n",
    "            adj_quarter='Q1'\n",
    "        else:\n",
    "            adj_quarter=None\n",
    "    return adj_quarter\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning_regression(fundamental_df,price_df,index_df,pre_period,EAR_period,target_period):\n",
    "    ''' \n",
    "    Enhance the existing fundamental data after CSV_fundamental function\n",
    "    Variables include pre_announcement return, EAR, target_return, Sector dummy, adjusted quarter\n",
    "    Also transform some data to make it closer to normal distribution\n",
    "    '''\n",
    "    fundamental_df[\"adj_quarter\"]=fundamental_df.apply(lambda x:quarter_transform(x[\"Quarter\"]),axis=1)\n",
    "    \n",
    "    fundamental_df[\"pre_return\"]=fundamental_df.apply(lambda x:return_calc(x.name[0],x.name[1],price_df,index_df,\\\n",
    "                                                                                   (-pre_period-2,-pre_period),'rel'),axis=1)\n",
    "    \n",
    "    fundamental_df[\"EAR\"]=fundamental_df.apply(lambda x:return_calc(x.name[0],x.name[1],price_df,index_df,\\\n",
    "                                                                                   (-1,EAR_period),'rel'),axis=1)  \n",
    "\n",
    "    fundamental_df[\"post_return\"]=fundamental_df.apply(lambda x:return_calc(x.name[0],x.name[1],price_df,index_df,\\\n",
    "                                                                                   (EAR_period-1,target_period),'rel'),axis=1)\n",
    "    \n",
    "    for i in fundamental_df[\"Supersector\"].dropna().unique():\n",
    "        fundamental_df[i]=(fundamental_df[\"Supersector\"]==i)*1.0\n",
    "        \n",
    "    for i in fundamental_df[\"adj_quarter\"].dropna().unique():\n",
    "        fundamental_df[i]=(fundamental_df[\"adj_quarter\"]==i)*1.0    \n",
    "    \n",
    "    fundamental_df[\"Revision_real\"][fundamental_df[\"Revision_real\"]>0.99]=0.99\n",
    "    fundamental_df[\"Revision_real\"][fundamental_df[\"Revision_real\"]<-0.99]=-0.99\n",
    "    \n",
    "    fundamental_df[\"mom\"]=np.log(fundamental_df[\"mom\"])\n",
    "    \n",
    "    return fundamental_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Linear regression - US"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Data enhancement and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eric.Li\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.\n",
      "You can access NaTType as type(pandas.NaT)\n",
      "  \n",
      "C:\\Users\\Eric.Li\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Eric.Li\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "target_US=data_cleaning_regression(US1.copy(),US_price,US_index_price.loc['SPX Index'],10,3,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_US=target_US.dropna(how='any',axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0034702228142142622"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_US[\"pre_return\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 963,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 966,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00,  8.55917520e+00,  4.63320463e-02, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 1.00000000e+00,  1.32338746e+01, -3.54609929e-03, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 1.00000000e+00,  1.90122287e+01, -4.59363958e-02, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       ...,\n",
       "       [ 1.00000000e+00,  8.01902532e+00,  2.23880597e-02, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 1.00000000e+00,  6.82433118e+00, -8.30357143e-02, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 1.00000000e+00,  5.36243337e+00,  8.65800866e-03, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00]])"
      ]
     },
     "execution_count": 966,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polynomial_features= PolynomialFeatures(degree=2)\n",
    "x_poly=polynomial_features.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 988,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=target_US[['Volume','Revision_real','mom','30d_vol','pre_return','EAR','Broker','SI']+\\\n",
    "            target_US[\"Supersector\"].dropna().unique().tolist()+\\\n",
    "          target_US[\"adj_quarter\"].dropna().unique().tolist() ]\n",
    "y=target_US['post_return']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 991,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_US.corr().to_csv(\"corr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 983,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_features= PolynomialFeatures(degree=2)\n",
    "x=polynomial_features.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 984,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 985,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 986,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 986,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 987,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08751865590445351\n",
      "0.0161660997173646\n",
      "0.08359219959850947\n",
      "0.034694315132211884\n"
     ]
    }
   ],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_test,regr.predict(x_test)))\n",
    "r2 = r2_score(y_test,regr.predict(x_test))\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train,regr.predict(x_train)))\n",
    "r2_train = r2_score(y_train,regr.predict(x_train))\n",
    "print(rmse)\n",
    "print(r2)\n",
    "print(rmse_train)\n",
    "print(r2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 982,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0880094399643055\n",
      "0.005100942341206616\n",
      "0.08485628260805506\n",
      "0.0052788274014335945\n"
     ]
    }
   ],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_test,regr.predict(x_test)))\n",
    "r2 = r2_score(y_test,regr.predict(x_test))\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train,regr.predict(x_train)))\n",
    "r2_train = r2_score(y_train,regr.predict(x_train))\n",
    "print(rmse)\n",
    "print(r2)\n",
    "print(rmse_train)\n",
    "print(r2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result=y_train.to_frame().copy()\n",
    "train_result[\"predict\"]=regr.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result=y_test.to_frame().copy()\n",
    "test_result[\"predict\"]=regr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13943, 2)"
      ]
     },
     "execution_count": 620,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_return</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13943.000000</td>\n",
       "      <td>13943.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.004841</td>\n",
       "      <td>0.003545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.088238</td>\n",
       "      <td>0.006135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.656633</td>\n",
       "      <td>-0.031200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.037972</td>\n",
       "      <td>0.000615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.002613</td>\n",
       "      <td>0.003639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.043091</td>\n",
       "      <td>0.007565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.461976</td>\n",
       "      <td>0.028717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        post_return       predict\n",
       "count  13943.000000  13943.000000\n",
       "mean       0.004841      0.003545\n",
       "std        0.088238      0.006135\n",
       "min       -0.656633     -0.031200\n",
       "25%       -0.037972      0.000615\n",
       "50%        0.002613      0.003639\n",
       "75%        0.043091      0.007565\n",
       "max        1.461976      0.028717"
      ]
     },
     "execution_count": 626,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5318, 2)"
      ]
     },
     "execution_count": 629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result[test_result[\"predict\"]>0.005].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "post_return   -0.003067\n",
       "predict       -0.004995\n",
       "dtype: float64"
      ]
     },
     "execution_count": 631,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result[test_result[\"predict\"]<-0.00].sort_values(\"post_return\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=sm.OLS(y_test,x_test).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>post_return</td>   <th>  R-squared:         </th>  <td>   0.007</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.005</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   3.360</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 19 Jun 2019</td> <th>  Prob (F-statistic):</th>  <td>2.72e-09</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>14:56:11</td>     <th>  Log-Likelihood:    </th>  <td>  14115.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 13943</td>      <th>  AIC:               </th> <td>-2.817e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 13913</td>      <th>  BIC:               </th> <td>-2.794e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    29</td>      <th>                     </th>      <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "               <td></td>                  <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Volume</th>                      <td>-3.906e-06</td> <td> 3.44e-06</td> <td>   -1.136</td> <td> 0.256</td> <td>-1.06e-05</td> <td> 2.83e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Revision_real</th>               <td>    0.0032</td> <td>    0.006</td> <td>    0.555</td> <td> 0.579</td> <td>   -0.008</td> <td>    0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mom</th>                         <td>   -0.0042</td> <td>    0.002</td> <td>   -1.982</td> <td> 0.047</td> <td>   -0.008</td> <td>-4.69e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>30d_vol</th>                     <td>   -0.2099</td> <td>    0.061</td> <td>   -3.467</td> <td> 0.001</td> <td>   -0.329</td> <td>   -0.091</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pre_return</th>                  <td>   -0.0243</td> <td>    0.011</td> <td>   -2.135</td> <td> 0.033</td> <td>   -0.047</td> <td>   -0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>EAR</th>                         <td>    0.0115</td> <td>    0.009</td> <td>    1.230</td> <td> 0.219</td> <td>   -0.007</td> <td>    0.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Broker</th>                      <td>-8.414e-05</td> <td>    0.000</td> <td>   -0.692</td> <td> 0.489</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SI</th>                          <td>    0.0002</td> <td>    0.000</td> <td>    1.439</td> <td> 0.150</td> <td>-7.53e-05</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Utilities</th>                   <td>   -0.0021</td> <td>    0.004</td> <td>   -0.492</td> <td> 0.623</td> <td>   -0.010</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Oil & Gas</th>                   <td>   -0.0062</td> <td>    0.003</td> <td>   -2.081</td> <td> 0.037</td> <td>   -0.012</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Health Care</th>                 <td>    0.0149</td> <td>    0.002</td> <td>    6.263</td> <td> 0.000</td> <td>    0.010</td> <td>    0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Personal & Household Goods</th>  <td>   -0.0028</td> <td>    0.003</td> <td>   -0.881</td> <td> 0.378</td> <td>   -0.009</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Retail</th>                      <td>    0.0034</td> <td>    0.003</td> <td>    1.196</td> <td> 0.232</td> <td>   -0.002</td> <td>    0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Technology</th>                  <td>    0.0090</td> <td>    0.002</td> <td>    4.059</td> <td> 0.000</td> <td>    0.005</td> <td>    0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Telecommunications</th>          <td>    0.0094</td> <td>    0.007</td> <td>    1.382</td> <td> 0.167</td> <td>   -0.004</td> <td>    0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Financial Services</th>          <td>    0.0024</td> <td>    0.004</td> <td>    0.688</td> <td> 0.491</td> <td>   -0.004</td> <td>    0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Travel & Leisure</th>            <td>   -0.0006</td> <td>    0.003</td> <td>   -0.185</td> <td> 0.853</td> <td>   -0.007</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Industrial Goods & Services</th> <td>    0.0019</td> <td>    0.002</td> <td>    0.980</td> <td> 0.327</td> <td>   -0.002</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Banks</th>                       <td>    0.0021</td> <td>    0.003</td> <td>    0.626</td> <td> 0.531</td> <td>   -0.004</td> <td>    0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Media</th>                       <td>   -0.0027</td> <td>    0.004</td> <td>   -0.621</td> <td> 0.535</td> <td>   -0.011</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Automobiles & Parts</th>         <td>   -0.0061</td> <td>    0.007</td> <td>   -0.918</td> <td> 0.359</td> <td>   -0.019</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Insurance</th>                   <td>   -0.0014</td> <td>    0.004</td> <td>   -0.378</td> <td> 0.705</td> <td>   -0.009</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Basic Resources</th>             <td>   -0.0054</td> <td>    0.005</td> <td>   -1.056</td> <td> 0.291</td> <td>   -0.015</td> <td>    0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Real Estate</th>                 <td>   -0.0017</td> <td>    0.012</td> <td>   -0.146</td> <td> 0.884</td> <td>   -0.024</td> <td>    0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Food & Beverage</th>             <td>    0.0065</td> <td>    0.004</td> <td>    1.537</td> <td> 0.124</td> <td>   -0.002</td> <td>    0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Construction & Materials</th>    <td>    0.0008</td> <td>    0.005</td> <td>    0.175</td> <td> 0.861</td> <td>   -0.008</td> <td>    0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Chemicals</th>                   <td>    0.0047</td> <td>    0.005</td> <td>    0.999</td> <td> 0.318</td> <td>   -0.005</td> <td>    0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q4</th>                          <td>    0.0077</td> <td>    0.003</td> <td>    3.015</td> <td> 0.003</td> <td>    0.003</td> <td>    0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q3</th>                          <td>    0.0070</td> <td>    0.003</td> <td>    2.653</td> <td> 0.008</td> <td>    0.002</td> <td>    0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q1</th>                          <td>    0.0072</td> <td>    0.002</td> <td>    2.909</td> <td> 0.004</td> <td>    0.002</td> <td>    0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q2</th>                          <td>    0.0041</td> <td>    0.003</td> <td>    1.609</td> <td> 0.108</td> <td>   -0.001</td> <td>    0.009</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>6474.508</td> <th>  Durbin-Watson:     </th>  <td>   1.985</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>257707.804</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 1.545</td>  <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>23.834</td>  <th>  Cond. No.          </th>  <td>1.00e+16</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:            post_return   R-squared:                       0.007\n",
       "Model:                            OLS   Adj. R-squared:                  0.005\n",
       "Method:                 Least Squares   F-statistic:                     3.360\n",
       "Date:                Wed, 19 Jun 2019   Prob (F-statistic):           2.72e-09\n",
       "Time:                        14:56:11   Log-Likelihood:                 14115.\n",
       "No. Observations:               13943   AIC:                        -2.817e+04\n",
       "Df Residuals:                   13913   BIC:                        -2.794e+04\n",
       "Df Model:                          29                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===============================================================================================\n",
       "                                  coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------------------\n",
       "Volume                      -3.906e-06   3.44e-06     -1.136      0.256   -1.06e-05    2.83e-06\n",
       "Revision_real                   0.0032      0.006      0.555      0.579      -0.008       0.014\n",
       "mom                            -0.0042      0.002     -1.982      0.047      -0.008   -4.69e-05\n",
       "30d_vol                        -0.2099      0.061     -3.467      0.001      -0.329      -0.091\n",
       "pre_return                     -0.0243      0.011     -2.135      0.033      -0.047      -0.002\n",
       "EAR                             0.0115      0.009      1.230      0.219      -0.007       0.030\n",
       "Broker                      -8.414e-05      0.000     -0.692      0.489      -0.000       0.000\n",
       "SI                              0.0002      0.000      1.439      0.150   -7.53e-05       0.000\n",
       "Utilities                      -0.0021      0.004     -0.492      0.623      -0.010       0.006\n",
       "Oil & Gas                      -0.0062      0.003     -2.081      0.037      -0.012      -0.000\n",
       "Health Care                     0.0149      0.002      6.263      0.000       0.010       0.020\n",
       "Personal & Household Goods     -0.0028      0.003     -0.881      0.378      -0.009       0.003\n",
       "Retail                          0.0034      0.003      1.196      0.232      -0.002       0.009\n",
       "Technology                      0.0090      0.002      4.059      0.000       0.005       0.013\n",
       "Telecommunications              0.0094      0.007      1.382      0.167      -0.004       0.023\n",
       "Financial Services              0.0024      0.004      0.688      0.491      -0.004       0.009\n",
       "Travel & Leisure               -0.0006      0.003     -0.185      0.853      -0.007       0.006\n",
       "Industrial Goods & Services     0.0019      0.002      0.980      0.327      -0.002       0.006\n",
       "Banks                           0.0021      0.003      0.626      0.531      -0.004       0.009\n",
       "Media                          -0.0027      0.004     -0.621      0.535      -0.011       0.006\n",
       "Automobiles & Parts            -0.0061      0.007     -0.918      0.359      -0.019       0.007\n",
       "Insurance                      -0.0014      0.004     -0.378      0.705      -0.009       0.006\n",
       "Basic Resources                -0.0054      0.005     -1.056      0.291      -0.015       0.005\n",
       "Real Estate                    -0.0017      0.012     -0.146      0.884      -0.024       0.021\n",
       "Food & Beverage                 0.0065      0.004      1.537      0.124      -0.002       0.015\n",
       "Construction & Materials        0.0008      0.005      0.175      0.861      -0.008       0.010\n",
       "Chemicals                       0.0047      0.005      0.999      0.318      -0.005       0.014\n",
       "Q4                              0.0077      0.003      3.015      0.003       0.003       0.013\n",
       "Q3                              0.0070      0.003      2.653      0.008       0.002       0.012\n",
       "Q1                              0.0072      0.002      2.909      0.004       0.002       0.012\n",
       "Q2                              0.0041      0.003      1.609      0.108      -0.001       0.009\n",
       "==============================================================================\n",
       "Omnibus:                     6474.508   Durbin-Watson:                   1.985\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           257707.804\n",
       "Skew:                           1.545   Prob(JB):                         0.00\n",
       "Kurtosis:                      23.834   Cond. No.                     1.00e+16\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 8.43e-24. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Build the signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_signal(x,y,long_cutoff,short_cutoff):\n",
    "    '''\n",
    "    Build the target signal dataframe from regression result\n",
    "    '''\n",
    "    long_base=pd.DataFrame()\n",
    "    short_base=pd.DataFrame()\n",
    "    \n",
    "    year_series=x.apply(lambda x:int(x.name[1][-4:]),axis=1)\n",
    "    regression_dict={}\n",
    "    \n",
    "    for i in year_series.unique():\n",
    "        \n",
    "        x_year=x.loc[year_series==i,:]\n",
    "        y_year=y.loc[year_series==i,:].to_frame()\n",
    "        \n",
    "        regr=LinearRegression()\n",
    "        regr.fit(x_year,y_year)\n",
    "        y_year[\"predict\"]=regr.predict(x_year)\n",
    "        \n",
    "        upper_predict=y_year[\"predict\"].quantile(long_cutoff)\n",
    "        lower_predict=y_year[\"predict\"].quantile(short_cutoff)\n",
    "        \n",
    "        regression_dict[i]=[regr,upper_predict,lower_predict]\n",
    "        \n",
    "        if i==year_series.unique().min():\n",
    "            pass\n",
    "        else:\n",
    "            long_base_year=y_year[y_year[\"predict\"]>regression_dict[i-1][1]]\n",
    "            short_base_year=y_year[y_year[\"predict\"]<regression_dict[i-1][2]]\n",
    "            \n",
    "            long_base=pd.concat([long_base,long_base_year],axis=0)\n",
    "            short_base=pd.concat([short_base,short_base_year],axis=0)\n",
    "    \n",
    "    return long_base,short_base\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "metadata": {},
   "outputs": [],
   "source": [
    "class regression_signal(object):\n",
    "    def __init__(self,long_cutoff,short_cutoff,fundamental_df,price_df,index_price_df,pre_period,EAR_period,target_period,\n",
    "                 start,end,old_position,revision_adjust,new_signal):\n",
    "        \n",
    "        self.long_cutoff=long_cutoff\n",
    "        self.short_cutoff=short_cutoff  \n",
    "        self.fundamental_df=fundamental_df\n",
    "        self.price_df=price_df\n",
    "        self.abs_return=price_df.diff(1,axis=1)/price_df.shift(1,axis=1)\n",
    "        self.index_price_df=index_price_df\n",
    "        self.pre_period=pre_period\n",
    "        self.EAR_period=EAR_period\n",
    "        self.entry=EAR_period+1\n",
    "        self.holding=target_period\n",
    "        self.target_period=target_period\n",
    "\n",
    "        self.start=start\n",
    "        self.end=end\n",
    "        self.old_position=old_position\n",
    "        self.revision_adjust=revision_adjust\n",
    "        self.new_signal=new_signal\n",
    "\n",
    "    def data_cleaning_regression(self):\n",
    "        ''' \n",
    "        Enhance the existing fundamental data after CSV_fundamental function\n",
    "        Variables include pre_announcement return, EAR, target_return, Sector dummy, adjusted quarter\n",
    "        Also transform some data to make it closer to normal distribution\n",
    "        '''\n",
    "        fundamental_df=self.fundamental_df.copy()\n",
    "        price_df=self.price_df.copy()\n",
    "        index_price_df=self.index_price_df.copy()\n",
    "        \n",
    "        \n",
    "        fundamental_df[\"adj_quarter\"]=fundamental_df.apply(lambda x:quarter_transform(x[\"Quarter\"]),axis=1)\n",
    "\n",
    "        fundamental_df[\"pre_return\"]=fundamental_df.apply(lambda x:return_calc(x.name[0],x.name[1],price_df,index_price_df,\\\n",
    "                                                                                       (-self.pre_period-1,-self.pre_period)\\\n",
    "                                                                               ,'rel'),axis=1)\n",
    "\n",
    "        fundamental_df[\"EAR\"]=fundamental_df.apply(lambda x:return_calc(x.name[0],x.name[1],price_df,index_price_df,\\\n",
    "                                                                                       (0,self.EAR_period),'rel'),axis=1)  \n",
    "\n",
    "        fundamental_df[\"post_return\"]=fundamental_df.apply(lambda x:return_calc(x.name[0],x.name[1],price_df,index_price_df,\\\n",
    "                                                                                       (self.EAR_period,self.target_period),\\\n",
    "                                                                                'rel'),axis=1)\n",
    "\n",
    "        for i in fundamental_df[\"Supersector\"].dropna().unique():\n",
    "            fundamental_df[i]=(fundamental_df[\"Supersector\"]==i)*1.0\n",
    "\n",
    "        for i in fundamental_df[\"adj_quarter\"].dropna().unique():\n",
    "            fundamental_df[i]=(fundamental_df[\"adj_quarter\"]==i)*1.0    \n",
    "\n",
    "        fundamental_df[\"Revision_real\"][fundamental_df[\"Revision_real\"]>0.99]=0.99\n",
    "        fundamental_df[\"Revision_real\"][fundamental_df[\"Revision_real\"]<-0.99]=-0.99\n",
    "\n",
    "        fundamental_df[\"mom\"]=np.log(fundamental_df[\"mom\"])\n",
    "        \n",
    "        x=fundamental_df[['Volume','Revision_real','mom','30d_vol','pre_return','EAR','Broker','SI']+\\\n",
    "                         fundamental_df[\"Supersector\"].dropna().unique().tolist()+\\\n",
    "                         fundamental_df[\"adj_quarter\"].dropna().unique().tolist() ]\n",
    "        \n",
    "        y=target_US['post_return']\n",
    "        \n",
    "        self.x=x\n",
    "        self.y=y\n",
    "        \n",
    "        return x,y\n",
    "\n",
    "    def regression_signal(self):\n",
    "        '''\n",
    "        Build the target signal dataframe from regression result\n",
    "        '''\n",
    "        \n",
    "        try:\n",
    "            x=self.x.copy()\n",
    "            y=self.y.copy()\n",
    "        except:\n",
    "            x,y=self.data_cleaning_regression()\n",
    "        \n",
    "        long_base=pd.DataFrame()\n",
    "        short_base=pd.DataFrame()\n",
    "\n",
    "        year_series=x.apply(lambda x:int(x.name[1][-4:]),axis=1)\n",
    "        regression_dict={}\n",
    "\n",
    "        for i in year_series.unique():\n",
    "\n",
    "            x_year=x.loc[year_series==i,:]\n",
    "            y_year=y.loc[year_series==i,:].to_frame()\n",
    "\n",
    "            regr=LinearRegression()\n",
    "            regr.fit(x_year,y_year)\n",
    "            upper_predict=min(pd.DataFrame(regr.predict(x_year)).quantile(self.long_cutoff).iloc[0],0.02)\n",
    "            lower_predict=max(pd.DataFrame(regr.predict(x_year)).quantile(self.short_cutoff).iloc[0],-0.02)\n",
    "            regression_dict[i]=[regr,upper_predict,lower_predict]\n",
    "            \n",
    "            if i==year_series.unique().min():\n",
    "                pass\n",
    "            else:\n",
    "                y_year[\"predict\"]=regression_dict[i-1][0].predict(x_year)\n",
    "                    \n",
    "\n",
    "                long_base_year=y_year[y_year[\"predict\"]>regression_dict[i-1][1]]\n",
    "                short_base_year=y_year[y_year[\"predict\"]<regression_dict[i-1][2]]\n",
    "\n",
    "                long_base=pd.concat([long_base,long_base_year],axis=0)\n",
    "                short_base=pd.concat([short_base,short_base_year],axis=0)\n",
    "        \n",
    "        self.long_base=long_base\n",
    "        self.short_base=short_base\n",
    "        \n",
    "        return long_base,short_base,regression_dict    \n",
    "\n",
    "\n",
    "    def signal_df_date(self):\n",
    "        '''\n",
    "        Obtain the signal_df function over the whole time period from the target signal list\n",
    "        '''\n",
    "        try:\n",
    "            long_base=self.long_base.copy()\n",
    "            short_base=self.short_base.copy()\n",
    "        except:\n",
    "            long_base,short_base=signal.regression_signal(self)\n",
    "        \n",
    "        if long_base is None:\n",
    "            long_df=None\n",
    "        \n",
    "        else:\n",
    "            long_df=pd.DataFrame(index=self.price_df.columns)\n",
    "\n",
    "            for s in long_base.index:\n",
    "                return_series=self.abs_return.loc[s[0]]\n",
    "                if s[1] in return_series.index:\n",
    "                    if not np.isnan(return_series.loc[s[1]]): \n",
    "                        day0=return_series.index.tolist().index(s[1])\n",
    "                            \n",
    "                        if type(s[2])==float:##basically np.nan has type float\n",
    "                            period=self.holding+1\n",
    "                        \n",
    "                        else: ##assume that we are not holding through numbers\n",
    "                            period=min(self.holding,np.busday_count(datetime.strptime(s[1],\"%d/%b/%Y\").date(),\\\n",
    "                                                                    datetime.strptime(s[2],\"%d/%b/%Y\").date())-self.entry)\n",
    "                            \n",
    "                        if period is None:\n",
    "                            pass\n",
    "                        else:\n",
    "                            target_series=return_series.iloc[day0+self.entry-1:day0+min(period+self.entry, \\\n",
    "                                                                                           len(return_series[day0:]))].dropna()\n",
    "                            \n",
    "                            if len(target_series)==0:\n",
    "                                pass\n",
    "                            else:\n",
    "                                target_series.iloc[0]=0.0\n",
    "                                long_df[s]=target_series                        \n",
    "\n",
    "                    else:\n",
    "                        pass\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "            long_df=long_df.reindex(datetime.strptime(i,\"%d/%b/%Y\") for i in long_df.index)\n",
    "            long_df=long_df.sort_index()\n",
    "\n",
    "            if self.start is not None:\n",
    "                long_df=slice_universe(long_df,self.start,self.end,self.old_position)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            long_df=long_df.dropna(how=\"all\",axis=1)\n",
    "            #long_df.columns=pd.MultiIndex.from_tuples(pd.Series(list(long_df.columns)))\n",
    "            self.long_df=long_df\n",
    "            \n",
    "            \n",
    "        \n",
    "        if short_base is None:\n",
    "            short_df=None\n",
    "        \n",
    "        else:\n",
    "            short_df=pd.DataFrame(index=self.price_df.columns)\n",
    "\n",
    "            for s in short_base.index:\n",
    "                return_series=self.abs_return.loc[s[0]]\n",
    "                if s[1] in return_series.index:\n",
    "                    if not np.isnan(return_series.loc[s[1]]): \n",
    "                        day0=return_series.index.tolist().index(s[1])\n",
    "                        \n",
    "                        if type(s[2])==float:##basically np.nan has type float\n",
    "                            period=self.holding\n",
    "                    \n",
    "                        else: ##assume that we are not holding through numbers\n",
    "                            period=min(self.holding,np.busday_count(datetime.strptime(s[1],\"%d/%b/%Y\").date(),\\\n",
    "                                                                    datetime.strptime(s[2],\"%d/%b/%Y\").date())-self.entry)\\\n",
    "\n",
    "\n",
    "                        if period is None:\n",
    "                            pass\n",
    "                        else:\n",
    "                            target_series=return_series.iloc[day0+self.entry-1:day0+min(period+self.entry, \\\n",
    "                                                                                           len(return_series[day0:]))].dropna()\n",
    "                            \n",
    "                            if len(target_series)==0:\n",
    "                                pass\n",
    "                            else:\n",
    "                                target_series.iloc[0]=0.0\n",
    "                                short_df[s]=target_series         \n",
    "                            \n",
    "                    else:\n",
    "                        pass\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "            short_df=short_df.reindex(datetime.strptime(i,\"%d/%b/%Y\") for i in short_df.index)\n",
    "            short_df=short_df.sort_index()\n",
    "\n",
    "            if self.start is not None:\n",
    "                short_df=slice_universe(short_df,self.start,self.end,self.old_position)\n",
    "            else:\n",
    "                pass  \n",
    "            \n",
    "            short_df=short_df.dropna(how=\"all\",axis=1)\n",
    "            #short_df.columns=pd.MultiIndex.from_tuples(pd.Series(list(short_df.columns)))\n",
    "            self.short_df=short_df\n",
    "        \n",
    "        return long_df,short_df\n",
    "\n",
    "    def signal_account(self,stop,gross,index_df,net_level,risk_parity,liquidity,capital):\n",
    "        '''\n",
    "        Build the account curve with signal_df\n",
    "        Assume quarterly rebalancing that's why the period list has quarter as the key\n",
    "        Take extra care when building the account curve, the logic is: work out the size_df, then shift by 1 and * signal_df\n",
    "        Stop=(long_stop,short_stop,type)\n",
    "        index_df has to be a dataframe with a name\n",
    "        '''\n",
    "        \n",
    "        try:\n",
    "            long_df=self.long_df.copy()\n",
    "            short_df=self.short_df.copy()\n",
    "            \n",
    "        except:\n",
    "            long_df,short_df=self.signal_df_date()\n",
    "                   \n",
    "        '''\n",
    "        Assign values for later use\n",
    "        '''\n",
    "        \n",
    "        self.capital=capital\n",
    "        \n",
    "        self.index_df=index_df\n",
    "        self.index_df.index=[datetime.strptime(i,\"%d/%b/%Y\") for i in self.index_df.index]\n",
    "       \n",
    "        \n",
    "        '''\n",
    "        Define rebalance period first\n",
    "        '''\n",
    "        if long_df is None:\n",
    "            period_year=short_df.apply(lambda x:str(x.name.year),axis=1)\n",
    "            period_list=list(set(period_year))\n",
    "            period_list.sort()    \n",
    "            \n",
    "        else:\n",
    "            period_year=long_df.apply(lambda x:str(x.name.year),axis=1)\n",
    "            period_list=list(set(period_year))\n",
    "            period_list.sort()    \n",
    "                \n",
    "        '''\n",
    "        Separate out long and short\n",
    "        '''\n",
    "        if long_df is None:\n",
    "            long_cache=(None,None,None,None)\n",
    "        else:\n",
    "            if stop is None:\n",
    "                pass\n",
    "            else:\n",
    "                long_df=signal_filter_stop(long_df,stop[0],self.abs_return,30,stop[2],self.index_df)   \n",
    "                self.long_df=long_df\n",
    "\n",
    "            long_sub_signal={}\n",
    "            long_sub_size_row={}\n",
    "            long_sub_size_df={}\n",
    "            long_sub_pnl={}\n",
    "        \n",
    "            for s in period_list:\n",
    "            \n",
    "                long_sub_signal[s]=long_df[period_year==s].dropna(how='all',axis=1)\n",
    "                \n",
    "                if long_sub_signal[s].shape[1]==0:\n",
    "                    long_sub_size_df[s]=long_sub_signal[s]\n",
    "                    long_sub_pnl[s]=long_sub_signal[s]\n",
    "                    \n",
    "                else:\n",
    "                    if period_list.index(s)<4:##use last quarter's sizing as reference\n",
    "                        long_sub_size_row[s]=sizing(long_sub_signal[s],long_sub_signal[s],gross[0],self.fundamental_df,\\\n",
    "                                                       self.new_signal,self.abs_return,risk_parity,liquidity,capital,\\\n",
    "                                                    self.revision_adjust,True)[0]\n",
    "                    else:\n",
    "                        try:\n",
    "                            long_sub_size_row[s]=sizing(long_sub_signal[s],long_sub_signal[period_list[period_list.index(s)-1]],\\\n",
    "                                                       gross[0],self.fundamental_df,self.new_signal,self.abs_return,\\\n",
    "                                                        risk_parity,liquidity,capital,self.revision_adjust,True)[0]\n",
    "                        except:\n",
    "                            long_sub_size_row[s]=sizing(long_sub_signal[s],long_sub_signal[s],\\\n",
    "                                                       gross[0],self.fundamental_df,self.new_signal,self.abs_return,\\\n",
    "                                                        risk_parity,liquidity,capital,self.revision_adjust,True)[0] \n",
    "\n",
    "                    long_sub_size_df[s]=(1+long_sub_signal[s]).cumprod()*long_sub_size_row[s]\n",
    "                    long_sub_pnl[s]=(long_sub_size_df[s].shift(1))*long_sub_signal[s] \n",
    "                    # need to shift by 1 as the size is end of the day\n",
    "        \n",
    "            long_daily_pnl=pd.concat(list(long_sub_pnl.values()),axis=0)\n",
    "            long_acct_curve=long_daily_pnl.cumsum().ffill().sum(axis=1)\n",
    "            long_size_df=pd.concat(list(long_sub_size_df.values()),axis=0)\n",
    "            long_ind_return=long_daily_pnl.cumsum().ffill().iloc[-1].dropna()\n",
    "            long_cache=(long_daily_pnl,long_acct_curve,long_size_df,long_ind_return)\n",
    "            \n",
    "            self.long_cache=long_cache\n",
    "\n",
    "            \n",
    "        if short_df is None:\n",
    "            short_cache=(None,None,None,None)\n",
    "        else:\n",
    "            if stop is None:\n",
    "                pass\n",
    "            else:\n",
    "                short_df=-signal_filter_stop(-short_df,stop[1],self.abs_return,30,stop[2],self.index_df)   \n",
    "                self.short_df=short_df\n",
    "\n",
    "            short_sub_signal={}\n",
    "            short_sub_size_row={}\n",
    "            short_sub_size_df={}\n",
    "            short_sub_pnl={}\n",
    "        \n",
    "            for s in period_list:\n",
    "                short_sub_signal[s]=short_df[period_year==s].dropna(how='all',axis=1)\n",
    "\n",
    "                if short_sub_signal[s].shape[1]==0:\n",
    "                    short_sub_size_df[s]=short_sub_signal[s]\n",
    "                    short_sub_pnl[s]=short_sub_signal[s]\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    if period_list.index(s)<4:##use last quarter's sizing as reference\n",
    "                        short_sub_size_row[s]=-sizing(short_sub_signal[s],short_sub_signal[s],gross[1],self.fundamental_df,\\\n",
    "                                                       self.new_signal,self.abs_return,risk_parity,liquidity,capital,\\\n",
    "                                                      self.revision_adjust,False)[0]\n",
    "                    else:\n",
    "                        try:\n",
    "                            short_sub_size_row[s]=-sizing(short_sub_signal[s],short_sub_signal\\\n",
    "                                                          [period_list[period_list.index(s)-1]],\\\n",
    "                                                           gross[1],self.fundamental_df,self.new_signal,self.abs_return,\\\n",
    "                                                          risk_parity,liquidity,\\\n",
    "                                                          capital,self.revision_adjust,False)[0]\n",
    "                        except:\n",
    "                            short_sub_size_row[s]=-sizing(short_sub_signal[s],short_sub_signal[s],\\\n",
    "                                                           gross[1],self.fundamental_df,self.new_signal,self.abs_return,\\\n",
    "                                                          risk_parity,liquidity,\\\n",
    "                                                          capital,self.revision_adjust,False)[0]\n",
    "\n",
    "                    short_sub_size_df[s]=(1+short_sub_signal[s]).cumprod()*short_sub_size_row[s]\n",
    "                    short_sub_pnl[s]=(short_sub_size_df[s].shift(1))*short_sub_signal[s] \n",
    "                # need to shift by 1 as the size is end of the day\n",
    "        \n",
    "            short_daily_pnl=pd.concat(list(short_sub_pnl.values()),axis=0)\n",
    "            short_acct_curve=short_daily_pnl.cumsum().ffill().sum(axis=1)\n",
    "            short_size_df=pd.concat(list(short_sub_size_df.values()),axis=0)\n",
    "            short_ind_return=short_daily_pnl.cumsum().ffill().iloc[-1].dropna()\n",
    "            \n",
    "            short_cache=(short_daily_pnl,short_acct_curve,short_size_df,short_ind_return)\n",
    "            self.short_cache=short_cache\n",
    "    \n",
    "        '''Put alpha positions together to form the alpha part'''\n",
    "        alpha_df=pd.concat([long_df,short_df],axis=1)\n",
    "        self.alpha_df=alpha_df\n",
    "        \n",
    "        alpha_daily_pnl=pd.concat([long_cache[0],short_cache[0]],axis=1)\n",
    "        alpha_acct_curve=alpha_daily_pnl.cumsum().ffill().sum(axis=1)\n",
    "        alpha_size_df=pd.concat([long_cache[2],short_cache[2]],axis=1)\n",
    "        alpha_ind_return=pd.concat([long_cache[3],short_cache[3]],axis=0)\n",
    "        \n",
    "        alpha_cache=(alpha_daily_pnl,alpha_acct_curve,alpha_size_df,alpha_ind_return)\n",
    "        \n",
    "        \n",
    "        if self.index_df is not None:\n",
    "            if self.index_df.shape[1]==1:\n",
    "                index_df=self.index_df.copy().loc[alpha_df.index] \n",
    "                index_size_df=(net_level-alpha_size_df.sum(axis=1)).to_frame(index_df.columns[0])\n",
    "                index_daily_pnl=index_size_df.shift(1)*index_df\n",
    "                index_acct_curve=index_daily_pnl.cumsum()\n",
    "                index_ind_return=index_acct_curve.iloc[-1]\n",
    "                index_cache=(index_daily_pnl,index_acct_curve,index_size_df,index_ind_return)\n",
    "            else:\n",
    "                index_df=self.index_df.copy().loc[alpha_df.index] \n",
    "                alpha_temp=alpha_cache[2].copy().T\n",
    "                alpha_temp[\"index\"]=alpha_temp.apply(lambda x:Asia_mapping.loc[x.name[0][-2:]].iloc[0],axis=1)\n",
    "                index_size_df=net_level-alpha_temp.groupby(\"index\").apply(sum).T.iloc[:-1]\n",
    "                index_daily_pnl=index_size_df.shift(1)*index_df\n",
    "                index_acct_curve=index_daily_pnl.cumsum()\n",
    "                index_ind_return=index_acct_curve.iloc[-1]\n",
    "                index_cache=(index_daily_pnl,index_acct_curve,index_size_df,index_ind_return)\n",
    "        else:\n",
    "            index_cache=(None,None,None,None)\n",
    "            \n",
    "        '''Finally put everything together'''    \n",
    "        portfolio_df=pd.concat([alpha_df,index_df],axis=1)\n",
    "            \n",
    "        portfolio_size_df=pd.concat([alpha_cache[2],index_cache[2]],axis=1).sort_index()\n",
    "  \n",
    "        portfolio_daily_pnl=pd.concat([alpha_cache[0],index_cache[0]],axis=1).sort_index()\n",
    "\n",
    "        portfolio_acct_curve=portfolio_daily_pnl.cumsum().ffill().sum(axis=1)\n",
    "        portfolio_ind_return=alpha_cache[3].copy()\n",
    "        \n",
    "        portfolio_gross=np.abs(portfolio_size_df).sum(axis=1).sort_index()\n",
    "        portfolio_turnover=(np.abs(alpha_size_df.fillna(0.0).diff(1)).sum().sum())/(portfolio_size_df.shape[0]/260)\n",
    "        \n",
    "        portfolio_cache=(portfolio_daily_pnl,portfolio_acct_curve,portfolio_size_df,portfolio_ind_return,portfolio_gross,\\\n",
    "                         portfolio_turnover,portfolio_df)\n",
    "        \n",
    "        self.portfolio_account=portfolio_cache #save for later use\n",
    "        \n",
    "        \n",
    "        return long_cache,short_cache,alpha_cache,portfolio_cache\n",
    "    \n",
    "    def plot_account(self,title,figsize=[10,4],portfolio=None):\n",
    "        '''\n",
    "        Plot the account curve\n",
    "        '''\n",
    "        if portfolio is None:\n",
    "            try:\n",
    "                portfolio_cache=self.portfolio_account\n",
    "\n",
    "            except AttributeError:\n",
    "                print(\"Execute the signal_account first!\")  \n",
    "                return None\n",
    "        else:\n",
    "            portfolio_cache=portfolio\n",
    "        \n",
    "        plot_signal(title,figsize,portfolio_cache)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 949,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_class=regression_signal(long_cutoff=0.8,short_cutoff=0.2,fundamental_df=US1,price_df=US_price,\n",
    "                           index_price_df=US_index_price.loc['SPX Index'],pre_period=10,EAR_period=2,target_period=20,\n",
    "                           start=pd.Timestamp(2013,1,1),end=pd.Timestamp(2018,12,31),old_position=False,\n",
    "                          revision_adjust=('constant',0.004),new_signal=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 950,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_class.x=x\n",
    "US_class.y=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb,sb,regrdict=US_class.regression_signal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 952,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2007: [LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False),\n",
       "  0.006320534649009148,\n",
       "  -0.018378014233975292],\n",
       " 2008: [LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False),\n",
       "  0.02,\n",
       "  -0.015112736569507618],\n",
       " 2009: [LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False),\n",
       "  0.016848814574578243,\n",
       "  -0.02],\n",
       " 2010: [LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False),\n",
       "  0.02,\n",
       "  -0.004028463696123864],\n",
       " 2011: [LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False),\n",
       "  0.012358948876513204,\n",
       "  -0.01628254855208276],\n",
       " 2012: [LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False),\n",
       "  0.012127358347265636,\n",
       "  -0.01096568443714112],\n",
       " 2013: [LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False),\n",
       "  0.018452959206609228,\n",
       "  0.00031925864120359906],\n",
       " 2014: [LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False),\n",
       "  0.009992307518947259,\n",
       "  -0.011319829635962257],\n",
       " 2015: [LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False),\n",
       "  0.02,\n",
       "  -0.0012077361981103194],\n",
       " 2016: [LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False),\n",
       "  0.02,\n",
       "  -0.002664948937119901],\n",
       " 2017: [LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False),\n",
       "  0.009778754866293366,\n",
       "  -0.017115905475259093],\n",
       " 2018: [LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False),\n",
       "  0.02,\n",
       "  -0.0038324304157883317],\n",
       " 2019: [LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False),\n",
       "  0.00847090795496849,\n",
       "  -0.02]}"
      ]
     },
     "execution_count": 952,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regrdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 953,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa,ab=US_class.signal_df_date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 954,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,_,_=US_class.signal_account(None,gross=(30,15),\n",
    "                        index_df=(0.5*abs_return_index_US.loc[\"SPX Index\"]+\\\n",
    "                                  0.5*abs_return_index_US.loc[\"RTY Index\"]).to_frame(\"US_index\"),\n",
    "                        net_level=0,\n",
    "                        risk_parity=True,\n",
    "                        liquidity=0.2,\n",
    "                        capital=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011467611843906666"
      ]
     },
     "execution_count": 906,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "US_class.long_df.cumsum().ffill().iloc[-1].sort_values().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16017809469029864"
      ]
     },
     "execution_count": 948,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "US_class.long_cache[2].sum(axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 955,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x78cba908>"
      ]
     },
     "execution_count": 955,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x78cba240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "US_class.long_cache[1].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 944,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016798128152522657"
      ]
     },
     "execution_count": 944,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "US_class.short_df.cumsum().ffill().iloc[-1].sort_values().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 942,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0838256D US  24/Jan/2013  01/May/2013  29/Mar/2013   -0.001178\n",
       "1436513D US  21/Apr/2016  21/Jul/2016  30/Jun/2016   -0.002269\n",
       "1437361D US  26/Nov/2014  04/Mar/2015  31/Dec/2014   -0.000975\n",
       "             27/Aug/2014  26/Nov/2014  30/Sep/2014   -0.000955\n",
       "             27/Feb/2014  23/May/2014  31/Mar/2014   -0.000873\n",
       "1468826D US  15/Feb/2013  03/May/2013  29/Mar/2013   -0.001632\n",
       "1477069D US  05/May/2015  17/Jul/2015  30/Jun/2015   -0.000822\n",
       "             06/Aug/2013  07/Nov/2013  30/Sep/2013   -0.000926\n",
       "             07/May/2013  06/Aug/2013  28/Jun/2013   -0.000763\n",
       "             17/Jul/2015  NaN          30/Sep/2015   -0.001261\n",
       "             24/Feb/2015  05/May/2015  31/Mar/2015   -0.000429\n",
       "             26/Feb/2013  07/May/2013  29/Mar/2013   -0.000979\n",
       "1498499D US  29/Oct/2014  11/Feb/2015  31/Dec/2014   -0.001509\n",
       "1500785D US  01/May/2013  01/Aug/2013  28/Jun/2013   -0.002147\n",
       "             09/Feb/2015  29/Apr/2015  31/Mar/2015   -0.001729\n",
       "             30/Apr/2014  31/Jul/2014  30/Jun/2014   -0.001147\n",
       "             30/Jan/2014  30/Apr/2014  31/Mar/2014   -0.001617\n",
       "1504558D US  22/Feb/2013  26/Apr/2013  29/Mar/2013   -0.001754\n",
       "1519128D US  23/May/2014  30/Jul/2014  30/Jun/2014   -0.001978\n",
       "1536637D US  01/May/2013  13/Aug/2013  28/Jun/2013   -0.000341\n",
       "             30/Jan/2013  01/May/2013  29/Mar/2013   -0.001093\n",
       "1539941D US  02/Nov/2016  17/Feb/2017  30/Dec/2016   -0.002225\n",
       "             03/Aug/2016  02/Nov/2016  30/Sep/2016   -0.002333\n",
       "             03/May/2013  06/Aug/2013  28/Jun/2013   -0.001632\n",
       "             04/May/2016  03/Aug/2016  30/Jun/2016   -0.001336\n",
       "             04/Nov/2015  03/Feb/2016  31/Dec/2015   -0.001046\n",
       "             05/Aug/2015  04/Nov/2015  30/Sep/2015   -0.001481\n",
       "             06/May/2015  05/Aug/2015  30/Jun/2015   -0.002434\n",
       "1541931D US  24/Jan/2014  25/Apr/2014  31/Mar/2014   -0.001485\n",
       "             24/Oct/2014  23/Jan/2015  31/Dec/2014   -0.000786\n",
       "                                                        ...   \n",
       "ZG US        08/May/2018  07/Aug/2018  29/Jun/2018   -0.000584\n",
       "             09/Aug/2017  08/Nov/2017  29/Sep/2017   -0.001300\n",
       "ZINCQ US     05/Nov/2013  25/Feb/2014  31/Dec/2013   -0.000546\n",
       "             08/May/2015  NaN          30/Jun/2015   -0.001268\n",
       "             24/Feb/2015  08/May/2015  31/Mar/2015   -0.000729\n",
       "ZION US      20/Oct/2015  26/Jan/2016  31/Dec/2015   -0.001632\n",
       "             21/Apr/2015  21/Jul/2015  30/Jun/2015   -0.001566\n",
       "             23/Jan/2018  24/Apr/2018  30/Mar/2018   -0.001334\n",
       "             23/Oct/2018  23/Jan/2019  31/Dec/2018   -0.001013\n",
       "             24/Apr/2018  24/Jul/2018  29/Jun/2018   -0.000741\n",
       "             24/Jul/2018  23/Oct/2018  28/Sep/2018   -0.001242\n",
       "             27/Jan/2015  21/Apr/2015  31/Mar/2015   -0.001487\n",
       "ZIOP US      01/Aug/2017  07/Nov/2017  29/Sep/2017   -0.000894\n",
       "ZLTQ US      02/Mar/2017  NaN          31/Mar/2017   -0.000907\n",
       "ZNGA US      03/Aug/2017  08/Nov/2017  29/Sep/2017   -0.001443\n",
       "             05/May/2017  03/Aug/2017  30/Jun/2017   -0.002317\n",
       "             07/May/2015  07/Aug/2015  30/Jun/2015   -0.000656\n",
       "             10/Feb/2017  05/May/2017  31/Mar/2017   -0.001508\n",
       "             13/Feb/2015  07/May/2015  31/Mar/2015   -0.001214\n",
       "ZOES US      01/Jun/2016  23/Aug/2016  30/Jun/2016   -0.001142\n",
       "ZQKSQ US     03/Jun/2014  NaN          30/Jun/2014   -0.000547\n",
       "ZSPH US      09/Nov/2015  NaN          31/Dec/2015   -0.000253\n",
       "ZTS US       02/Nov/2017  15/Feb/2018  29/Dec/2017   -0.003826\n",
       "             04/May/2017  08/Aug/2017  30/Jun/2017   -0.003698\n",
       "             08/Aug/2017  02/Nov/2017  29/Sep/2017   -0.002780\n",
       "             16/Feb/2017  04/May/2017  31/Mar/2017   -0.003753\n",
       "ZU US        05/Nov/2014  12/Feb/2015  31/Dec/2014   -0.000558\n",
       "ZUMZ US      05/Dec/2014  13/Mar/2015  31/Dec/2014   -0.000737\n",
       "             15/Mar/2013  24/May/2013  29/Mar/2013   -0.000582\n",
       "             24/May/2013  06/Sep/2013  28/Jun/2013   -0.000521\n",
       "Length: 8440, dtype: float64"
      ]
     },
     "execution_count": 942,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "US_class.short_cache[2].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 938,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x53154cc0>"
      ]
     },
     "execution_count": 938,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7d177a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "US_class.short_cache[1].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EAR_calc(ticker,date,return_df,EAR_period,vol_lookback):\n",
    "    '''\n",
    "    Calculate EAR from ticker and reference date\n",
    "    '''\n",
    "    if type(date)==pd.tslib.NaTType:\n",
    "        return None\n",
    "    elif type(date)==pd.tslib.Timestamp:\n",
    "        date=date.strftime(\"%d/%b/%Y\")\n",
    "    elif type(date)==str:\n",
    "        date=date\n",
    "    return_series=return_df.loc[ticker].dropna()\n",
    "    date_series=return_series.index.tolist()\n",
    "    if date in date_series:\n",
    "        day0=date_series.index(date)\n",
    "        post_series=return_series.iloc[day0:]\n",
    "        pre_series=return_series.iloc[:day0]\n",
    "        vol= return_series.iloc[day0-min(len(pre_series),vol_lookback+1):day0].std()\n",
    "        ret=(return_series.iloc[day0:day0+EAR_period]+1).prod()-1\n",
    "        nmove=ret/vol\n",
    "        return nmove\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_vol(signal_column,return_df,vol_lookback):\n",
    "    '''\n",
    "    Calculate simple vol from signal tuple\n",
    "    '''\n",
    "    signal_series=return_df.loc[signal_column.name[0]]\n",
    "    location=signal_series.index.tolist().index(signal_column.name[1])\n",
    "    vol_range=min(vol_lookback,len(signal_series[:location]))\n",
    "    signal_vol=signal_series[location-vol_range-1:location].std()\n",
    "    return signal_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_universe(signal_df,start_datetime,end_datetime,old_position):\n",
    "    '''\n",
    "    Slice the signal_df, both the index and entry date have to be \n",
    "    '''\n",
    "    \n",
    "\n",
    "    signal_df=signal_df.loc[start_datetime:end_datetime]\n",
    "    \n",
    "    if old_position is True:  \n",
    "        adj_signal_df=signal_df\n",
    "    else:\n",
    "        entry=signal_df.apply(lambda x:datetime.strptime(x.name[1],\"%d/%b/%Y\"),axis=0)\n",
    "        period_evaluate=(entry>=start_datetime)&(entry<=end_datetime)\n",
    "        adj_signal_df=signal_df.loc[:,period_evaluate]\n",
    "    \n",
    "    \n",
    "    zero_index=pd.Series(1,index=pd.date_range(start_datetime,end_datetime,freq='B')).to_frame()\n",
    "    adj_signal_df=pd.concat([adj_signal_df.drop_duplicates(),zero_index],axis=1).iloc[:,:-1]\n",
    "    return adj_signal_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_filter_stop(signal_df,stop_level,return_df,vol_lookback,stop_type,index_df):\n",
    "    '''\n",
    "    Input - signal_df\n",
    "    Get the updated signal df after the stop loss\n",
    "    stop_type:abs,rel\n",
    "    \n",
    "    '''\n",
    "    if stop_type=='abs':\n",
    "        vol_row=signal_df.apply(lambda column:signal_vol(column,return_df,vol_lookback),axis=0)\n",
    "        signal_cum_nmove=((1+signal_df).cumprod()-1).ffill()/vol_row\n",
    "        signal_df_stop=signal_df[-(signal_cum_nmove.expanding().min().shift(1,axis=0)<-stop_level)]\n",
    "    elif stop_type=='rel':\n",
    "        if index_df.shape[1]==1:\n",
    "            signal_count=signal_df.copy()\n",
    "            signal_count[((signal_count)>0) | ((signal_count)<0)]=1.0\n",
    "            signal_hedge=signal_count.apply(lambda x:x.multiply(index_df.iloc[:,0],axis=0))\n",
    "            \n",
    "            vol_row=signal_df.apply(lambda x:signal_vol(x,return_df,vol_lookback),axis=0)\n",
    "            rel_signal_cum_nmove=((1+signal_df).cumprod()-(1+signal_hedge).cumprod()).ffill()/vol_row\n",
    "            signal_df_stop=signal_df[-(rel_signal_cum_nmove.expanding().min().shift(1,axis=0)<-stop_level)]\n",
    "        else:\n",
    "            signal_count=signal_df.copy()\n",
    "            signal_count[((signal_count)>0) | ((signal_count)<0)]=1.0\n",
    "            signal_hedge=signal_count.apply(lambda x:x.multiply(index_df[Asia_mapping.loc[x.name[0][-2:]].iloc[0]],axis=0))\n",
    "            \n",
    "            vol_row=signal_df.apply(lambda x:signal_vol(x,return_df,vol_lookback),axis=0)\n",
    "            rel_signal_cum_nmove=((1+signal_df).cumprod()-(1+signal_hedge).cumprod()).ffill()/vol_row\n",
    "            signal_df_stop=signal_df[-(rel_signal_cum_nmove.expanding().min().shift(1,axis=0)<-stop_level)]            \n",
    "            \n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    return signal_df_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revision_adjusted_size(reference_signal_df,lower_revision,higher_revision,size_multiple,revision_row,revision_row_reference,gross,long):\n",
    "    \n",
    "    ''' \n",
    "    Use positive size\n",
    "    '''\n",
    "    \n",
    "    lower_size=0.01\n",
    "    higher_size=lower_size*size_multiple\n",
    "    \n",
    "    if long is True:\n",
    "        \n",
    "        size_row_reference=revision_row_reference.to_frame().copy().apply(lambda x: lower_size+(higher_size-lower_size)\\\n",
    "                                                      *(np.abs(x.iloc[0]-lower_revision))/np.abs(higher_revision-lower_revision) \\\n",
    "                                                      if np.abs(x.iloc[0])<=np.abs(higher_revision) else higher_size,axis=1)\n",
    "\n",
    "        size_df_reference=(1+reference_signal_df).cumprod()*size_row_reference\n",
    "\n",
    "        trial_gross=np.abs(size_df_reference.sum(axis=1).mean())\n",
    "        new_lower_size=lower_size/(trial_gross*100/gross)\n",
    "        new_higher_size=higher_size/(trial_gross*100/gross)\n",
    "\n",
    "        size_row=revision_row.to_frame().copy().apply(lambda x: new_lower_size+(new_higher_size-new_lower_size)\\\n",
    "                                                      *(np.abs(x.iloc[0]-lower_revision))/np.abs(higher_revision-lower_revision) \\\n",
    "                                                      if np.abs(x.iloc[0])<=np.abs(higher_revision) else new_higher_size,axis=1)\n",
    "    else:\n",
    "        size_row_reference=revision_row_reference.to_frame().copy().apply(lambda x: lower_size+(higher_size-lower_size)\\\n",
    "                                                      *(np.abs(x.iloc[0]-lower_revision))/np.abs(higher_revision-lower_revision) \\\n",
    "                                                      if np.abs(x.iloc[0])<=np.abs(lower_revision) else higher_size,axis=1)\n",
    "\n",
    "        size_df_reference=(1-reference_signal_df).cumprod()*size_row_reference\n",
    "\n",
    "        trial_gross=np.abs(size_df_reference.sum(axis=1).mean())\n",
    "        new_lower_size=lower_size/(trial_gross*100/gross)\n",
    "        new_higher_size=higher_size/(trial_gross*100/gross)\n",
    "\n",
    "        size_row=revision_row.to_frame().copy().apply(lambda x: new_lower_size+(new_higher_size-new_lower_size)\\\n",
    "                                                      *(np.abs(x.iloc[0]-lower_revision))/np.abs(higher_revision-lower_revision) \\\n",
    "                                                      if np.abs(x.iloc[0])<=np.abs(lower_revision) else new_higher_size,axis=1)\n",
    "\n",
    "    return size_row, new_lower_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sizing(signal_df,reference_signal_df,gross,fundamental_df,new_signal,return_df,risk_parity,liquidity,capital,\\\n",
    "           revision_adjust,long):\n",
    "    '''\n",
    "    Use historical signal_df range to calculate the size row for the current signal_df range\n",
    "    Idea is to use historical as a benchmark for future sizing\n",
    "    '''\n",
    "    \n",
    "    fundamental_df=fundamental_df.copy().sort_index()\n",
    "    vol_reference=reference_signal_df.apply(lambda x:signal_vol(x,return_df,30),axis=0).mean()\n",
    "    vol_row=signal_df.apply(lambda x:signal_vol(x,return_df,30),axis=0)\n",
    "    \n",
    "    '''\n",
    "    Revision row needs to be updated using reference_signal\n",
    "    new sizing scheme is a linear function\n",
    "    revision_adjust=(True/False,lower_revision,higher_revision,size_multiple)\n",
    "    '''\n",
    "    \n",
    "    if revision_adjust[0] is True:\n",
    "        \n",
    "        if new_signal is True:     \n",
    "            revision_row_reference=reference_signal_df.apply(lambda x:fundamental_df.loc[x.name,\"Revision_real\"],axis=0)\n",
    "            revision_row=signal_df.apply(lambda x:fundamental_df.loc[x.name,\"Revision_real\"],axis=0)\n",
    "        else:\n",
    "            revision_row_reference=reference_signal_df.apply(lambda x:fundamental_df.loc[x.name,\"Revision_20\"],axis=0)\n",
    "            revision_row=signal_df.apply(lambda x:fundamental_df.loc[x.name,\"Revision_20\"],axis=0)            \n",
    "        \n",
    "        if long is True and revision_adjust[1] is not None:\n",
    "            lower_revision=revision_adjust[1][0]\n",
    "            higher_revision=revision_adjust[1][1]\n",
    "            size_multiple=revision_adjust[3]\n",
    "        \n",
    "            base_size,low_size=revision_adjusted_size(reference_signal_df,lower_revision,higher_revision,size_multiple,\\\n",
    "                                                      revision_row,revision_row_reference,gross,True)\n",
    "            if risk_parity is True:\n",
    "                size_row=signal_df.apply(lambda x: min(base_size[x.name]/(vol_row[x.name]/vol_reference),\\\n",
    "                                                       fundamental_df.loc[x.name[0],x.name[1]][\"Volume\"].iloc[0]*\\\n",
    "                                                       liquidity/capital),axis=0)\n",
    "            else:\n",
    "                size_row=signal_df.apply(lambda x: min(base_size[x.name], fundamental_df.loc[x.name[0],x.name[1]][\"Volume\"].iloc[0]\\\n",
    "                                                       *liquidity/capital),axis=0)            \n",
    "\n",
    "        elif long is False and revision_adjust[2] is not None:\n",
    "            lower_revision=revision_adjust[2][0]\n",
    "            higher_revision=revision_adjust[2][1]\n",
    "            size_multiple=revision_adjust[3]\n",
    "        \n",
    "            base_size,low_size=revision_adjusted_size(reference_signal_df,lower_revision,higher_revision,size_multiple,revision_row,\\\n",
    "                                             revision_row_reference,gross,False) \n",
    "\n",
    "            if risk_parity is True:\n",
    "                size_row=signal_df.apply(lambda x: min(base_size[x.name]/(vol_row[x.name]/vol_reference),\\\n",
    "                                                       fundamental_df.loc[x.name[0],x.name[1]][\"Volume\"].iloc[0]*\\\n",
    "                                                       liquidity/capital),axis=0)\n",
    "            else:\n",
    "                size_row=signal_df.apply(lambda x: min(base_size[x.name], fundamental_df.loc[x.name[0],x.name[1]][\"Volume\"].iloc[0]\\\n",
    "                                                       *liquidity/capital),axis=0)\n",
    "\n",
    "        else:\n",
    "            size_row=None\n",
    "            low_size=None\n",
    "\n",
    "    elif revision_adjust[0]=='constant':\n",
    "        if risk_parity is True:\n",
    "            size_row=signal_df.apply(lambda x: min(revision_adjust[1]/(vol_row[x.name]/vol_reference),\\\n",
    "                                                   fundamental_df.loc[x.name[0],x.name[1]][\"Volume\"].iloc[0]*\\\n",
    "                                                   liquidity/capital),axis=0)\n",
    "        else:\n",
    "            size_row=signal_df.apply(lambda x: min(revision_adjust[1], fundamental_df.loc[x.name[0],x.name[1]][\"Volume\"].iloc[0]\\\n",
    "                                                   *liquidity/capital),axis=0)\n",
    "        low_size=None\n",
    "\n",
    "    else:\n",
    "        number=reference_signal_df.count(axis=1).mean()\n",
    "        avg_size=gross/100/number\n",
    "\n",
    "        if risk_parity is True:\n",
    "            size_row=signal_df.apply(lambda x: min(avg_size/(vol_row[x.name]/vol_reference),\\\n",
    "                                                   fundamental_df.loc[x.name[0],x.name[1]][\"Volume\"].iloc[0]*\\\n",
    "                                                   liquidity/capital),axis=0)\n",
    "        else:\n",
    "            size_row=signal_df.apply(lambda x: min(avg_size, fundamental_df.loc[x.name[0],x.name[1]][\"Volume\"].iloc[0]\\\n",
    "                                                   *liquidity/capital),axis=0)\n",
    "        low_size=None\n",
    "    return size_row,low_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trading_analytics_date(portfolio_cache):\n",
    "    '''\n",
    "    Key portfolio metrics from portfolio cache\n",
    "    Feed into plot function\n",
    "    '''\n",
    "    \n",
    "    ind_return=portfolio_cache[3]\n",
    "    signal_count=len(ind_return)\n",
    "    account_curve=portfolio_cache[1]\n",
    "    \n",
    "    if signal_count==0:\n",
    "        return None,None,None,None,None,None,None\n",
    "    else:\n",
    "        mean_return=ind_return.mean()\n",
    "        hit_rate=len(ind_return[ind_return>0])/len(ind_return)*1.0\n",
    "        payoff_ratio=ind_return[ind_return>0].mean()/ind_return[ind_return<0].mean()*-1.0\n",
    "        \n",
    "        account_price=account_curve+1\n",
    "        ann_vol=np.std(account_price.diff()/account_price.shift(1))*(260**0.5)\n",
    "        ann_ret=(account_price.iloc[-1]**(1/len(account_price)))**260-1\n",
    "        ann_sharpe=ann_ret/ann_vol\n",
    "        \n",
    "        max_dd=-((1+account_curve)-(1+account_curve).cummax(axis=0)).expanding().min().min()\n",
    "        \n",
    "        #low_date=(np.maximum.accumulate(account_curve)-account_curve).idxmax()\n",
    "        #high_date=account_curve[:low_date].idxmax()\n",
    "        #max_dd=1-(1+account_curve[low_date])/(1+account_curve[high_date])\n",
    "        \n",
    "        return signal_count,hit_rate,payoff_ratio,ann_ret,ann_vol,ann_sharpe,max_dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trading_analytics_simp(account_curve):\n",
    "    '''\n",
    "    Key portfolio metrics from portfolio account curve\n",
    "    Only sharpe and drawdown\n",
    "    '''\n",
    "\n",
    "\n",
    "    account_price=account_curve+1\n",
    "    ann_vol=np.std(account_price.diff()/account_price.shift(1))*(260**0.5)\n",
    "    ann_ret=(account_price.iloc[-1]**(1/len(account_price)))**260-1\n",
    "    ann_sharpe=ann_ret/ann_vol\n",
    "\n",
    "    max_dd=-((1+account_curve)-(1+account_curve).cummax(axis=0)).expanding().min().min()\n",
    "\n",
    "    #low_date=(np.maximum.accumulate(account_curve)-account_curve).idxmax()\n",
    "    #high_date=account_curve[:low_date].idxmax()\n",
    "    #max_dd=1-(1+account_curve[low_date])/(1+account_curve[high_date])\n",
    "\n",
    "    return ann_sharpe,max_dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_signal(title,figsize,portfolio_cache):\n",
    "\n",
    "    account_curve=portfolio_cache[1]\n",
    "    avg_size=np.abs(portfolio_cache[2]).mean(axis=0).mean()\n",
    "    ind_return=portfolio_cache[3]\n",
    "    gross=portfolio_cache[4]\n",
    "    turnover=portfolio_cache[5]\n",
    "\n",
    "    fig=plt.figure(figsize=figsize)\n",
    "    ax1=fig.add_subplot(1,1,1)\n",
    "    ln1=ax1.plot(account_curve,label='signal',color='b')\n",
    "\n",
    "    val1=ax1.get_yticks()\n",
    "    start=val1[0]\n",
    "    end=val1[-1]\n",
    "    ax1.set_yticks(np.arange(start,end,0.01))  \n",
    "    adj_val1=ax1.get_yticks()\n",
    "    ax1.set_yticklabels([\"{:.0%}\".format(x) for x in adj_val1])\n",
    "\n",
    "    ax2=ax1.twinx()\n",
    "    ln2=ax2.plot(gross,label='gross',color='silver')\n",
    "\n",
    "    val2=ax2.get_yticks()\n",
    "    start=val2[0]\n",
    "    end=val2[-1]\n",
    "    ax2.set_yticks(np.arange(start,end,0.1))  \n",
    "    adj_val2=ax2.get_yticks()\n",
    "    ax2.set_yticklabels([\"{:.0%}\".format(x) for x in adj_val2])\n",
    "\n",
    "    count,hit,payoff,ret,vol,sharpe,max_dd=trading_analytics_date(portfolio_cache)\n",
    "\n",
    "    plt.title(\"\\n\".join(wrap('count='+str(count)+\n",
    "                             ',avg_size='+str(\"{:.1%}\".format(avg_size))+\n",
    "                             ',hit_rate='+str(\"{:.0%}\".format(hit))+\n",
    "                             ',payoff='+str(round(payoff,1))+\n",
    "                             ',return='+str(\"{:.1%}\".format(ret))+\n",
    "                             ',vol='+str(\"{:.1%}\".format(vol))+\n",
    "                             ',sharpe='+str(round(sharpe,1))+\n",
    "                             ',turnover='+str(round(turnover,1))+'x'+                             \n",
    "                             ',max_drawdown='+str(\"{:.1%}\".format(max_dd)))),fontsize=10)\n",
    "\n",
    "    ax1.set_xlabel('Year')\n",
    "    ax1.set_ylabel('Return')\n",
    "    ax2.set_ylabel('Exposure')\n",
    "    plt.suptitle(title,y=1.05,fontsize=16)\n",
    "    plt.grid(linestyle='dashed')\n",
    "    plt.legend(ln1+ln2,[l.get_label() for l in ln1+ln2],loc=2)\n",
    "    ax1.axhline(y=0,color='k')\n",
    "\n",
    "    plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Signal class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {},
   "outputs": [],
   "source": [
    "class signal(object):\n",
    "    '''\n",
    "    Signal class is built to initialize the signal_df and account curve from base parameters \n",
    "    '''\n",
    "    def __init__(self,fundamental_df,price_df,EAR_period,entry,long_criteria,short_criteria,holding,start,end,old_position\\\n",
    "                 ,new_signal,revision_adjust,early_exit):\n",
    "        '''\n",
    "        Define the key free parameters of the signal\n",
    "        Criteria:(EAR,revision,revision_norm,size)\n",
    "        revision_adjust:(True/False,(long lower revision,long higher revision),(short low abs revision,short high abs_revision),size_multiple)\n",
    "        '''\n",
    "        self.fundamental_df=fundamental_df\n",
    "        self.price_df=price_df\n",
    "        self.abs_return=price_df.diff(1,axis=1)/price_df.shift(1,axis=1)\n",
    "        self.EAR_period=EAR_period\n",
    "        self.entry=entry\n",
    "        self.long_criteria=long_criteria\n",
    "        self.short_criteria=short_criteria\n",
    "        self.holding=holding\n",
    "        self.start=start\n",
    "        self.end=end\n",
    "        self.old_position=old_position\n",
    "        self.new_signal=new_signal\n",
    "        self.revision_adjust=revision_adjust #(True/False,lower_revision,higher_revision in absolute terms,size_multiple)\n",
    "        self.early_exit=early_exit\n",
    "        \n",
    "    def signal_base(self):\n",
    "        '''\n",
    "        Filter the signal criteria like EAR and rperioevision, get the target signal list\n",
    "        From the fundamental information\n",
    "        For both long and short side\n",
    "        '''\n",
    "        \n",
    "        long_base=self.fundamental_df.copy()\n",
    "        short_base=self.fundamental_df.copy()\n",
    "        \n",
    "\n",
    "        long_base[\"EAR\"]=long_base.apply(lambda x:EAR_calc(x.name[0],x.name[1],self.abs_return,self.EAR_period,30)\\\n",
    "                                     if x.name[0] in self.abs_return.index else None,axis=1)\n",
    "\n",
    "        short_base[\"EAR\"]=short_base.apply(lambda x:EAR_calc(x.name[0],x.name[1],self.abs_return,self.EAR_period,30)\\\n",
    "                                     if x.name[0] in self.abs_return.index else None,axis=1)\n",
    "\n",
    "\n",
    "        if self.long_criteria is None:\n",
    "            long_base=None\n",
    "        else:\n",
    "            \n",
    "            if self.long_criteria[1] is None:\n",
    "                pass\n",
    "            else:\n",
    "                if self.new_signal is True:\n",
    "                    long_base=long_base[(long_base[\"Revision_real\"]>self.long_criteria[1][0])\\\n",
    "                                                    &(long_base[\"Revision_real\"]<self.long_criteria[1][1])]\n",
    "                else:\n",
    "                    long_base=long_base[(long_base[\"Revision_20\"]>self.long_criteria[1][0])\\\n",
    "                                                    &(long_base[\"Revision_20\"]<self.long_criteria[1][1])]                    \n",
    "\n",
    "            if self.long_criteria[2] is None:\n",
    "                pass\n",
    "            else:\n",
    "                long_base=long_base[(long_base[\"Revision_norm\"]>self.long_criteria[2][0])\\\n",
    "                                                    &(long_base[\"Revision_norm\"]<self.long_criteria[2][1])]\n",
    "                \n",
    "            if self.long_criteria[3] is None:\n",
    "                pass\n",
    "            else:\n",
    "                long_base=long_base[(long_base[\"Market cap\"]>self.long_criteria[3][0])&\\\n",
    "                                        (long_base[\"Market cap\"]<self.long_criteria[3][1])]\n",
    "            \n",
    "            if self.long_criteria[0] is None:\n",
    "                pass\n",
    "            else:\n",
    "                long_base=long_base[(long_base[\"EAR\"]>self.long_criteria[0][0])&(long_base[\"EAR\"]<self.long_criteria[0][1])]\n",
    "                \n",
    "                \n",
    "        if self.short_criteria is None:\n",
    "            short_base=None\n",
    "        else:\n",
    "            \n",
    "            if self.short_criteria[1] is None:\n",
    "                pass\n",
    "            else:\n",
    "                if self.new_signal is True:\n",
    "                    short_base=short_base[(short_base[\"Revision_real\"]>self.short_criteria[1][0])\\\n",
    "                                                    &(short_base[\"Revision_real\"]<self.short_criteria[1][1])]\n",
    "                else:\n",
    "                    short_base=short_base[(short_base[\"Revision_20\"]>self.short_criteria[1][0])\\\n",
    "                                                    &(short_base[\"Revision_20\"]<self.short_criteria[1][1])]                    \n",
    "\n",
    "            if self.short_criteria[2] is None:\n",
    "                pass\n",
    "            else:\n",
    "                short_base=short_base[(short_base[\"Revision_norm\"]>self.short_criteria[2][0])\\\n",
    "                                                    &(short_base[\"Revision_norm\"]<self.short_criteria[2][1])]\n",
    "                \n",
    "            if self.short_criteria[3] is None:\n",
    "                pass\n",
    "            else:\n",
    "                short_base=short_base[(short_base[\"Market cap\"]>self.short_criteria[3][0])&\\\n",
    "                                        (short_base[\"Market cap\"]<self.short_criteria[3][1])]\n",
    "            \n",
    "            if self.short_criteria[0] is None:\n",
    "                pass\n",
    "            else:\n",
    "                short_base=short_base[(short_base[\"EAR\"]>self.short_criteria[0][0])&\\\n",
    "                                      (short_base[\"EAR\"]<self.short_criteria[0][1])]\n",
    "        \n",
    "        self.long_base=long_base\n",
    "        self.short_base=short_base\n",
    "        \n",
    "        return long_base,short_base\n",
    "    \n",
    "    def signal_df_date(self):#if we hold them through next earning\n",
    "        '''\n",
    "        Obtain the signal_df function over the whole time period from the target signal list\n",
    "        '''\n",
    "        try:\n",
    "            long_base=self.long_base.copy()\n",
    "            short_base=self.short_base.copy()\n",
    "        except:\n",
    "            long_base,short_base=signal.signal_base(self)\n",
    "        \n",
    "        if long_base is None:\n",
    "            long_df=None\n",
    "        \n",
    "        else:\n",
    "            long_df=pd.DataFrame(index=self.price_df.columns)\n",
    "\n",
    "            for s in long_base.index:\n",
    "                return_series=self.abs_return.loc[s[0]]\n",
    "                if s[1] in return_series.index:\n",
    "                    if not np.isnan(return_series.loc[s[1]]): \n",
    "                        day0=return_series.index.tolist().index(s[1])\n",
    "                        \n",
    "                        if self.early_exit is True and datetime.strptime(s[1],\"%d/%b/%Y\").date()>=\\\n",
    "                        datetime.strptime(s[3],\"%d/%b/%Y\").date():\n",
    "                            period=None\n",
    "                        \n",
    "                        elif self.early_exit is True and type(s[2])!=float and datetime.strptime(s[1],\"%d/%b/%Y\").date()<\\\n",
    "                        datetime.strptime(s[3],\"%d/%b/%Y\").date():\n",
    "                            period=min(self.holding,np.busday_count(datetime.strptime(s[1],\"%d/%b/%Y\").date(),\\\n",
    "                                                                    datetime.strptime(s[2],\"%d/%b/%Y\").date())-self.entry,\n",
    "                                      np.busday_count(datetime.strptime(s[1],\"%d/%b/%Y\").date(),\\\n",
    "                                                                    datetime.strptime(s[3],\"%d/%b/%Y\").date())-self.entry+1)                            \n",
    "                        \n",
    "                        elif self.early_exit is True and type(s[2])==float and datetime.strptime(s[1],\"%d/%b/%Y\").date()<\\\n",
    "                        datetime.strptime(s[3],\"%d/%b/%Y\").date():\n",
    "                            period=min(self.holding,np.busday_count(datetime.strptime(s[1],\"%d/%b/%Y\").date(),\\\n",
    "                                                                    datetime.strptime(s[3],\"%d/%b/%Y\").date())-self.entry+1)\n",
    "                            \n",
    "                        elif type(s[2])==float:##basically np.nan has type float\n",
    "                            period=self.holding+1\n",
    "                        \n",
    "                        else: ##assume that we are not holding through numbers\n",
    "                            period=min(self.holding,np.busday_count(datetime.strptime(s[1],\"%d/%b/%Y\").date(),\\\n",
    "                                                                    datetime.strptime(s[2],\"%d/%b/%Y\").date())-self.entry)\n",
    "                            \n",
    "                        if period is None:\n",
    "                            pass\n",
    "                        else:\n",
    "                            target_series=return_series.iloc[day0+self.entry-1:day0+min(period+self.entry, \\\n",
    "                                                                                           len(return_series[day0:]))].dropna()\n",
    "                            \n",
    "                            if len(target_series)==0:\n",
    "                                pass\n",
    "                            else:\n",
    "                                target_series.iloc[0]=0.0\n",
    "                                long_df[s]=target_series                        \n",
    "\n",
    "                    else:\n",
    "                        pass\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "            long_df=long_df.reindex(datetime.strptime(i,\"%d/%b/%Y\") for i in long_df.index)\n",
    "            long_df=long_df.sort_index()\n",
    "\n",
    "            if self.start is not None:\n",
    "                long_df=slice_universe(long_df,self.start,self.end,self.old_position)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            long_df=long_df.dropna(how=\"all\",axis=1)\n",
    "            #long_df.columns=pd.MultiIndex.from_tuples(pd.Series(list(long_df.columns)))\n",
    "            self.long_df=long_df\n",
    "            \n",
    "            \n",
    "        \n",
    "        if short_base is None:\n",
    "            short_df=None\n",
    "        \n",
    "        else:\n",
    "            short_df=pd.DataFrame(index=self.price_df.columns)\n",
    "\n",
    "            for s in short_base.index:\n",
    "                return_series=self.abs_return.loc[s[0]]\n",
    "                if s[1] in return_series.index:\n",
    "                    if not np.isnan(return_series.loc[s[1]]): \n",
    "                        day0=return_series.index.tolist().index(s[1])\n",
    "                        \n",
    "                        if self.early_exit is True and datetime.strptime(s[1],\"%d/%b/%Y\").date()>=\\\n",
    "                        datetime.strptime(s[3],\"%d/%b/%Y\").date():\n",
    "                            period=None\n",
    "                            \n",
    "                        elif self.early_exit is True and type(s[2])!=float and datetime.strptime(s[1],\"%d/%b/%Y\").date()<\\\n",
    "                        datetime.strptime(s[3],\"%d/%b/%Y\").date():\n",
    "                            period=min(self.holding,np.busday_count(datetime.strptime(s[1],\"%d/%b/%Y\").date(),\\\n",
    "                                                                    datetime.strptime(s[2],\"%d/%b/%Y\").date())-self.entry,\n",
    "                                      np.busday_count(datetime.strptime(s[1],\"%d/%b/%Y\").date(),\\\n",
    "                                                                    datetime.strptime(s[3],\"%d/%b/%Y\").date())-self.entry+1)  \n",
    "                            \n",
    "                        elif self.early_exit is True and type(s[2])==float and datetime.strptime(s[1],\"%d/%b/%Y\").date()<\\\n",
    "                        datetime.strptime(s[3],\"%d/%b/%Y\").date():\n",
    "                            period=min(self.holding,np.busday_count(datetime.strptime(s[1],\"%d/%b/%Y\").date(),\\\n",
    "                                                                    datetime.strptime(s[3],\"%d/%b/%Y\").date())-self.entry+1)\n",
    "                        elif type(s[2])==float:##basically np.nan has type float\n",
    "                            period=self.holding\n",
    "                    \n",
    "                        else: ##assume that we are not holding through numbers\n",
    "                            period=min(self.holding,np.busday_count(datetime.strptime(s[1],\"%d/%b/%Y\").date(),\\\n",
    "                                                                    datetime.strptime(s[2],\"%d/%b/%Y\").date())-self.entry)\\\n",
    "\n",
    "\n",
    "                        if period is None:\n",
    "                            pass\n",
    "                        else:\n",
    "                            target_series=return_series.iloc[day0+self.entry-1:day0+min(period+self.entry, \\\n",
    "                                                                                           len(return_series[day0:]))].dropna()\n",
    "                            \n",
    "                            if len(target_series)==0:\n",
    "                                pass\n",
    "                            else:\n",
    "                                target_series.iloc[0]=0.0\n",
    "                                short_df[s]=target_series         \n",
    "                            \n",
    "                    else:\n",
    "                        pass\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "            short_df=short_df.reindex(datetime.strptime(i,\"%d/%b/%Y\") for i in short_df.index)\n",
    "            short_df=short_df.sort_index()\n",
    "\n",
    "            if self.start is not None:\n",
    "                short_df=slice_universe(short_df,self.start,self.end,self.old_position)\n",
    "            else:\n",
    "                pass  \n",
    "            \n",
    "            short_df=short_df.dropna(how=\"all\",axis=1)\n",
    "            #short_df.columns=pd.MultiIndex.from_tuples(pd.Series(list(short_df.columns)))\n",
    "            self.short_df=short_df\n",
    "        \n",
    "        return long_df,short_df\n",
    "\n",
    "    def signal_account(self,stop,gross,index_df,net_level,risk_parity,liquidity,capital):\n",
    "        '''\n",
    "        Build the account curve with signal_df\n",
    "        Assume quarterly rebalancing that's why the period list has quarter as the key\n",
    "        Take extra care when building the account curve, the logic is: work out the size_df, then shift by 1 and * signal_df\n",
    "        Stop=(long_stop,short_stop,type)\n",
    "        index_df has to be a dataframe with a name\n",
    "        '''\n",
    "        \n",
    "        try:\n",
    "            long_df=self.long_df.copy()\n",
    "            short_df=self.short_df.copy()\n",
    "            \n",
    "        except:\n",
    "            long_df,short_df=self.signal_df_date()\n",
    "                   \n",
    "        '''\n",
    "        Assign values for later use\n",
    "        '''\n",
    "        \n",
    "        self.capital=capital\n",
    "        \n",
    "        self.index_df=index_df\n",
    "        self.index_df.index=[datetime.strptime(i,\"%d/%b/%Y\") for i in self.index_df.index]\n",
    "       \n",
    "        \n",
    "        '''\n",
    "        Define rebalance period first\n",
    "        '''\n",
    "        if long_df is None:\n",
    "            period_quarter=short_df.apply(lambda x:str(x.name.year)+\" \"+str(x.name.quarter),axis=1)\n",
    "            period_list=list(set(period_quarter))\n",
    "            period_list.sort()    \n",
    "            \n",
    "        else:\n",
    "            period_quarter=long_df.apply(lambda x:str(x.name.year)+\" \"+str(x.name.quarter),axis=1)\n",
    "            period_list=list(set(period_quarter))\n",
    "            period_list.sort()    \n",
    "                \n",
    "        '''\n",
    "        Separate out long and short\n",
    "        '''\n",
    "        if long_df is None:\n",
    "            long_cache=(None,None,None,None)\n",
    "        else:\n",
    "            if stop is None:\n",
    "                pass\n",
    "            else:\n",
    "                long_df=signal_filter_stop(long_df,stop[0],self.abs_return,30,stop[2],self.index_df)   \n",
    "                self.long_df=long_df\n",
    "\n",
    "            long_sub_signal={}\n",
    "            long_sub_size_row={}\n",
    "            long_sub_size_df={}\n",
    "            long_sub_pnl={}\n",
    "        \n",
    "            for s in period_list:\n",
    "            \n",
    "                long_sub_signal[s]=long_df[period_quarter==s].dropna(how='all',axis=1)\n",
    "                \n",
    "                if long_sub_signal[s].shape[1]==0:\n",
    "                    long_sub_size_df[s]=long_sub_signal[s]\n",
    "                    long_sub_pnl[s]=long_sub_signal[s]\n",
    "                    \n",
    "                else:\n",
    "                    if period_list.index(s)<4:##use last quarter's sizing as reference\n",
    "                        long_sub_size_row[s]=sizing(long_sub_signal[s],long_sub_signal[s],gross[0],self.fundamental_df,\\\n",
    "                                                       self.new_signal,self.abs_return,risk_parity,liquidity,capital,\\\n",
    "                                                    self.revision_adjust,True)[0]\n",
    "                    else:\n",
    "                        try:\n",
    "                            long_sub_size_row[s]=sizing(long_sub_signal[s],long_sub_signal[period_list[period_list.index(s)-1]],\\\n",
    "                                                       gross[0],self.fundamental_df,self.new_signal,self.abs_return,\\\n",
    "                                                        risk_parity,liquidity,capital,self.revision_adjust,True)[0]\n",
    "                        except:\n",
    "                            long_sub_size_row[s]=sizing(long_sub_signal[s],long_sub_signal[s],\\\n",
    "                                                       gross[0],self.fundamental_df,self.new_signal,self.abs_return,\\\n",
    "                                                        risk_parity,liquidity,capital,self.revision_adjust,True)[0] \n",
    "\n",
    "                    long_sub_size_df[s]=(1+long_sub_signal[s]).cumprod()*long_sub_size_row[s]\n",
    "                    long_sub_pnl[s]=(long_sub_size_df[s].shift(1))*long_sub_signal[s] \n",
    "                    # need to shift by 1 as the size is end of the day\n",
    "        \n",
    "            long_daily_pnl=pd.concat(list(long_sub_pnl.values()),axis=0)\n",
    "            long_acct_curve=long_daily_pnl.cumsum().ffill().sum(axis=1)\n",
    "            long_size_df=pd.concat(list(long_sub_size_df.values()),axis=0)\n",
    "            long_ind_return=long_daily_pnl.cumsum().ffill().iloc[-1].dropna()\n",
    "            long_cache=(long_daily_pnl,long_acct_curve,long_size_df,long_ind_return)\n",
    "            \n",
    "            self.long_cache=long_cache\n",
    "\n",
    "            \n",
    "        if short_df is None:\n",
    "            short_cache=(None,None,None,None)\n",
    "        else:\n",
    "            if stop is None:\n",
    "                pass\n",
    "            else:\n",
    "                short_df=-signal_filter_stop(-short_df,stop[1],self.abs_return,30,stop[2],self.index_df)   \n",
    "                self.short_df=short_df\n",
    "\n",
    "            short_sub_signal={}\n",
    "            short_sub_size_row={}\n",
    "            short_sub_size_df={}\n",
    "            short_sub_pnl={}\n",
    "        \n",
    "            for s in period_list:\n",
    "                short_sub_signal[s]=short_df[period_quarter==s].dropna(how='all',axis=1)\n",
    "\n",
    "                if short_sub_signal[s].shape[1]==0:\n",
    "                    short_sub_size_df[s]=short_sub_signal[s]\n",
    "                    short_sub_pnl[s]=short_sub_signal[s]\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    if period_list.index(s)<4:##use last quarter's sizing as reference\n",
    "                        short_sub_size_row[s]=-sizing(short_sub_signal[s],short_sub_signal[s],gross[1],self.fundamental_df,\\\n",
    "                                                       self.new_signal,self.abs_return,risk_parity,liquidity,capital,\\\n",
    "                                                      self.revision_adjust,False)[0]\n",
    "                    else:\n",
    "                        try:\n",
    "                            short_sub_size_row[s]=-sizing(short_sub_signal[s],short_sub_signal\\\n",
    "                                                          [period_list[period_list.index(s)-1]],\\\n",
    "                                                           gross[1],self.fundamental_df,self.new_signal,self.abs_return,\\\n",
    "                                                          risk_parity,liquidity,\\\n",
    "                                                          capital,self.revision_adjust,False)[0]\n",
    "                        except:\n",
    "                            short_sub_size_row[s]=-sizing(short_sub_signal[s],short_sub_signal[s],\\\n",
    "                                                           gross[1],self.fundamental_df,self.new_signal,self.abs_return,\\\n",
    "                                                          risk_parity,liquidity,\\\n",
    "                                                          capital,self.revision_adjust,False)[0]\n",
    "\n",
    "                    short_sub_size_df[s]=(1+short_sub_signal[s]).cumprod()*short_sub_size_row[s]\n",
    "                    short_sub_pnl[s]=(short_sub_size_df[s].shift(1))*short_sub_signal[s] \n",
    "                # need to shift by 1 as the size is end of the day\n",
    "        \n",
    "            short_daily_pnl=pd.concat(list(short_sub_pnl.values()),axis=0)\n",
    "            short_acct_curve=short_daily_pnl.cumsum().ffill().sum(axis=1)\n",
    "            short_size_df=pd.concat(list(short_sub_size_df.values()),axis=0)\n",
    "            short_ind_return=short_daily_pnl.cumsum().ffill().iloc[-1].dropna()\n",
    "            \n",
    "            short_cache=(short_daily_pnl,short_acct_curve,short_size_df,short_ind_return)\n",
    "            self.short_cache=short_cache\n",
    "    \n",
    "        '''Put alpha positions together to form the alpha part'''\n",
    "        alpha_df=pd.concat([long_df,short_df],axis=1)\n",
    "        self.alpha_df=alpha_df\n",
    "        \n",
    "        alpha_daily_pnl=pd.concat([long_cache[0],short_cache[0]],axis=1)\n",
    "        alpha_acct_curve=alpha_daily_pnl.cumsum().ffill().sum(axis=1)\n",
    "        alpha_size_df=pd.concat([long_cache[2],short_cache[2]],axis=1)\n",
    "        alpha_ind_return=pd.concat([long_cache[3],short_cache[3]],axis=0)\n",
    "        \n",
    "        alpha_cache=(alpha_daily_pnl,alpha_acct_curve,alpha_size_df,alpha_ind_return)\n",
    "        \n",
    "        \n",
    "        if self.index_df is not None:\n",
    "            if self.index_df.shape[1]==1:\n",
    "                index_df=self.index_df.copy().loc[alpha_df.index] \n",
    "                index_size_df=(net_level-alpha_size_df.sum(axis=1)).to_frame(index_df.columns[0])\n",
    "                index_daily_pnl=index_size_df.shift(1)*index_df\n",
    "                index_acct_curve=index_daily_pnl.cumsum()\n",
    "                index_ind_return=index_acct_curve.iloc[-1]\n",
    "                index_cache=(index_daily_pnl,index_acct_curve,index_size_df,index_ind_return)\n",
    "            else:\n",
    "                index_df=self.index_df.copy().loc[alpha_df.index] \n",
    "                alpha_temp=alpha_cache[2].copy().T\n",
    "                alpha_temp[\"index\"]=alpha_temp.apply(lambda x:Asia_mapping.loc[x.name[0][-2:]].iloc[0],axis=1)\n",
    "                index_size_df=net_level-alpha_temp.groupby(\"index\").apply(sum).T.iloc[:-1]\n",
    "                index_daily_pnl=index_size_df.shift(1)*index_df\n",
    "                index_acct_curve=index_daily_pnl.cumsum()\n",
    "                index_ind_return=index_acct_curve.iloc[-1]\n",
    "                index_cache=(index_daily_pnl,index_acct_curve,index_size_df,index_ind_return)\n",
    "        else:\n",
    "            index_cache=(None,None,None,None)\n",
    "            \n",
    "        '''Finally put everything together'''    \n",
    "        portfolio_df=pd.concat([alpha_df,index_df],axis=1)\n",
    "            \n",
    "        portfolio_size_df=pd.concat([alpha_cache[2],index_cache[2]],axis=1).sort_index()\n",
    "  \n",
    "        portfolio_daily_pnl=pd.concat([alpha_cache[0],index_cache[0]],axis=1).sort_index()\n",
    "\n",
    "        portfolio_acct_curve=portfolio_daily_pnl.cumsum().ffill().sum(axis=1)\n",
    "        portfolio_ind_return=alpha_cache[3].copy()\n",
    "        \n",
    "        portfolio_gross=np.abs(portfolio_size_df).sum(axis=1).sort_index()\n",
    "        portfolio_turnover=(np.abs(alpha_size_df.fillna(0.0).diff(1)).sum().sum())/(portfolio_size_df.shape[0]/260)\n",
    "        \n",
    "        portfolio_cache=(portfolio_daily_pnl,portfolio_acct_curve,portfolio_size_df,portfolio_ind_return,portfolio_gross,\\\n",
    "                         portfolio_turnover,portfolio_df)\n",
    "        \n",
    "        self.portfolio_account=portfolio_cache #save for later use\n",
    "        \n",
    "        \n",
    "        return long_cache,short_cache,alpha_cache,portfolio_cache\n",
    "    \n",
    "    def plot_account(self,title,figsize=[10,4],portfolio=None):\n",
    "        '''\n",
    "        Plot the account curve\n",
    "        '''\n",
    "        if portfolio is None:\n",
    "            try:\n",
    "                portfolio_cache=self.portfolio_account\n",
    "\n",
    "            except AttributeError:\n",
    "                print(\"Execute the signal_account first!\")  \n",
    "                return None\n",
    "        else:\n",
    "            portfolio_cache=portfolio\n",
    "        \n",
    "        plot_signal(title,figsize,portfolio_cache)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_signal(portfolio_list,capital_list):\n",
    "    \n",
    "    \n",
    "    daily_pnl_list=[]\n",
    "    signal_df_list=[]\n",
    "    portfolio_size_list=[]\n",
    "    for i in range(len(portfolio_list)):\n",
    "\n",
    "        daily_pnl_list.append(portfolio_list[i][0]*capital_list[i]/np.sum(capital_list))\n",
    "        signal_df_list.append(portfolio_list[i][-1]*capital_list[i]/np.sum(capital_list))\n",
    "        portfolio_size_list.append(portfolio_list[i][2]*capital_list[i]/np.sum(capital_list))\n",
    "    \n",
    "    daily_pnl=pd.concat(daily_pnl_list,axis=1)\n",
    "    signal_df=pd.concat(signal_df_list,axis=1)\n",
    "    size_df=pd.concat(portfolio_size_list,axis=1)\n",
    "    \n",
    "    account_curve=daily_pnl.cumsum().ffill().sum(axis=1)\n",
    "    ind_return=daily_pnl.cumsum().ffill().iloc[-1]\n",
    "\n",
    "    gross=np.abs(size_df).sum(axis=1)\n",
    "    turnover=(np.abs(size_df.fillna(0.0).diff(1)).sum().sum())/(size_df.shape[0]/260)\n",
    "    \n",
    "    portfolio_cache=(daily_pnl,account_curve,size_df,ind_return,gross,turnover,signal_df)\n",
    "    return portfolio_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpe_by_year(account_curve,year_list):\n",
    "    result_dict={}\n",
    "    for i in year_list:\n",
    "        sub_curve=account_curve.iloc[(account_curve.index>=pd.Timestamp(i,1,1))&(account_curve.index<=pd.Timestamp(i,12,31))]\n",
    "        result_dict[i]=trading_analytics_simp(sub_curve)[0]\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dd_by_year(account_curve,year_list):\n",
    "    result_dict={}\n",
    "    for i in year_list:\n",
    "        sub_curve=account_curve.iloc[(account_curve.index>=pd.Timestamp(i,1,1))&(account_curve.index<=pd.Timestamp(i,12,31))]\n",
    "        result_dict[i]=trading_analytics_simp(sub_curve)[1]\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1-Europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Europe_new=signal(fundamental_df=Europe2_old,\n",
    "                   price_df=Europe_VWAP,\n",
    "                   EAR_period=1,\n",
    "                   entry=3,\n",
    "                   long_criteria=((2,1000),(0.02,1000),None,None),\n",
    "                   short_criteria=((-1000,-2),(-1000,-0.02),None,(5000,10000000)),\n",
    "                   holding=30,\n",
    "                   start=pd.Timestamp(2010,1,1),\n",
    "                   end=pd.Timestamp(2019,3,29),\n",
    "                   old_position=False,\n",
    "                   new_signal=True,\n",
    "                   revision_adjust=(True,(0.02,0.05),(-0.05,-0.02),2),\n",
    "                  early_exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_long_Europe,_short_Europe,_alpha_Europe,_portfolio_Europe=Europe_new.signal_account(stop=[9,12,\"rel\"],\n",
    "                                                           gross=(26,13),\n",
    "                                                           index_df=abs_return_index_Europe.loc[\"SX5E Index\"].to_frame('Europe'),\n",
    "                                                           net_level=0,\n",
    "                                                           risk_parity=True,\n",
    "                                                           liquidity=0.2,\n",
    "                                                           capital=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signal(\"Europe new signal - no stop\",[10,4],Europe_new.portfolio_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signal(\"Europe new signal - 100k abs stop\",[10,4],Europe_new.portfolio_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signal(\"Europe new signal - 200k abs stop\",[10,4],Europe_new.portfolio_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signal(\"Europe new signal - 100k rel stop\",[10,4],Europe_new.portfolio_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signal(\"Europe new signal - 200k rel stop\",[10,4],Europe_new.portfolio_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Europe_orig=signal(fundamental_df=Europe2_old,\n",
    "                    price_df=Europe_price,\n",
    "                    EAR_period=1,\n",
    "                    entry=2,\n",
    "                    long_criteria=((1,1000),(0.03,1000),None,None),\n",
    "                    short_criteria=((-1000,-2),(-1000,-0.03),None,(5000,10000000)),\n",
    "                    holding=30,\n",
    "                    start=pd.Timestamp(2010,1,1),\n",
    "                    end=pd.Timestamp(2019,3,29),\n",
    "                    old_position=False,\n",
    "                    new_signal=False,\n",
    "                    revision_adjust=(False,0.02,0.1,2),\n",
    "                    early_exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_long_Europe,_short_Europe,_alpha_Europe,_portfolio_Europe=Europe_orig.signal_account(stop=[4.5,6,\"rel\"],\n",
    "                                                           gross=(25,13),\n",
    "                                                           index_df=abs_return_index_Europe.loc[\"SX5E Index\"].to_frame('Europe'),\n",
    "                                                           net_level=0,\n",
    "                                                           risk_parity=True,\n",
    "                                                           liquidity=0.2,\n",
    "                                                           capital=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signal(\"Europe orig signal - no stop\",[10,4],Europe_orig.portfolio_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signal(\"Europe orig signal - abs stop 100k\",[10,4],Europe_orig.portfolio_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signal(\"Europe orig signal - abs stop 200k\",[10,4],Europe_orig.portfolio_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signal(\"Europe orig signal - rel stop 100k\",[10,4],Europe_orig.portfolio_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signal(\"Europe orig signal - rel stop 200k\",[10,4],Europe_orig.portfolio_account)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 - Asia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia_new=signal(fundamental_df=Asia2_old,\n",
    "                price_df=Asia_VWAP,\n",
    "                EAR_period=2,\n",
    "                entry=3,\n",
    "                long_criteria=((1,1000),(0.02,1000),None,None),\n",
    "                short_criteria=None,\n",
    "                holding=20,\n",
    "                start=pd.Timestamp(2010,1,1),\n",
    "                end=pd.Timestamp(2019,3,29),\n",
    "                old_position=False,\n",
    "                new_signal=True,\n",
    "                revision_adjust=(True,(0.02,0.05),(-0.1,-0.02),2),\n",
    "                early_exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_long_Asia,_short_Asia,_alpha_Asia,_portfolio_Asia=Asia_new.signal_account(stop=[5.5,None,'rel'],\n",
    "                                                           gross=(28,None),\n",
    "                                                           index_df=abs_return_index_Asia.loc[[\"AS51 Index\",\"HSI Index\",\n",
    "                                                                                                \"TPX Index\",\"KOSPI Index\",\n",
    "                                                                                                \"MXSG Index\",\"TAMSCI Index\"]].T,\n",
    "                                                           net_level=0,\n",
    "                                                           risk_parity=True,\n",
    "                                                           liquidity=0.2,\n",
    "                                                           capital=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia_new.plot_account(\"Asia new signal - no stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Asia_new.plot_account(\"Asia new signal - 100k abs stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia_new.plot_account(\"Asia new signal - 200k abs stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia_new.plot_account(\"Asia new signal - 100k rel stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia_new.plot_account(\"Asia new signal - 200k rel stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia_orig=signal(fundamental_df=Asia2_old,\n",
    "                 price_df=Asia_price,\n",
    "                 EAR_period=2,\n",
    "                 entry=2,\n",
    "                 long_criteria=((2,1000),(0.02,1000),None,None),\n",
    "                 short_criteria=None,\n",
    "                 holding=20,\n",
    "                 start=pd.Timestamp(2010,1,1),\n",
    "                 end=pd.Timestamp(2019,3,29),\n",
    "                 old_position=False,\n",
    "                 new_signal=False,\n",
    "                 revision_adjust=(False,(0.02,0.1),(-0.1,-0.02),3),\n",
    "                 early_exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_long_Asia,_short_Asia,_alpha_Asia,_portfolio_Asia=Asia_orig.signal_account(stop=[16,None,'rel'],\n",
    "                                                           gross=(29,None),\n",
    "                                                           index_df=abs_return_index_Asia.loc[[\"AS51 Index\",\"HSI Index\",\n",
    "                                                                                                \"TPX Index\",\"KOSPI Index\",\n",
    "                                                                                                \"MXSG Index\",\"TAMSCI Index\"]].T,\n",
    "                                                           net_level=0,\n",
    "                                                           risk_parity=True,\n",
    "                                                           liquidity=0.2,\n",
    "                                                           capital=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia_orig.plot_account(\"Asia orig signal - no stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia_orig.plot_account(\"Asia orig signal - 100k abs stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia_orig.plot_account(\"Asia orig signal - 200k abs stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia_orig.plot_account(\"Asia orig signal - 100k rel stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia_orig.plot_account(\"Asia orig signal - 200k rel stop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 - US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_new=signal(fundamental_df=US1_old,\n",
    "               price_df=US_VWAP,\n",
    "               EAR_period=1,\n",
    "               entry=2,\n",
    "               long_criteria=((2,1000),(0.05,1000),None,None),\n",
    "               short_criteria=((-1000,-2),(-1000,-0.03),None,(5000,10000000)),\n",
    "               holding=30,\n",
    "               start=pd.Timestamp(2010,1,1),\n",
    "               end=pd.Timestamp(2019,3,29),\n",
    "               old_position=False,\n",
    "               new_signal=True,\n",
    "               revision_adjust=(True,(0.05,0.1),(-0.1,-0.03),2),\n",
    "               early_exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_long,_short,_alpha,_portfolio=US_new.signal_account(stop=[20,18,\"rel\"],\n",
    "                                                           gross=(30,15),\n",
    "                                                           index_df=(0.5*abs_return_index_US.loc[\"SPX Index\"]+\\\n",
    "                                                          0.5*abs_return_index_US.loc[\"RTY Index\"]).to_frame(\"US_index\"),\n",
    "                                                           net_level=0,\n",
    "                                                           risk_parity=True,\n",
    "                                                           liquidity=0.2,\n",
    "                                                           capital=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signal(\"US new signal - no stop\",[10,4],US_new.portfolio_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signal(\"US new signal - abs stop 100k\",[10,4],US_new.portfolio_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signal(\"US new signal - abs stop 200k\",[10,4],US_new.portfolio_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signal(\"US new signal - rel stop 100k\",[10,4],US_new.portfolio_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signal(\"US new signal - rel stop 200k\",[10,4],US_new.portfolio_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_orig=signal(fundamental_df=US1_old,\n",
    "                price_df=US_price,\n",
    "                EAR_period=1,\n",
    "                entry=2,\n",
    "                long_criteria=((3,1000),(0.05,1000),None,None),\n",
    "                short_criteria=((-1000,-2),(-1000,-0.03),None,(5000,10000000)),\n",
    "                holding=30,\n",
    "                start=pd.Timestamp(2010,1,1),\n",
    "                end=pd.Timestamp(2019,3,29),\n",
    "                old_position=False,\n",
    "                new_signal=False,\n",
    "                revision_adjust=(False,(0.05,0.2),(-0.1,-0.03),2),\n",
    "               early_exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_long,_short,_alpha,_portfolio=US_orig.signal_account(stop=None,\n",
    "                                                           gross=(26,15),\n",
    "                                                           index_df=(0.5*abs_return_index_US.loc[\"SPX Index\"]+\\\n",
    "                                                          0.5*abs_return_index_US.loc[\"RTY Index\"]).to_frame(\"US_index\"),\n",
    "                                                           net_level=0,\n",
    "                                                           risk_parity=True,\n",
    "                                                           liquidity=0.2,\n",
    "                                                           capital=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signal(\"US orig signal - no exit\",[10,4],US_orig.portfolio_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Fitting choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitting_df(long,EAR_list,revision_list,fundamental_df,price_df,EAR_period,holding,start,end,stop,gross,index_df,\n",
    "               net_level,risk_parity,liquidity,capital,exclusive,old_position,new_signal):\n",
    "    '''\n",
    "    Create a dataframe of sharpe and drawdown for different fitting variations\n",
    "    EAR/revision list includes the upper bound\n",
    "    Exclusive means gap and non eclusive means from that to the maximum\n",
    "    '''\n",
    "    year_list=range(start.year,end.year+1)\n",
    "    EAR_list.sort()\n",
    "    revision_list.sort()\n",
    "    \n",
    "    criteria_list_short=list(itertools.product(*[EAR_list[1:],revision_list[1:]]))\n",
    "    criteria_list_long=list(itertools.product(*[EAR_list[:-1],revision_list[:-1]]))\n",
    "    \n",
    "    sharpe_dict={}\n",
    "    dd_dict={}\n",
    "    if exclusive:\n",
    "        if long:\n",
    "            for s in criteria_list_long:\n",
    "                signal_class=signal(fundamental_df=fundamental_df,\n",
    "                                   price_df=price_df,\n",
    "                                   EAR_period=EAR_period,\n",
    "                                   long_criteria=((s[0],EAR_list[EAR_list.index(s[0])+1]),\\\n",
    "                                                  (s[1],revision_list[revision_list.index(s[1])+1]),None,None),\n",
    "                                   short_criteria=None,\n",
    "                                   holding=holding,\n",
    "                                   start=start,\n",
    "                                   end=end,\n",
    "                                   old_position=old_position,\n",
    "                                   new_signal=new_signal)\n",
    "                \n",
    "                portfolio=signal_class.signal_account(stop=stop,\n",
    "                                                      gross=gross,\n",
    "                                                      index_df=index_df.copy(),\n",
    "                                                      net_level=net_level,\n",
    "                                                      risk_parity=risk_parity,\n",
    "                                                      liquidity=liquidity,\n",
    "                                                      capital=capital)\n",
    "\n",
    "                sharpe_dict[s]=sharpe_by_year(portfolio[-1][1],year_list)\n",
    "                dd_dict[s]=dd_by_year(portfolio[-1][1],year_list)\n",
    "                \n",
    "        else:\n",
    "            for s in criteria_list_short:\n",
    "                signal_class=signal(fundamental_df=fundamental_df,\n",
    "                                   price_df=price_df,\n",
    "                                   EAR_period=EAR_period,\n",
    "                                   long_criteria=None,\n",
    "                                   short_criteria=((EAR_list[EAR_list.index(s[0])-1],s[0]),\\\n",
    "                                                   (revision_list[revision_list.index(s[1])-1],s[1]),None,(5000,10000000)),\n",
    "                                   holding=holding,\n",
    "                                   start=start,\n",
    "                                   end=end,\n",
    "                                   old_position=old_position,\n",
    "                                   new_signal=new_signal)\n",
    "\n",
    "                portfolio=signal_class.signal_account(stop=stop,\n",
    "                                                      gross=gross,\n",
    "                                                      index_df=index_df.copy(),\n",
    "                                                      net_level=net_level,\n",
    "                                                      risk_parity=risk_parity,\n",
    "                                                      liquidity=liquidity,\n",
    "                                                      capital=capital)\n",
    "\n",
    "                sharpe_dict[s]=sharpe_by_year(portfolio[-1][1],year_list)\n",
    "                dd_dict[s]=dd_by_year(portfolio[-1][1],year_list)\n",
    "    \n",
    "    else:\n",
    "        if long:\n",
    "            for i in criteria_list_long:\n",
    "                signal_class=signal(fundamental_df=fundamental_df,\n",
    "                                   price_df=price_df,\n",
    "                                   EAR_period=EAR_period,\n",
    "                                   long_criteria=((i[0],10),(i[1],10),None,None),\n",
    "                                   short_criteria=None,\n",
    "                                   holding=holding,\n",
    "                                   start=start,\n",
    "                                   end=end,\n",
    "                                   old_position=old_position,\n",
    "                                   new_signal=new_signal)\n",
    "\n",
    "                portfolio=signal_class.signal_account(stop=stop,\n",
    "                                                      gross=gross,\n",
    "                                                      index_df=index_df.copy(),\n",
    "                                                      net_level=net_level,\n",
    "                                                      risk_parity=risk_parity,\n",
    "                                                      liquidity=liquidity,\n",
    "                                                      capital=capital)\n",
    "\n",
    "                sharpe_dict[i]=sharpe_by_year(portfolio[-1][1],year_list)\n",
    "                dd_dict[i]=dd_by_year(portfolio[-1][1],year_list)\n",
    "                \n",
    "        else:\n",
    "            for i in criteria_list_short:\n",
    "                signal_class=signal(fundamental_df=fundamental_df,\n",
    "                                   price_df=price_df,\n",
    "                                   EAR_period=EAR_period,\n",
    "                                   long_criteria=None,\n",
    "                                   short_criteria=((-10,i[0]),(-10,i[1]),None,(5000,10000000)),\n",
    "                                   holding=holding,\n",
    "                                   start=start,\n",
    "                                   end=end,\n",
    "                                   old_position=old_position,\n",
    "                                   new_signal=new_signal)\n",
    "                \n",
    "                portfolio=signal_class.signal_account(stop=stop,\n",
    "                                                      gross=gross,\n",
    "                                                      index_df=index_df.copy(),\n",
    "                                                      net_level=net_level,\n",
    "                                                      risk_parity=risk_parity,\n",
    "                                                      liquidity=liquidity,\n",
    "                                                      capital=capital)\n",
    "\n",
    "                sharpe_dict[i]=sharpe_by_year(portfolio[-1][1],year_list)\n",
    "                dd_dict[i]=dd_by_year(portfolio[-1][1],year_list)\n",
    "\n",
    "    sharpe_df=pd.DataFrame(sharpe_dict)\n",
    "    dd_df=pd.DataFrame(dd_dict)\n",
    "        \n",
    "    return sharpe_df,dd_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Europe_long_sharpe,Europe_long_dd=fitting_df(True,[1,2,3,4],[0.02,0.03,0.04,0.05,10],Europe2,Europe_price,2,30,\\\n",
    "                                             pd.Timestamp(2010,1,1),pd.Timestamp(2018,12,31),(8,None),(30,None)\\\n",
    "                                             ,abs_return_index_Europe.loc[\"SX5E Index\"].to_frame('Europe'),0,True,0.2,50,True,\n",
    "                                            False,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Europe_long_sharpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Europe_short_sharpe,Europe_short_dd=fitting_df(False,[-1,-2,-3,-4],[-0.02,-0.03,-0.04,-0.05,-0.06],Europe,Europe_price,2,30,\\\n",
    "                                             pd.Timestamp(2007,1,1),pd.Timestamp(2018,12,31),(8,8),(30,30)\\\n",
    "                                             ,abs_return_index_Europe.loc[\"SX5E Index\"].to_frame('Europe'),0,True,0.2,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Europe_short_sharpe.to_csv(\"Europe_short_sharpe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Europe_short_dd.to_csv(\"Europe_short_dd.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_long_sharpe,US_long_dd=fitting_df(True,[1,2,3,4],[0.02,0.03,0.04,0.05,0.06,0.07],\\\n",
    "                                     US,US_price,2,30,pd.Timestamp(2007,1,1),pd.Timestamp(2018,12,31),(8,None),(30,None)\\\n",
    "                                             ,(0.5*abs_return_index_US.loc[\"SPX Index\"]+0.5*abs_return_index_US.loc[\"RTY Index\"])\\\n",
    "                                     .to_frame('US'),0,True,0.2,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_US_long_sharpe,adj_US_long_dd=fitting_df(True,[1,2,3,4],[0.02,0.03,0.04,0.05,0.06,0.07],\\\n",
    "                                     adj_US,US_price,2,30,pd.Timestamp(2007,1,1),pd.Timestamp(2018,12,31),(8,None),(30,None)\\\n",
    "                                             ,(0.5*abs_return_index_US.loc[\"SPX Index\"]+0.5*abs_return_index_US.loc[\"RTY Index\"])\\\n",
    "                                     .to_frame('US'),0,True,0.2,50,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_short_sharpe,US_short_dd=fitting_df(False,[-1,-2,-3,-4],[-0.02,-0.03,-0.04,-0.05,-0.06,-0.07],\\\n",
    "                                     US,US_price,2,30,pd.Timestamp(2007,1,1),pd.Timestamp(2018,12,31),(8,8),(30,30)\\\n",
    "                                             ,(0.5*abs_return_index_US.loc[\"SPX Index\"]+0.5*abs_return_index_US.loc[\"RTY Index\"])\\\n",
    "                                     .to_frame('US'),0,True,0.2,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_US_short_sharpe,adj_US_short_dd=fitting_df(False,[-1,-2,-3],[-0.02,-0.03,-0.04,-0.05],\\\n",
    "                                     adj_US,US_price,2,30,pd.Timestamp(2007,1,1),pd.Timestamp(2018,12,31),(8,8),(30,30)\\\n",
    "                                             ,(0.5*abs_return_index_US.loc[\"SPX Index\"]+0.5*abs_return_index_US.loc[\"RTY Index\"])\\\n",
    "                                     .to_frame('US'),0,True,0.2,50,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_short_sharpe.to_csv(\"US_short_sharpe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_short_dd.to_csv(\"US_short_dd.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia_long_sharpe,Asia_long_dd=fitting_df(True,[1,2,3,4],[0.02,0.03,0.04,0.05,0.06,0.07],\\\n",
    "                                     Asia,Asia_price,2,20,pd.Timestamp(2007,1,1),pd.Timestamp(2018,12,31),(8,None),(30,None)\\\n",
    "                                             ,abs_return_index_Asia.loc[[\"AS51 Index\",\"HSI Index\",\n",
    "                                                                                                \"TPX Index\",\"KOSPI Index\",\n",
    "                                                                                                \"MXSG Index\",\"TAMSCI Index\"]].T\\\n",
    "                                     ,0,True,0.2,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia_long_sharpe.to_csv(\"Asian_long_sharpe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia_long_dd.to_csv(\"Asian_long_dd.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 - Drawdown analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawdown_days(acct_curve,timestamp):\n",
    "    dd=acct_curve-acct_curve.cummax()\n",
    "    previous_high=acct_curve.loc[:timestamp].max()\n",
    "    high_timestamp=acct_curve[acct_curve==previous_high].index[0]\n",
    "    days=acct_curve.index.tolist().index(timestamp)-acct_curve.index.tolist().index(high_timestamp)\n",
    "    return (high_timestamp,days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawdown_df(portfolio_cache):\n",
    "    drawdown=portfolio_cache[-1].iloc[:,-1].to_frame()\n",
    "    drawdown.columns=[\"Index\"]\n",
    "    drawdown[\"Index vol\"]=drawdown[\"Index\"].rolling(30).std()\n",
    "    drawdown[\"Portfolio\"]=portfolio_cache[1].diff(1)\n",
    "    drawdown[\"Account\"]=portfolio_cache[1].copy()\n",
    "    drawdown[\"Drawdown\"]= drawdown[\"Account\"]- drawdown[\"Account\"].cummax()\n",
    "    drawdown[\"Drawdown days\"]= drawdown.apply(lambda x: drawdown_days(drawdown[\"Account\"],x.name)[1],axis=1)\n",
    "    drawdown[\"Drawdown start\"]= drawdown.apply(lambda x: drawdown_days(drawdown[\"Account\"],x.name)[0],axis=1)\n",
    "    drawdown[\"Gross\"]=np.abs(portfolio_cache[2]).sum(axis=1)\n",
    "    return drawdown\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def portfolio_drawdown_df(portfolio_cache,stop,multiple):\n",
    "    portfolio_cache_list=list(portfolio_cache)\n",
    "    orig_size=portfolio_cache_list[2]\n",
    "    orig_account=portfolio_cache_list[1]\n",
    "    dd=orig_account-orig_account.cummax()\n",
    "    stop_evaluate=dd[dd.shift(1)<stop]\n",
    "    adj_size=orig_size.apply(lambda x: x*multiple if x.name in stop_evaluate.index else x,axis=1)\n",
    "    \n",
    "    new_portfolio_cache=portfolio_cache_list.copy()\n",
    "    new_portfolio_cache[2]=adj_size\n",
    "    new_portfolio_cache[0]=adj_size.shift(1)*new_portfolio_cache[-1]\n",
    "    new_portfolio_cache[1]=new_portfolio_cache[0].sum(axis=1).cumsum()\n",
    "    return tuple(new_portfolio_cache)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8- Quarterly review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_df(signal_class,start,end,old_position):\n",
    "    '''\n",
    "    dataframe for review purpose\n",
    "    '''\n",
    "    try:\n",
    "        long_abs_return=slice_universe(signal_class.long_df.copy(),start,end,old_position)\n",
    "        long_abs_return[long_abs_return==0]=None\n",
    "        long_rel_return=long_abs_return.subtract(signal_class.portfolio_account[-1].iloc[:,-1],axis=0)\n",
    "        long_df=((1+long_rel_return).cumprod()-1).ffill().iloc[-1].to_frame()\n",
    "        long_df[\"Ticker\"]=long_df.apply(lambda x:x.name[0],axis=1)\n",
    "        long_df[\"Date\"]=long_df.apply(lambda x:datetime.strptime(x.name[1],\"%d/%b/%Y\"),axis=1)\n",
    "        \n",
    "        long_size=slice_universe(signal_class.long_cache[2].copy(),start,end,old_position)\n",
    "        long_df[\"Size\"]=long_size.apply(lambda x: x.dropna().iloc[0],axis=0)\n",
    "    except:\n",
    "        long_df=None\n",
    "\n",
    "    try:\n",
    "        short_abs_return=-slice_universe(signal_class.short_df.copy(),start,end,old_position)\n",
    "        short_abs_return[short_abs_return==0]=None\n",
    "        short_rel_return=short_abs_return.add(signal_class.portfolio_account[-1].iloc[:,-1],axis=0)\n",
    "        short_df=((1+short_rel_return).cumprod()-1).ffill().iloc[-1].to_frame()\n",
    "        short_df[\"Ticker\"]=short_df.apply(lambda x:x.name[0],axis=1)\n",
    "        short_df[\"Date\"]=short_df.apply(lambda x:datetime.strptime(x.name[1],\"%d/%b/%Y\"),axis=1)\n",
    "        \n",
    "        short_size=slice_universe(signal_class.short_cache[2].copy(),start,end,old_position)\n",
    "        short_df[\"Size\"]=short_size.apply(lambda x: x.dropna().iloc[0],axis=0)\n",
    "    except:\n",
    "        short_df=None\n",
    "        \n",
    "    return long_df,short_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia_long,Asia_short=review_df(Asia_fit1,pd.Timestamp(2019,1,1),pd.Timestamp(2019,3,29),False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia_long.to_csv(\"Asia_long.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 - Sizing for the live trading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 -  US live sizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_live_orig=signal(fundamental_df=US2,\n",
    "                   price_df=US_price,\n",
    "                   EAR_period=1,\n",
    "               entry=2,\n",
    "                   long_criteria=((3,1000),(0.05,1000),None,None),\n",
    "                   short_criteria=((-1000,-2),(-1000,-0.03),None,(5000,10000000)),\n",
    "                   holding=30,\n",
    "                   start=pd.Timestamp(2019,1,1),\n",
    "                   end=pd.Timestamp(2019,3,29),\n",
    "                  old_position=False,\n",
    "                  new_signal=False,\n",
    "                 revision_adjust=(False,(0.05,0.2),(-0.1,-0.03),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_long_US_live,_short_US_live,_alpha_US_live,_portfolio_US_live=US_live_orig.signal_account(stop=(8,8),\n",
    "                                                           gross=(30,15),\n",
    "                                                           index_df=(0.5*abs_return_index_US.loc[\"SPX Index\"]+\\\n",
    "                                                          0.5*abs_return_index_US.loc[\"RTY Index\"]).to_frame(\"US_index\"),\n",
    "                                                           net_level=0,\n",
    "                                                           risk_parity=True,\n",
    "                                                           liquidity=0.2,\n",
    "                                                           capital=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_live.plot_account(\"US reference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "US reference sizing and volatility\n",
    "'''\n",
    "print(\"US long size is \"+str(_long_US_live[2].apply(lambda x:x.dropna().iloc[0],axis=0).mean()))\n",
    "print(\"US long volatility is \"+str(US2.loc[_long_US_live[-1].index][\"30d_vol\"].mean()))\n",
    "print(\"US long number is \"+str(_long_US_live[2].count(axis=1).mean()))\n",
    "print(\"US short size is \"+str(_short_US_live[2].apply(lambda x:x.dropna().iloc[0],axis=0).mean()))\n",
    "print(\"US short volatility is \"+str(US2.loc[_short_US_live[-1].index][\"30d_vol\"].mean()))\n",
    "print(\"US short number is \"+str(_short_US_live[2].count(axis=1).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_live_new=signal(fundamental_df=US1,\n",
    "                   price_df=US_VWAP,\n",
    "                   EAR_period=1,\n",
    "               entry=2,\n",
    "                   long_criteria=((2,1000),(0.05,1000),None,None),\n",
    "                   short_criteria=((-1000,-2),(-1000,-0.03),None,(5000,10000000)),\n",
    "                   holding=30,\n",
    "                   start=pd.Timestamp(2019,1,1),\n",
    "                   end=pd.Timestamp(2019,3,29),\n",
    "                  old_position=False,\n",
    "                  new_signal=True,\n",
    "                 revision_adjust=(True,(0.05,0.2),(-0.1,-0.03),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_long_US,_short_US,_alpha_US,_portfolio_US=US_live_new.signal_account(stop=(8,8),\n",
    "                                                           gross=(30,15),\n",
    "                                                           index_df=(0.5*abs_return_index_US.loc[\"SPX Index\"]+\\\n",
    "                                                          0.5*abs_return_index_US.loc[\"RTY Index\"]).to_frame(\"US_index\"),\n",
    "                                                                                      net_level=0,\n",
    "                                                           risk_parity=True,\n",
    "                                                           liquidity=0.2,\n",
    "                                                           capital=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,long_lower_size_US=sizing(US_live_new.long_df,US_live_new.long_df,30,US_live_new.fundamental_df,True,abs_return_US\\\n",
    "                    ,True,0.2,100, US_live_new.revision_adjust, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,short_lower_size_US=sizing(US_live_new.short_df,US_live_new.short_df,30,US_live_new.fundamental_df,True,abs_return_US\\\n",
    "                    ,True,0.2,100, US_live_new.revision_adjust,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "US reference sizing and volatility\n",
    "'''\n",
    "print(\"US long size is \"+str(long_lower_size_US))\n",
    "print(\"US long volatility is \"+str(US1.loc[_long_US[-1].index][\"30d_vol\"].mean()))\n",
    "print(\"US long number is \"+str(_long_US[2].count(axis=1).mean()))\n",
    "print(\"US short size is \"+str(short_lower_size_US))\n",
    "print(\"US short volatility is \"+str(US1.loc[_short_US[-1].index][\"30d_vol\"].mean()))\n",
    "print(\"US short number is \"+str(_short_US[2].count(axis=1).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 - Europe live sizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Europe_live_old=signal(fundamental_df=Europe2,\n",
    "                    price_df=Europe_price,\n",
    "                   EAR_period=1,\n",
    "                    entry=2,\n",
    "                   long_criteria=((1,1000),(0.03,1000),None,None),\n",
    "                   short_criteria=((-1000,-2),(-1000,-0.03),None,(5000,10000000)),\n",
    "                   holding=30,\n",
    "                   start=pd.Timestamp(2019,1,1),\n",
    "                   end=pd.Timestamp(2019,3,29),\n",
    "                  old_position=False,\n",
    "                  new_signal=False,\n",
    "                 revision_adjust=(False,0.02,0.1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_long_Europe_live,_short_Europe_live,_alpha_Europe_live,_portfolio_Europe_live=Europe_live_old.signal_account(stop=(8,8),\n",
    "                                                           gross=(30,15),\n",
    "                                                           index_df=abs_return_index_Europe.loc[\"SX5E Index\"].to_frame('Europe'),\n",
    "                                                           net_level=0,\n",
    "                                                           risk_parity=True,\n",
    "                                                           liquidity=0.2,\n",
    "                                                           capital=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Europe_live1.plot_account(\"Europe reference 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Europe reference sizing and volatility\n",
    "'''\n",
    "print(\"Europe long size is \"+str(_long_Europe_live[2].apply(lambda x:x.dropna().iloc[0],axis=0).mean()))\n",
    "print(\"Europe long volatility is \"+str(Europe2.loc[_long_Europe_live[-1].index][\"30d_vol\"].mean()))\n",
    "print(\"Europe long number is \"+str(_long_Europe_live[2].count(axis=1).mean()))\n",
    "print(\"Europe short size is \"+str(_short_Europe_live[2].apply(lambda x:x.dropna().iloc[0],axis=0).mean()))\n",
    "print(\"Europe short volatility is \"+str(Europe2.loc[_short_Europe_live[-1].index][\"30d_vol\"].mean()))\n",
    "print(\"Europe short number is \"+str(_short_Europe_live[2].count(axis=1).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Europe_live_new=signal(fundamental_df=Europe2,\n",
    "                   price_df=Europe_VWAP,\n",
    "                   EAR_period=1,\n",
    "                   entry=3,\n",
    "                   long_criteria=((2,1000),(0.02,1000),None,None),\n",
    "                   short_criteria=((-1000,-2),(-1000,-0.02),None,(5000,10000000)),\n",
    "                   holding=30,\n",
    "                   start=pd.Timestamp(2019,1,1),\n",
    "                   end=pd.Timestamp(2019,3,29),\n",
    "                  old_position=False,\n",
    "                  new_signal=True,\n",
    "                 revision_adjust=(True,(0.02,0.05),(-0.05,-0.02),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_long_Europe,_short_Europe,_alpha_Europe,_portfolio_Europe=Europe_live_new.signal_account(stop=(8,8),\n",
    "                                                           gross=(30,15),\n",
    "                                                           index_df=abs_return_index_Europe.loc[\"SX5E Index\"].to_frame('Europe'),\n",
    "                                                           net_level=0,\n",
    "                                                           risk_parity=True,\n",
    "                                                           liquidity=0.2,\n",
    "                                                           capital=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,long_lower_size=sizing(Europe_live_new.long_df,Europe_live_new.long_df,30,Europe_live_new.fundamental_df,True,abs_return_Europe\\\n",
    "                    ,True,0.2,100, Europe_live_new.revision_adjust, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,short_lower_size=sizing(Europe_live_new.short_df,Europe_live_new.short_df,30,Europe_live_new.fundamental_df,True,abs_return_Europe\\\n",
    "                    ,True,0.2,100, Europe_live_new.revision_adjust,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Europe reference sizing and volatility\n",
    "'''\n",
    "print(\"Europe long size is \"+str(long_lower_size))\n",
    "print(\"Europe long volatility is \"+str(Europe2.loc[_long_Europe[-1].index][\"30d_vol\"].mean()))\n",
    "print(\"Europe long number is \"+str(_long_Europe[2].count(axis=1).mean()))\n",
    "print(\"Europe short size is \"+str(short_lower_size))\n",
    "print(\"Europe short volatility is \"+str(Europe2.loc[_short_Europe[-1].index][\"30d_vol\"].mean()))\n",
    "print(\"Europe short number is \"+str(_short_Europe[2].count(axis=1).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia_live_new=signal(fundamental_df=Asia2,\n",
    "                   price_df=Asia_VWAP,\n",
    "                   EAR_period=1,\n",
    "                entry=2,\n",
    "                   long_criteria=((1,1000),(0.02,1000),None,None),\n",
    "                   short_criteria=None,\n",
    "                   holding=20,\n",
    "                   start=pd.Timestamp(2010,1,1),\n",
    "                   end=pd.Timestamp(2019,3,29),\n",
    "                  old_position=False,\n",
    "                  new_signal=True,\n",
    "                 revision_adjust=(True,(0.02,0.05),(-0.1,-0.02),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asia_live_old=signal(fundamental_df=Asia2,\n",
    "                   price_df=Asia_price,\n",
    "                   EAR_period=1,\n",
    "                entry=2,\n",
    "                   long_criteria=((1,1000),(0.02,1000),None,None),\n",
    "                   short_criteria=None,\n",
    "                   holding=20,\n",
    "                   start=pd.Timestamp(2019,1,1),\n",
    "                   end=pd.Timestamp(2019,3,29),\n",
    "                  old_position=False,\n",
    "                  new_signal=False,\n",
    "                 revision_adjust=(False,(0.02,0.05),(-0.1,-0.02),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_long_Asia_live,_short_Asia_live,_alpha_Asia_live,_portfolio_Asia_live=Asia_live_new.signal_account(stop=(8,None),\n",
    "                                                           gross=(20,None),\n",
    "                                                           index_df=abs_return_index_Asia.loc[[\"AS51 Index\",\"HSI Index\",\n",
    "                                                                                                \"TPX Index\",\"KOSPI Index\",\n",
    "                                                                                                \"MXSG Index\",\"TAMSCI Index\"]].T,\n",
    "                                                           net_level=0,\n",
    "                                                           risk_parity=True,\n",
    "                                                           liquidity=0.2,\n",
    "                                                           capital=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Europe reference sizing and volatility\n",
    "'''\n",
    "print(\"Asia long size is \"+str(_long_Asia_live[2].apply(lambda x:x.dropna().iloc[0],axis=0).mean()))\n",
    "print(\"Asia long volatility is \"+str(Asia2.loc[_long_Asia_live[-1].index][\"30d_vol\"].mean()))\n",
    "print(\"Asia long number is \"+str(_long_Asia_live[2].count(axis=1).mean()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
